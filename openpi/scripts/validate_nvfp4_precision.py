#!/usr/bin/env python3
"""
éªŒè¯ NVFP4 ç²¾åº¦æ­£ç¡®æ€§ - è·³è¿‡ LIBERO ç¯å¢ƒ

æµ‹è¯•é¡¹ï¼š
1. NVFP4Linear å•å±‚ç²¾åº¦
2. NVFP4MLP æ¨¡å—ç²¾åº¦
3. å®Œæ•´æ¨¡å‹è¾“å‡ºä¸€è‡´æ€§
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import sys
import time

sys.path.insert(0, '/home/heima-thor/suliang/Turbo-Pi/openpi/src')

from openpi.models_pytorch.nvfp4_mlp import (
    NVFP4Linear,
    NVFP4MLP,
    quantize_to_nvfp4_sim,
    dequantize_nvfp4_sim,
    BLOCK_SIZE,
)


def test_nvfp4_linear_precision():
    """æµ‹è¯• NVFP4Linear å•å±‚ç²¾åº¦"""
    print("=" * 70)
    print("Test 1: NVFP4Linear å•å±‚ç²¾åº¦")
    print("=" * 70)

    device = torch.device('cuda')
    in_features = 2048
    out_features = 8192
    batch_size = 16

    # åˆ›å»º BF16 Linear
    linear_bf16 = nn.Linear(in_features, out_features, bias=False).to(device, dtype=torch.bfloat16)

    # åˆ›å»º NVFP4 Linear (ä½¿ç”¨ simulation mode - æ…¢ä½†ç²¾ç¡®)
    nvfp4_linear = NVFP4Linear.from_linear(linear_bf16, BLOCK_SIZE, use_cutlass=True)
    nvfp4_linear = nvfp4_linear.to(device)

    # æµ‹è¯•è¾“å…¥
    x = torch.randn(batch_size, in_features, device=device, dtype=torch.bfloat16)

    # BF16 è¾“å‡º
    with torch.no_grad():
        out_bf16 = linear_bf16(x)

    # NVFP4 æ¨¡æ‹Ÿè¾“å‡º (ä½¿ç”¨ simulation mode)
    nvfp4_linear.use_cutlass = False
    with torch.no_grad():
        out_nvfp4_sim = nvfp4_linear(x)

    # NVFP4 CUTLASS è¾“å‡º
    nvfp4_linear.use_cutlass = True
    with torch.no_grad():
        out_nvfp4_cutlass = nvfp4_linear(x)

    # è®¡ç®— cosine similarity
    cos_sim_vs_bf16 = F.cosine_similarity(
        out_nvfp4_sim.flatten().float().unsqueeze(0),
        out_bf16.flatten().float().unsqueeze(0)
    ).item()

    cos_cutlass_vs_sim = F.cosine_similarity(
        out_nvfp4_cutlass.flatten().float().unsqueeze(0),
        out_nvfp4_sim.flatten().float().unsqueeze(0)
    ).item()

    cos_cutlass_vs_bf16 = F.cosine_similarity(
        out_nvfp4_cutlass.flatten().float().unsqueeze(0),
        out_bf16.flatten().float().unsqueeze(0)
    ).item()

    print(f"  Input shape:  {x.shape}")
    print(f"  Output shape: {out_bf16.shape}")
    print()
    print(f"  NVFP4 Sim vs BF16:     {cos_sim_vs_bf16:.6f}")
    print(f"  CUTLASS vs NVFP4 Sim:  {cos_cutlass_vs_sim:.6f}")
    print(f"  CUTLASS vs BF16:       {cos_cutlass_vs_bf16:.6f}")
    print()

    if cos_cutlass_vs_sim > 0.99:
        print("  âœ… CUTLASS ä¸ Python æ¨¡æ‹Ÿä¸€è‡´ (>0.99)")
    else:
        print("  âŒ CUTLASS ä¸ Python æ¨¡æ‹Ÿä¸ä¸€è‡´")

    return cos_cutlass_vs_sim > 0.99


def test_nvfp4_mlp_precision():
    """æµ‹è¯• NVFP4MLP æ¨¡å—ç²¾åº¦"""
    print()
    print("=" * 70)
    print("Test 2: NVFP4MLP æ¨¡å—ç²¾åº¦ (GemmaMLP ç»“æ„)")
    print("=" * 70)

    device = torch.device('cuda')
    hidden_size = 2048
    intermediate_size = 8192
    batch_size = 16

    # åˆ›å»ºæ¨¡æ‹Ÿçš„ GemmaMLP
    class DummyGemmaMLP(nn.Module):
        def __init__(self, hidden_size, intermediate_size):
            super().__init__()
            self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=False)
            self.up_proj = nn.Linear(hidden_size, intermediate_size, bias=False)
            self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False)

        def forward(self, x):
            gate = F.gelu(self.gate_proj(x), approximate='tanh')
            up = self.up_proj(x)
            return self.down_proj(gate * up)

    # åˆ›å»º BF16 MLP
    mlp_bf16 = DummyGemmaMLP(hidden_size, intermediate_size).to(device, dtype=torch.bfloat16)

    # åˆ›å»º NVFP4 MLP
    nvfp4_mlp = NVFP4MLP.from_gemma_mlp(mlp_bf16, BLOCK_SIZE, use_cutlass=True)
    nvfp4_mlp = nvfp4_mlp.to(device)

    # æµ‹è¯•è¾“å…¥
    x = torch.randn(batch_size, hidden_size, device=device, dtype=torch.bfloat16)

    # BF16 è¾“å‡º
    with torch.no_grad():
        out_bf16 = mlp_bf16(x)

    # NVFP4 CUTLASS è¾“å‡º
    with torch.no_grad():
        out_nvfp4 = nvfp4_mlp(x)

    # è®¡ç®— cosine similarity
    cos_sim = F.cosine_similarity(
        out_nvfp4.flatten().float().unsqueeze(0),
        out_bf16.flatten().float().unsqueeze(0)
    ).item()

    print(f"  Input shape:  {x.shape}")
    print(f"  Output shape: {out_bf16.shape}")
    print()
    print(f"  NVFP4 MLP vs BF16 MLP: {cos_sim:.6f}")
    print()

    if cos_sim > 0.95:
        print("  âœ… NVFP4 MLP ç²¾åº¦åˆæ ¼ (>0.95)")
    elif cos_sim > 0.90:
        print("  âš ï¸  NVFP4 MLP ç²¾åº¦å¯ç”¨ (>0.90)")
    else:
        print("  âŒ NVFP4 MLP ç²¾åº¦ä¸è¶³")

    return cos_sim > 0.90


def test_timing_breakdown():
    """æµ‹è¯•æ—¶é—´åˆ†è§£ - ç¡®è®¤ç“¶é¢ˆ"""
    print()
    print("=" * 70)
    print("Test 3: æ—¶é—´åˆ†è§£ (ç¡®è®¤åœ¨çº¿é‡åŒ–ç“¶é¢ˆ)")
    print("=" * 70)

    device = torch.device('cuda')
    in_features = 2048
    out_features = 8192
    batch_size = 256

    linear = nn.Linear(in_features, out_features, bias=False).to(device)
    nvfp4_linear = NVFP4Linear.from_linear(linear, BLOCK_SIZE, use_cutlass=True)
    nvfp4_linear = nvfp4_linear.to(device)

    x = torch.randn(batch_size, in_features, device=device, dtype=torch.float32)

    # Warmup
    for _ in range(3):
        _ = nvfp4_linear(x)
    torch.cuda.synchronize()

    # æµ‹é‡ quantize_to_nvfp4_sim (è¿™æ˜¯ç“¶é¢ˆ)
    x_2d = x.view(-1, in_features)

    torch.cuda.synchronize()
    start = time.time()
    n_iters = 10
    for _ in range(n_iters):
        x_q, x_scales = quantize_to_nvfp4_sim(x_2d, BLOCK_SIZE, use_mse_search=False)
    torch.cuda.synchronize()
    quant_time = (time.time() - start) / n_iters * 1000

    # æµ‹é‡å®Œæ•´ forward
    torch.cuda.synchronize()
    start = time.time()
    for _ in range(n_iters):
        _ = nvfp4_linear(x)
    torch.cuda.synchronize()
    forward_time = (time.time() - start) / n_iters * 1000

    print(f"  Batch size: {batch_size}")
    print(f"  Matrix:     [{batch_size}x{in_features}] x [{out_features}x{in_features}]")
    print()
    print(f"  æ¿€æ´»é‡åŒ–æ—¶é—´ (quantize_to_nvfp4_sim): {quant_time:.2f} ms")
    print(f"  å®Œæ•´ forward æ—¶é—´:                    {forward_time:.2f} ms")
    print(f"  é‡åŒ–å æ¯”:                             {quant_time/forward_time*100:.1f}%")
    print()

    if quant_time / forward_time > 0.7:
        print("  âš ï¸  åœ¨çº¿é‡åŒ–æ˜¯ä¸»è¦ç“¶é¢ˆï¼")
        print("     è§£å†³æ–¹æ¡ˆ:")
        print("     1. W4A16: åªé‡åŒ–æƒé‡ï¼Œæ¿€æ´»ä¿æŒ BF16")
        print("     2. W4A4:  å†™ CUDA kernel åšå¿«é€Ÿæ¿€æ´»é‡åŒ–")
    else:
        print("  âœ… åœ¨çº¿é‡åŒ–å¼€é”€å¯æ¥å—")


def main():
    print("=" * 70)
    print("NVFP4 ç²¾åº¦éªŒè¯æµ‹è¯•")
    print("=" * 70)
    print()

    try:
        import nvfp4_gemm
        print(f"CUTLASS extension: âœ… Loaded")
        print(f"  Available functions: {[f for f in dir(nvfp4_gemm) if not f.startswith('_')]}")
    except ImportError as e:
        print(f"CUTLASS extension: âŒ Not available ({e})")
        print("  Will use simulation mode only")

    print(f"Device: {torch.cuda.get_device_name(0)}")
    print()

    # è¿è¡Œæµ‹è¯•
    test1_passed = test_nvfp4_linear_precision()
    test2_passed = test_nvfp4_mlp_precision()
    test_timing_breakdown()

    print()
    print("=" * 70)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 70)
    print(f"  NVFP4Linear ç²¾åº¦: {'âœ… PASS' if test1_passed else 'âŒ FAIL'}")
    print(f"  NVFP4MLP ç²¾åº¦:    {'âœ… PASS' if test2_passed else 'âŒ FAIL'}")
    print()

    if test1_passed and test2_passed:
        print("  ğŸ‰ NVFP4 ç²¾åº¦éªŒè¯é€šè¿‡ï¼")
        print("     Scale Layout ä¿®å¤æˆåŠŸã€‚")
        print()
        print("  ä¸‹ä¸€æ­¥ï¼šä¼˜åŒ–åœ¨çº¿é‡åŒ–é€Ÿåº¦")
        print("     - é€‰é¡¹ A: W4A16 (åªé‡åŒ–æƒé‡ï¼Œæ¿€æ´»ä¿æŒ BF16)")
        print("     - é€‰é¡¹ B: å†™ CUDA kernel åšå¿«é€Ÿæ¿€æ´»é‡åŒ–")
    else:
        print("  âŒ NVFP4 ç²¾åº¦éªŒè¯å¤±è´¥ï¼Œéœ€è¦æ£€æŸ¥ Scale Layout é€»è¾‘")


if __name__ == "__main__":
    main()
