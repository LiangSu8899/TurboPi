#!/usr/bin/env python3
"""
Permutation Grid Search for CUTLASS Scale Layout

æš´åŠ›é€†å‘å·¥ç¨‹ CUTLASS çš„ Scale Layoutã€‚

æ ¸å¿ƒæ€è·¯:
- ä¸æ¨å¯¼ CuTe Layout å…¬å¼ï¼ˆå¤ªå¤æ‚ï¼‰
- è®© GPU "å‘Šè¯‰" æˆ‘ä»¬æ­£ç¡®çš„æ’åˆ—æ–¹å¼
- éå†æ‰€æœ‰å¯èƒ½çš„ reshape + permute ç»„åˆ
- æ‰¾åˆ°ä½¿ cosine similarity æœ€é«˜çš„ç»„åˆ

å…³é”®ç»´åº¦ (from sm100_blockscaled_layout.hpp):
- SfKMajorAtom = Layout<Shape<Shape<_32,_4>, Shape<_16,_4>>, Stride<...>>
- å…³é”®æ•°å­—: 32, 4, 16, 128
"""

import torch
import torch.nn.functional as F
import itertools
import sys
from typing import List, Tuple, Callable, Optional
from dataclasses import dataclass

sys.path.insert(0, '/home/heima-thor/suliang/Turbo-Pi/openpi/src')

from openpi.models_pytorch.nvfp4_mlp import (
    quantize_to_nvfp4_sim,
    dequantize_nvfp4_sim,
    pack_nvfp4_data,
    convert_scales_to_fp8,
    BLOCK_SIZE,
    CUTLASS_ROW_TILE,
    CUTLASS_K_TILE,
    CUTLASS_ROW_GROUP,
)


@dataclass
class PermResult:
    """æ’åˆ—ç»„åˆæœç´¢ç»“æœ"""
    name: str
    cos_sim: float
    cos_bf16: float
    success: bool
    error: str = ""


def generate_all_permutations() -> List[Tuple[str, Callable]]:
    """
    ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„ Scale é‡æ’å‡½æ•° - æš´åŠ›ç©·ä¸¾ç‰ˆã€‚

    å…³é”®ç»´åº¦: 32, 4, 16
    SfKMajorAtom = Shape<Shape<_32,_4>, Shape<_16,_4>>
    """
    permutations = []

    # ========================================================================
    # åŸºç¡€æ–¹æ³•
    # ========================================================================

    def v0_original(scales, M, K_blocks, M_padded, K_padded):
        """åŸå§‹: repeat_interleave + flatten"""
        s = torch.zeros(M_padded, K_padded, device=scales.device, dtype=scales.dtype)
        s[:M, :K_blocks] = scales
        return s.repeat_interleave(BLOCK_SIZE, dim=1).flatten()
    permutations.append(("v0_original", v0_original))

    def v0b_flatten_only(scales, M, K_blocks, M_padded, K_padded):
        """åª flattenï¼Œä¸æ‰©å±•"""
        s = torch.zeros(M_padded, K_padded, device=scales.device, dtype=scales.dtype)
        s[:M, :K_blocks] = scales
        return s.flatten()
    permutations.append(("v0b_flatten_only", v0b_flatten_only))

    def v0c_transpose(scales, M, K_blocks, M_padded, K_padded):
        """è½¬ç½®å flatten"""
        s = torch.zeros(M_padded, K_padded, device=scales.device, dtype=scales.dtype)
        s[:M, :K_blocks] = scales
        return s.T.contiguous().flatten()
    permutations.append(("v0c_transpose", v0c_transpose))

    # ========================================================================
    # (32, 4) ç³»åˆ— - SfKMajorAtom çš„å†…å±‚
    # ========================================================================

    # å¯¹äºæ¯ä¸ª (32, K) çš„ scale sliceï¼ŒæŒ‰ 4 ä¸ºå•ä½é‡æ’
    def gen_32_4_permute(dim_order):
        def fn(scales, M, K_blocks, M_padded, K_padded):
            s = torch.zeros(M_padded, K_padded, device=scales.device, dtype=scales.dtype)
            s[:M, :K_blocks] = scales

            # ç¡®ä¿ç»´åº¦å¯æ•´é™¤
            if M_padded % 32 != 0 or K_padded % 4 != 0:
                return s.flatten()

            num_m_groups = M_padded // 32
            num_k_groups = K_padded // 4

            # [num_m_groups, 32, num_k_groups, 4]
            s = s.view(num_m_groups, 32, num_k_groups, 4)
            s = s.permute(*dim_order).contiguous()
            return s.flatten()
        return fn

    # æ‰€æœ‰ 4D permute ç»„åˆ
    for perm in itertools.permutations([0, 1, 2, 3]):
        name = f"v1_32x4_perm{perm}"
        permutations.append((name, gen_32_4_permute(perm)))

    # ========================================================================
    # (32, 16) ç³»åˆ— - SfKMajorAtom çš„å¦ä¸€ç§è§£é‡Š
    # ========================================================================

    def gen_32_16_permute(dim_order):
        def fn(scales, M, K_blocks, M_padded, K_padded):
            s = torch.zeros(M_padded, K_padded, device=scales.device, dtype=scales.dtype)
            s[:M, :K_blocks] = scales

            if M_padded % 32 != 0 or K_padded % 16 != 0:
                # å¦‚æœä¸èƒ½æ•´é™¤ï¼Œpadding K
                K_padded_16 = ((K_padded + 15) // 16) * 16
                s_new = torch.zeros(M_padded, K_padded_16, device=scales.device, dtype=scales.dtype)
                s_new[:, :K_padded] = s
                s = s_new
                K_padded = K_padded_16

            num_m_groups = M_padded // 32
            num_k_groups = K_padded // 16

            # [num_m_groups, 32, num_k_groups, 16]
            s = s.view(num_m_groups, 32, num_k_groups, 16)
            s = s.permute(*dim_order).contiguous()
            return s.flatten()
        return fn

    for perm in itertools.permutations([0, 1, 2, 3]):
        name = f"v2_32x16_perm{perm}"
        permutations.append((name, gen_32_16_permute(perm)))

    # ========================================================================
    # (128, 4) ç³»åˆ— - CUTLASS row tile
    # ========================================================================

    def gen_128_4_permute(dim_order):
        def fn(scales, M, K_blocks, M_padded, K_padded):
            s = torch.zeros(M_padded, K_padded, device=scales.device, dtype=scales.dtype)
            s[:M, :K_blocks] = scales

            if M_padded % 128 != 0 or K_padded % 4 != 0:
                return s.flatten()

            num_m_tiles = M_padded // 128
            num_k_tiles = K_padded // 4

            # [num_m_tiles, 128, num_k_tiles, 4]
            s = s.view(num_m_tiles, 128, num_k_tiles, 4)
            s = s.permute(*dim_order).contiguous()
            return s.flatten()
        return fn

    for perm in itertools.permutations([0, 1, 2, 3]):
        name = f"v3_128x4_perm{perm}"
        permutations.append((name, gen_128_4_permute(perm)))

    # ========================================================================
    # åµŒå¥— Shape<Shape<32,4>, Shape<16,4>> - 6D é‡æ’
    # ========================================================================

    def gen_nested_32_4_16_4(outer_order, inner_order):
        """
        SfKMajorAtom = Shape<Shape<_32,_4>, Shape<_16,_4>>

        è§£é‡Š 1: (32, 4) æ˜¯ M æ–¹å‘çš„åµŒå¥—ï¼Œ(16, 4) æ˜¯ K æ–¹å‘çš„åµŒå¥—
        M -> (num_m32, 32) -> (num_m32, 8, 4)
        K -> (num_k4, 4)   -> ...

        è¿™é‡Œæˆ‘ä»¬å°è¯• 6D reshape + permute
        """
        def fn(scales, M, K_blocks, M_padded, K_padded):
            s = torch.zeros(M_padded, K_padded, device=scales.device, dtype=scales.dtype)
            s[:M, :K_blocks] = scales

            # éœ€è¦ M % 32 == 0 ä¸” K % 4 == 0
            if M_padded % 32 != 0 or K_padded % 4 != 0:
                return s.flatten()

            # è¿›ä¸€æ­¥åˆ†è§£: 32 = 8 * 4, ä½†è¿™å¯èƒ½ä¸æ˜¯æ­£ç¡®çš„åˆ†è§£
            # å…ˆå°è¯•: [num_m32, 32, num_k4, 4] -> å†…éƒ¨å†åˆ†
            num_m32 = M_padded // 32
            num_k4 = K_padded // 4

            # [num_m32, 32, num_k4, 4]
            s = s.view(num_m32, 32, num_k4, 4)
            # å…ˆåšå¤–å±‚ permute
            s = s.permute(*outer_order).contiguous()
            # ç„¶å flatten
            return s.flatten()
        return fn

    # åªé€‰æ‹©ä¸€äº›æœ‰ä»£è¡¨æ€§çš„ç»„åˆï¼ˆä¸ç„¶å¤ªå¤šäº†ï¼‰
    key_perms = [
        (0, 1, 2, 3),  # åŸå§‹
        (2, 0, 1, 3),  # K major
        (0, 2, 1, 3),  # äº¤é”™
        (2, 0, 3, 1),  # K major + å†…éƒ¨äº¤é”™
        (0, 2, 3, 1),  # å¦ä¸€ç§äº¤é”™
        (2, 3, 0, 1),  # å®Œå…¨åè½¬
    ]

    for outer in key_perms:
        name = f"v4_nested_outer{outer}"
        permutations.append((name, gen_nested_32_4_16_4(outer, None)))

    # ========================================================================
    # (32, 4, 16) 3D ç³»åˆ— - ç©·ä¸¾æ‰€æœ‰ 3D æ’åˆ—
    # ========================================================================

    def gen_3d_reshape_permute(shape, perm):
        """
        å°† [M, K] reshape æˆ 3D å permuteã€‚
        shape: (a, b, c) where M = num_a * a, K = num_bc * b * c
        """
        a, b, c = shape
        def fn(scales, M, K_blocks, M_padded, K_padded):
            s = torch.zeros(M_padded, K_padded, device=scales.device, dtype=scales.dtype)
            s[:M, :K_blocks] = scales

            # è®¡ç®—ç»´åº¦
            if M_padded % a != 0:
                return s.flatten()

            num_a = M_padded // a
            total_k = K_padded

            # å°è¯•å°† K reshape æˆ (b, c) æˆ– (c, b)
            if total_k % (b * c) == 0:
                num_bc = total_k // (b * c)
                # [num_a, a, num_bc, b, c]
                try:
                    s = s.view(num_a, a, num_bc, b, c)
                    s = s.permute(*perm).contiguous()
                except:
                    pass

            return s.flatten()
        return fn

    # æ ¸å¿ƒ 3D å½¢çŠ¶ (32, 4, 16) çš„å„ç§æ’åˆ—
    shapes_3d = [
        (32, 4, 16),  # SfKMajorAtom æç¤º
        (32, 16, 4),
        (128, 4, 4),  # row_tile x k_tile å˜ä½“
        (128, 1, 4),
        (32, 1, 4),
    ]

    # å¯¹äº 5D tensorï¼Œé€‰æ‹©ä¸€äº› key permutations
    key_5d_perms = [
        (0, 1, 2, 3, 4),  # åŸå§‹
        (2, 0, 1, 3, 4),  # K major
        (0, 2, 1, 3, 4),
        (2, 0, 3, 1, 4),
        (0, 2, 3, 4, 1),
        (2, 3, 0, 1, 4),
        (2, 3, 0, 4, 1),
        (4, 3, 2, 1, 0),  # å®Œå…¨åè½¬
    ]

    for shape in shapes_3d:
        for perm in key_5d_perms:
            name = f"v5_3d_{shape[0]}x{shape[1]}x{shape[2]}_perm{perm}"
            permutations.append((name, gen_3d_reshape_permute(shape, perm)))

    # ========================================================================
    # Stride æ¨¡æ‹Ÿç³»åˆ—
    # ========================================================================

    def gen_stride_pattern(stride_m, stride_k):
        """æ¨¡æ‹Ÿ CUTLASS stride æ¨¡å¼"""
        def fn(scales, M, K_blocks, M_padded, K_padded):
            s = torch.zeros(M_padded, K_padded, device=scales.device, dtype=scales.dtype)
            s[:M, :K_blocks] = scales

            # è®¡ç®—è¾“å‡ºå¤§å°
            out_size = M_padded * K_padded
            output = torch.zeros(out_size, device=scales.device, dtype=scales.dtype)

            # æŒ‰ stride å¡«å……
            for m in range(M_padded):
                for k in range(K_padded):
                    idx = (m * stride_m + k * stride_k) % out_size
                    output[idx] = s[m, k]

            return output
        return fn

    # SfKMajorAtom Stride çº¿ç´¢: Stride<Stride<_16, _4>, Stride<_64, _1>>
    stride_patterns = [
        (16, 4),    # Stride<_16, _4>
        (4, 16),
        (1, 64),    # Stride<_1, _64>
        (64, 1),
        (32, 4),    # åŸºäº 32 çš„å˜ä½“
        (4, 32),
        (128, 1),   # åŸºäº 128 çš„å˜ä½“
        (1, 128),
    ]

    for stride_m, stride_k in stride_patterns:
        name = f"v6_stride_m{stride_m}_k{stride_k}"
        permutations.append((name, gen_stride_pattern(stride_m, stride_k)))

    # ========================================================================
    # K-expansion å˜ä½“
    # ========================================================================

    def gen_k_expand_method(method):
        """ä¸åŒçš„ K æ‰©å±•æ–¹å¼"""
        def fn(scales, M, K_blocks, M_padded, K_padded):
            s = torch.zeros(M_padded, K_padded, device=scales.device, dtype=scales.dtype)
            s[:M, :K_blocks] = scales

            if method == "repeat_last":
                # repeat_interleave on dim=1
                s = s.repeat_interleave(BLOCK_SIZE, dim=1)
            elif method == "repeat_first":
                # å…ˆ transposeï¼Œrepeatï¼Œå† transpose
                s = s.T.repeat_interleave(BLOCK_SIZE, dim=1).T.contiguous()
            elif method == "tile":
                # tile instead of repeat
                s = s.unsqueeze(-1).expand(-1, -1, BLOCK_SIZE).reshape(M_padded, -1)
            elif method == "interleave":
                # äº¤é”™æ‰©å±•
                s = s.unsqueeze(-1).expand(-1, -1, BLOCK_SIZE)
                s = s.permute(0, 2, 1).contiguous().reshape(M_padded, -1)
            else:
                pass

            return s.flatten()
        return fn

    for method in ["repeat_last", "repeat_first", "tile", "interleave"]:
        name = f"v7_kexpand_{method}"
        permutations.append((name, gen_k_expand_method(method)))

    # ========================================================================
    # æŒ‰ block é‡æ’
    # ========================================================================

    def gen_block_reorder(block_m, block_k, perm_4d):
        """æŒ‰ block é‡æ’å permute"""
        def fn(scales, M, K_blocks, M_padded, K_padded):
            s = torch.zeros(M_padded, K_padded, device=scales.device, dtype=scales.dtype)
            s[:M, :K_blocks] = scales

            if M_padded % block_m != 0 or K_padded % block_k != 0:
                return s.flatten()

            num_m = M_padded // block_m
            num_k = K_padded // block_k

            # [num_m, block_m, num_k, block_k]
            s = s.view(num_m, block_m, num_k, block_k)
            s = s.permute(*perm_4d).contiguous()
            return s.flatten()
        return fn

    # å…³é”® block å¤§å°
    block_sizes = [
        (32, 4),
        (32, 16),
        (128, 4),
        (128, 16),
    ]

    # æ‰€æœ‰ 4D permute
    for bm, bk in block_sizes:
        for perm in itertools.permutations([0, 1, 2, 3]):
            name = f"v8_block_{bm}x{bk}_perm{perm}"
            permutations.append((name, gen_block_reorder(bm, bk, perm)))

    # ========================================================================
    # ç‰¹æ®Šç»„åˆ: å…ˆ expand Kï¼Œå† tile permute
    # ========================================================================

    def gen_expand_then_tile(tile_m, tile_k, perm_4d):
        """å…ˆæ‰©å±• K (Ã—32)ï¼Œå†æŒ‰ tile é‡æ’"""
        def fn(scales, M, K_blocks, M_padded, K_padded):
            s = torch.zeros(M_padded, K_padded, device=scales.device, dtype=scales.dtype)
            s[:M, :K_blocks] = scales

            # æ‰©å±• K
            s = s.repeat_interleave(BLOCK_SIZE, dim=1)  # [M, K]
            K_full = s.shape[1]

            if M_padded % tile_m != 0 or K_full % tile_k != 0:
                return s.flatten()

            num_m = M_padded // tile_m
            num_k = K_full // tile_k

            s = s.view(num_m, tile_m, num_k, tile_k)
            s = s.permute(*perm_4d).contiguous()
            return s.flatten()
        return fn

    tile_sizes_full = [
        (128, 128),  # full K block
        (128, 32),
        (32, 128),
        (32, 32),
    ]

    key_4d_perms = [
        (0, 1, 2, 3),
        (2, 0, 1, 3),
        (0, 2, 1, 3),
        (2, 0, 3, 1),
        (2, 3, 0, 1),
    ]

    for tm, tk in tile_sizes_full:
        for perm in key_4d_perms:
            name = f"v9_expand_tile_{tm}x{tk}_perm{perm}"
            permutations.append((name, gen_expand_then_tile(tm, tk, perm)))

    return permutations


def run_grid_search(verbose: bool = True):
    """è¿è¡Œ Permutation Grid Search"""
    print("=" * 70)
    print("CUTLASS Scale Layout - Permutation Grid Search (æš´åŠ›ç‰ˆ)")
    print("=" * 70)

    try:
        import nvfp4_gemm
        print("[OK] CUTLASS extension loaded")
    except ImportError:
        print("[ERROR] nvfp4_gemm not available")
        print("Please build the extension first")
        return None

    device = torch.device('cuda')
    print(f"Device: {torch.cuda.get_device_name(0)}\n")

    # æµ‹è¯•é…ç½® - ä½¿ç”¨å’Œå®é™…æ¨¡å‹æ¥è¿‘çš„å°ºå¯¸
    test_configs = [
        # (M, K, N) - å…ˆç”¨å°å°ºå¯¸å¿«é€Ÿæµ‹è¯•
        (256, 128, 256),   # å°å°ºå¯¸ï¼Œå¿«é€Ÿ
        (256, 2048, 256),  # ä¸­å°ºå¯¸
    ]

    all_results = []

    for M, K, N in test_configs:
        print(f"\n{'='*70}")
        print(f"Testing: M={M}, K={K}, N={N}")
        print(f"{'='*70}")

        block_size = BLOCK_SIZE
        num_k_blocks = K // block_size

        # Padding
        row_tile = CUTLASS_ROW_TILE
        k_tile = CUTLASS_K_TILE
        M_padded = ((M + row_tile - 1) // row_tile) * row_tile
        N_padded = ((N + row_tile - 1) // row_tile) * row_tile
        K_blocks_padded = ((num_k_blocks + k_tile - 1) // k_tile) * k_tile
        K_padded = K_blocks_padded

        print(f"Padded: M_pad={M_padded}, K_blocks_pad={K_blocks_padded}")

        # å‡†å¤‡æµ‹è¯•æ•°æ®
        torch.manual_seed(42)
        x = torch.randn(M, K, device=device, dtype=torch.float32)
        w = torch.randn(N, K, device=device, dtype=torch.float32)

        # é‡åŒ–
        x_q, x_scales = quantize_to_nvfp4_sim(x, block_size, use_mse_search=False)
        w_q, w_scales = quantize_to_nvfp4_sim(w, block_size, use_mse_search=False)

        x_packed = pack_nvfp4_data(x_q, block_size)
        w_packed = pack_nvfp4_data(w_q, block_size)

        # Python å‚è€ƒ (ä½¿ç”¨ FP8 scales)
        x_deq = dequantize_nvfp4_sim(x_q, x_scales, block_size, use_fp8_scales=True)
        w_deq = dequantize_nvfp4_sim(w_q, w_scales, block_size, use_fp8_scales=True)
        ref = torch.matmul(x_deq, w_deq.T)

        # BF16 å‚è€ƒ
        bf16_ref = torch.matmul(x, w.T)

        # è·å–æ‰€æœ‰ permutation å‡½æ•°
        permutations = generate_all_permutations()
        print(f"\nTesting {len(permutations)} permutations...")

        best_name = None
        best_cosine = -1
        results = []

        for i, (name, perm_func) in enumerate(permutations):
            try:
                # å‡†å¤‡ scales
                x_scales_perm = perm_func(x_scales.clone(), M, num_k_blocks, M_padded, K_blocks_padded)
                w_scales_perm = perm_func(w_scales.clone(), N, num_k_blocks, N_padded, K_blocks_padded)

                # è½¬æ¢ä¸º FP8
                x_scales_fp8 = convert_scales_to_fp8(x_scales_perm)
                w_scales_fp8 = convert_scales_to_fp8(w_scales_perm)

                # ç¡®ä¿å¤§å°æ­£ç¡® (CUTLASS æœŸæœ› M * K ä¸ª scales)
                expected_x = M_padded * K_blocks_padded * BLOCK_SIZE
                expected_w = N_padded * K_blocks_padded * BLOCK_SIZE

                if x_scales_fp8.numel() < expected_x:
                    pad = torch.zeros(expected_x - x_scales_fp8.numel(), dtype=torch.uint8, device=device)
                    x_scales_fp8 = torch.cat([x_scales_fp8, pad])
                elif x_scales_fp8.numel() > expected_x:
                    x_scales_fp8 = x_scales_fp8[:expected_x]

                if w_scales_fp8.numel() < expected_w:
                    pad = torch.zeros(expected_w - w_scales_fp8.numel(), dtype=torch.uint8, device=device)
                    w_scales_fp8 = torch.cat([w_scales_fp8, pad])
                elif w_scales_fp8.numel() > expected_w:
                    w_scales_fp8 = w_scales_fp8[:expected_w]

                # è¿è¡Œ CUTLASS
                output = nvfp4_gemm.gemm_prepared(
                    x_packed, w_packed,
                    x_scales_fp8, w_scales_fp8,
                    M, N, K
                )

                # è®¡ç®— cosine similarity
                cos_ref = F.cosine_similarity(
                    output.flatten().float().unsqueeze(0),
                    ref.flatten().unsqueeze(0)
                ).item()

                cos_bf16 = F.cosine_similarity(
                    output.flatten().float().unsqueeze(0),
                    bf16_ref.flatten().unsqueeze(0)
                ).item()

                results.append(PermResult(name, cos_ref, cos_bf16, True))

                if cos_ref > best_cosine:
                    best_cosine = cos_ref
                    best_name = name

                # åªæ‰“å°å¥½çš„ç»“æœ
                if verbose and cos_ref > 0.95:
                    print(f"  [{i+1}/{len(permutations)}] {name:45s}: {cos_ref:.6f} ***")
                elif verbose and i % 50 == 0:
                    print(f"  [{i+1}/{len(permutations)}] Progress... best so far: {best_cosine:.6f}")

            except Exception as e:
                results.append(PermResult(name, 0, 0, False, str(e)[:50]))
                if verbose and i % 100 == 0:
                    pass  # é™é»˜é”™è¯¯

        all_results.extend(results)

        # æ‰“å°è¿™ä¸ªé…ç½®çš„æœ€ä½³ç»“æœ
        print(f"\n--- Config M={M}, K={K}, N={N} ---")
        print(f"Best: {best_name} with cos_sim={best_cosine:.6f}")

        # Top 10
        sorted_results = sorted(results, key=lambda x: x.cos_sim, reverse=True)
        print("\nTop 10:")
        for r in sorted_results[:10]:
            if r.success:
                print(f"  {r.name:45s}: {r.cos_sim:.6f}")

    # å…¨å±€æœ€ä½³
    print("\n" + "=" * 70)
    print("GLOBAL RESULTS")
    print("=" * 70)

    sorted_all = sorted(all_results, key=lambda x: x.cos_sim, reverse=True)
    best = sorted_all[0]

    print(f"\nBest permutation: {best.name}")
    print(f"Best cosine sim:  {best.cos_sim:.6f}")

    if best.cos_sim > 0.99:
        print("\nğŸ‰ SUCCESS! Found permutation with >0.99 cosine similarity!")
    elif best.cos_sim > 0.98:
        print("\nâœ“ GOOD! Found permutation with >0.98 cosine similarity!")
    elif best.cos_sim > 0.95:
        print("\nâ†’ Progress. Found permutation with >0.95 cosine similarity.")
    else:
        print("\nâœ— No significant improvement. Issue may be:")
        print("  1. SfAtom internal stride pattern more complex")
        print("  2. Data packing format mismatch")
        print("  3. Need more permutation combinations")

    return sorted_all


def quick_test_permutation(perm_name: str):
    """å¿«é€Ÿæµ‹è¯•æŒ‡å®šçš„ permutation"""
    print(f"Quick test for: {perm_name}")

    permutations = generate_all_permutations()
    perm_dict = dict(permutations)

    if perm_name not in perm_dict:
        print(f"Permutation '{perm_name}' not found")
        print(f"Available: {list(perm_dict.keys())[:10]}...")
        return

    perm_func = perm_dict[perm_name]

    try:
        import nvfp4_gemm
    except ImportError:
        print("nvfp4_gemm not available")
        return

    device = torch.device('cuda')
    M, K, N = 256, 2048, 256
    block_size = BLOCK_SIZE
    num_k_blocks = K // block_size

    M_padded = ((M + CUTLASS_ROW_TILE - 1) // CUTLASS_ROW_TILE) * CUTLASS_ROW_TILE
    N_padded = ((N + CUTLASS_ROW_TILE - 1) // CUTLASS_ROW_TILE) * CUTLASS_ROW_TILE
    K_blocks_padded = ((num_k_blocks + CUTLASS_K_TILE - 1) // CUTLASS_K_TILE) * CUTLASS_K_TILE

    torch.manual_seed(42)
    x = torch.randn(M, K, device=device, dtype=torch.float32)
    w = torch.randn(N, K, device=device, dtype=torch.float32)

    x_q, x_scales = quantize_to_nvfp4_sim(x, block_size, use_mse_search=False)
    w_q, w_scales = quantize_to_nvfp4_sim(w, block_size, use_mse_search=False)

    x_packed = pack_nvfp4_data(x_q, block_size)
    w_packed = pack_nvfp4_data(w_q, block_size)

    x_deq = dequantize_nvfp4_sim(x_q, x_scales, block_size, use_fp8_scales=True)
    w_deq = dequantize_nvfp4_sim(w_q, w_scales, block_size, use_fp8_scales=True)
    ref = torch.matmul(x_deq, w_deq.T)

    # Apply permutation
    x_scales_perm = perm_func(x_scales.clone(), M, num_k_blocks, M_padded, K_blocks_padded)
    w_scales_perm = perm_func(w_scales.clone(), N, num_k_blocks, N_padded, K_blocks_padded)

    x_scales_fp8 = convert_scales_to_fp8(x_scales_perm)
    w_scales_fp8 = convert_scales_to_fp8(w_scales_perm)

    # Padding
    expected_x = M_padded * K_blocks_padded * BLOCK_SIZE
    expected_w = N_padded * K_blocks_padded * BLOCK_SIZE

    if x_scales_fp8.numel() < expected_x:
        x_scales_fp8 = torch.cat([x_scales_fp8, torch.zeros(expected_x - x_scales_fp8.numel(), dtype=torch.uint8, device=device)])
    if w_scales_fp8.numel() < expected_w:
        w_scales_fp8 = torch.cat([w_scales_fp8, torch.zeros(expected_w - w_scales_fp8.numel(), dtype=torch.uint8, device=device)])

    output = nvfp4_gemm.gemm_prepared(
        x_packed, w_packed,
        x_scales_fp8[:expected_x], w_scales_fp8[:expected_w],
        M, N, K
    )

    cos_sim = F.cosine_similarity(
        output.flatten().float().unsqueeze(0),
        ref.flatten().unsqueeze(0)
    ).item()

    print(f"Cosine similarity: {cos_sim:.6f}")


if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1:
        # æµ‹è¯•ç‰¹å®š permutation
        quick_test_permutation(sys.argv[1])
    else:
        # å®Œæ•´ grid search
        run_grid_search()
