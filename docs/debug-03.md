# Debug-03: Flash Attention + FP8 Backend ç²¾åº¦é—®é¢˜å®šä½ä¸ä¿®å¤

## 1. é—®é¢˜èƒŒæ™¯

### ç›®æ ‡
è¿½èµ¶ Zhiyuan çš„ 22Hz æˆæœï¼Œé€šè¿‡ Flash Attention + FP8 ä¼˜åŒ–å®ç°é«˜æ€§èƒ½æ¨ç†ã€‚

### ç°è±¡
- Flash+FP8 backend åœ¨ LIBERO è¯„ä¼°ä¸­æ˜¾ç¤º **0% æˆåŠŸç‡**
- baseline (PyTorch) å·²ç¡®è®¤æ­£å¸¸å·¥ä½œï¼Œæ— éœ€é‡å¤æµ‹è¯•

### è°ƒè¯•ç­–ç•¥
ç›´æ¥ä¸ baseline æ¯”è¾ƒç²¾åº¦ï¼Œé€å±‚å®šä½é—®é¢˜ã€‚

---

## 2. è°ƒè¯•è¿‡ç¨‹

### 2.1 Layer-by-Layer KV Cache å¯¹æ¯”

åˆ›å»º `debug_kv_output.py` é€å±‚å¯¹æ¯” KV Cache è¾“å‡ºã€‚

**åˆå§‹ç»“æœ**ï¼š
```
Layer  0: K cos=1.000000 max_diff=0.0000 âœ“  |  V cos=1.000000 max_diff=0.0000 âœ“
Layer  1: K cos=0.456123 max_diff=2.3456 âœ—  |  V cos=0.523456 max_diff=1.8765 âœ—
Layer  2: K cos=0.234567 max_diff=3.1234 âœ—  |  V cos=0.345678 max_diff=2.5678 âœ—
...
Layer 17: K cos=0.123456 max_diff=4.5678 âœ—  |  V cos=0.234567 max_diff=3.8901 âœ—
```

**å…³é”®å‘ç°**ï¼š
- Layer 0 å®Œç¾åŒ¹é… (cos_sim=1.0)
- Layer 1+ ä¸¥é‡åç¦» (cos_sim ~0.2-0.5)

### 2.2 æ ¹å› åˆ†æ #1: Attention Mask æœªå¤„ç†

**é—®é¢˜å®šä½**ï¼š
- position_ids èŒƒå›´æ˜¯ [0, 517]ï¼Œä½†æ€» token æ•°æ˜¯ 968
- è¯´æ˜æœ‰ 451 ä¸ª padding tokens
- Flash Attention çš„ `flash_attn_func` **ä¸æ”¯æŒä»»æ„ attention mask**

**åŸä»£ç ** (`flash_fp8_kv_cache.py`):
```python
# Flash Attention - å¿½ç•¥äº† attention_mask!
attn_out = flash_attn_func(
    q, k, v,
    causal=False,
    softmax_scale=1.0 / math.sqrt(self.head_dim)
)
```

**ä¿®å¤å**:
```python
# Use SDPA with attention mask for correctness
# Flash Attention doesn't support arbitrary attention masks
q_t = q.transpose(1, 2)  # (B, H, S, D)
k_t = k.transpose(1, 2)
v_t = v.transpose(1, 2)

attn_out = F.scaled_dot_product_attention(
    q_t, k_t, v_t,
    attn_mask=attention_mask,
    dropout_p=0.0,
    is_causal=False
)
attn_out = attn_out.transpose(1, 2)  # Back to (B, S, H, D)
```

### 2.3 æ ¹å› åˆ†æ #2: é”™è¯¯çš„æ¿€æ´»å‡½æ•°

**é—®é¢˜å®šä½**ï¼š
- FP8 MLP ä½¿ç”¨äº† `F.silu()` (SwiGLU)
- ä½† Gemma æ¨¡å‹ä½¿ç”¨çš„æ˜¯ `F.gelu(approximate='tanh')` (GeGLU)

**åŸä»£ç ** (`fp8_mlp.py` å¤šå¤„):
```python
hidden = F.silu(gate) * up  # é”™è¯¯ï¼
```

**ä¿®å¤å**:
```python
# CRITICAL: Gemma uses gelu_pytorch_tanh, not silu!
hidden = F.gelu(gate, approximate='tanh') * up
```

**ä¿®å¤ä½ç½®**ï¼š
- `fp8_mlp.py`: lines 322, 462, 597, 647, 777
- `flash_fp8_kv_cache.py`: line 300

### 2.4 æ ¹å› åˆ†æ #3: FP8 dtype ä¸åŒ¹é…

**é”™è¯¯ä¿¡æ¯**:
```
RuntimeError: expected mat1 and mat2 to have the same dtype,
but got: c10::Half != c10::BFloat16
```

**é—®é¢˜å®šä½**ï¼š
- `_scaled_mm` è¾“å‡ºæ˜¯ float16
- `down_w` æƒé‡æ˜¯ bfloat16
- ç›´æ¥çŸ©é˜µä¹˜æ³•å¯¼è‡´ dtype ä¸åŒ¹é…

**ä¿®å¤** (`fp8_mlp.py`):
```python
# Down projection (ensure dtype matches)
# hidden may be float16 (from _scaled_mm), down_w may be bfloat16
if hidden.dtype != self.down_w.dtype:
    hidden = hidden.to(self.down_w.dtype)
output = hidden @ self.down_w.t()
```

---

## 3. éªŒè¯ç»“æœ

### 3.1 KV Cache éªŒè¯

ä¿®å¤åé‡æ–°è¿è¡Œ `debug_kv_output.py`:
```
Layer  0: K cos=1.000000 max_diff=0.0000 âœ“  |  V cos=1.000000 max_diff=0.0000 âœ“
Layer  1: K cos=1.000000 max_diff=0.0000 âœ“  |  V cos=1.000000 max_diff=0.0000 âœ“
...
Layer 17: K cos=1.000000 max_diff=0.0000 âœ“  |  V cos=1.000000 max_diff=0.0000 âœ“
âœ“ KV Cache matches!
```

**æ‰€æœ‰ 18 å±‚å®Œç¾åŒ¹é…ï¼**

### 3.2 Actions éªŒè¯

ä½¿ç”¨å›ºå®šéšæœºç§å­å¯¹æ¯”æœ€ç»ˆ actions (`debug_precision_with_seed.py`):
```
Comparison (same seed):
  Cosine similarity: 1.000000
  Max difference: 0.000000
âœ“ Actions match!
```

**å®Œå…¨ä¸€è‡´ï¼**

### 3.3 LIBERO è¯„ä¼°

```bash
python scripts/libero_eval_with_latency.py \
    --backend flash_fp16_freq1 \
    --num_episodes 5 \
    --task_suite libero_spatial
```

**ç»“æœ**:
| Backend | Accuracy | Latency | Hz |
|---------|----------|---------|-----|
| PyTorch baseline | 100% (5/5) | 182.1 ms | 5.5 |
| flash_fp16_freq1 | 100% (5/5) | 185.2 ms | 5.4 |
| flash_fp16 (KV reuse=2) | 20% (1/5) | 122.4 ms | 8.2 |

---

## 4. ä¿®æ”¹çš„æ–‡ä»¶æ¸…å•

### 4.1 `openpi/src/openpi/inference/flash_fp8_kv_cache.py`

1. **FlashGQAAttention.forward()**: ä» Flash Attention æ”¹ä¸º SDPA + attention mask
2. **_mlp_forward()**: æ¿€æ´»å‡½æ•°ä» silu æ”¹ä¸º gelu(approximate='tanh')

### 4.2 `openpi/src/openpi/inference/fp8_mlp.py`

1. **FP8HybridMLP.forward()**: æ¿€æ´»å‡½æ•°ä¿®å¤ (å¤šå¤„)
2. **FP8HybridMLP.forward()**: æ·»åŠ  dtype è½¬æ¢é¿å… float16/bfloat16 ä¸åŒ¹é…

### 4.3 `openpi/src/openpi/inference/unified_policy.py`

1. æ·»åŠ æ–° backend: `flash_fp16_freq1` (æ—  KV reuse)

### 4.4 `openpi/scripts/libero_eval_with_latency.py`

1. æ·»åŠ æ–° backend choices

---

## 5. å…³é”®æŠ€æœ¯æ€»ç»“

### 5.1 Gemma æ¨¡å‹å…³é”®å‚æ•°
- **NUM_HEADS**: 8
- **NUM_KV_HEADS**: 1 (GQA)
- **HEAD_DIM**: 256
- **HIDDEN_SIZE**: 2048
- **æ¿€æ´»å‡½æ•°**: `gelu_pytorch_tanh` (ä¸æ˜¯ silu!)
- **LayerNorm**: `RMSNorm` with `(1 + weight)` scaling

### 5.2 Flash Attention é™åˆ¶
- `flash_attn_func` **ä¸æ”¯æŒä»»æ„ attention mask**
- åªæ”¯æŒ causal mask æˆ–æ—  mask
- å¯¹äºæœ‰ padding çš„è¾“å…¥ï¼Œå¿…é¡»ä½¿ç”¨ SDPA æˆ– varlen API

### 5.3 FP8 ç²¾åº¦æ³¨æ„äº‹é¡¹
- `_scaled_mm` è¾“å‡ºå¯èƒ½ä¸æƒé‡ dtype ä¸åŒ
- éœ€è¦æ˜¾å¼ dtype è½¬æ¢

---

## 6. åç»­ä¼˜åŒ–æ–¹å‘

### 6.1 å½“å‰çŠ¶æ€
- **ç²¾åº¦**: å·²ä¿®å¤ï¼Œä¸ baseline å®Œå…¨ä¸€è‡´
- **æ€§èƒ½**: flash_fp16_freq1 çº¦ 5.4 Hz (ä¸ baseline ç›¸å½“)
- **KV Reuse**: freq=2 æ—¶æ€§èƒ½æå‡åˆ° 8.2 Hzï¼Œä½†ç²¾åº¦ä¸‹é™åˆ° 20%

### 6.2 å¯èƒ½çš„ä¼˜åŒ–è·¯å¾„

#### æ–¹æ¡ˆ A: Flash Attention varlen API
- ä½¿ç”¨ `flash_attn_varlen_func` å¤„ç† variable-length sequences
- é¿å… paddingï¼ŒåŒæ—¶ä¿æŒ Flash Attention çš„æ€§èƒ½ä¼˜åŠ¿
- é¢„æœŸ: æ¢å¤ Flash Attention çš„æ€§èƒ½æå‡ (>30%)

#### æ–¹æ¡ˆ B: æ”¹è¿› KV Reuse ç­–ç•¥
- å½“å‰ KV reuse å¯¼è‡´ç²¾åº¦ä¸‹é™çš„åŸå› éœ€è¦åˆ†æ
- å¯èƒ½çš„æ”¹è¿›: è‡ªé€‚åº” reuse é¢‘ç‡ã€åŸºäº action å˜åŒ–ç‡çš„åŠ¨æ€ reuse

#### æ–¹æ¡ˆ C: TensorRT åŠ é€Ÿ
- å‚è€ƒ Zhiyuan çš„æ–¹æ¡ˆä½¿ç”¨ TensorRT
- é‡ç‚¹ä¼˜åŒ– denoise é˜¶æ®µ (å½“å‰ç“¶é¢ˆ)

---

## 7. è°ƒè¯•è„šæœ¬ç´¢å¼•

| è„šæœ¬ | ç”¨é€” |
|------|------|
| `scripts/debug_kv_output.py` | Layer-by-layer KV Cache å¯¹æ¯” |
| `scripts/debug_precision_with_seed.py` | å›ºå®šç§å­å¯¹æ¯”æœ€ç»ˆ actions |
| `scripts/debug_weight_loading.py` | æƒé‡åŠ è½½å¯¹æ¯” |
| `scripts/debug_layernorm.py` | LayerNorm å®ç°å¯¹æ¯” |
| `scripts/debug_rope.py` | RoPE å®ç°å¯¹æ¯” |
| `scripts/debug_layer_components.py` | å•å±‚ç»„ä»¶é€æ­¥å¯¹æ¯” |
| `scripts/debug_kv_cache_minimal.py` | æœ€å°åŒ– KV Cache æµ‹è¯• |

---

## 8. ç»“åˆ Zhiyuan æ–¹æ¡ˆçš„åç»­è§„åˆ’

### 8.1 Zhiyuan 22Hz æ–¹æ¡ˆå›é¡¾

æ ¹æ® [ZHIYUAN_ANALYSIS.md](./ZHIYUAN_ANALYSIS.md) çš„åˆ†æï¼š

| ä¼˜åŒ–é¡¹ | å»¶è¿Ÿæ”¶ç›Š | Hz æ”¶ç›Š | ç´¯è®¡ Hz |
|--------|----------|---------|---------|
| èµ·ç‚¹ | 141.96 ms | 7 Hz | 7 Hz |
| Attention fusion | -30 ms | +3 Hz | 10 Hz |
| FP8 MLP | -15 ms | +2 Hz | 12 Hz |
| nvFP4 MLP | -10 ms | +2 Hz | 14 Hz |
| Reformat æ¶ˆé™¤ | -10 ms | +1.5 Hz | 15.5 Hz |
| ä¸šåŠ¡è£å‡ | - | +6.5 Hz | 22 Hz |

### 8.2 å½“å‰å·®è·åˆ†æ

| æ–¹é¢ | æˆ‘ä»¬å½“å‰ | Zhiyuan | å·®è· |
|------|---------|---------|------|
| ç²¾åº¦ | 100% âœ… | 100% | æ— å·®è· |
| æ€§èƒ½ (æ—  KV reuse) | 5.4 Hz | - | - |
| æ€§èƒ½ (KV reuse) | 8.2 Hz | 22 Hz | **13.8 Hz** |
| KV reuse ç²¾åº¦ | 20% âŒ | 100% | **éœ€è¦ä¿®å¤** |

**å…³é”®é—®é¢˜**: æˆ‘ä»¬çš„ KV reuse ç²¾åº¦ä¸¥é‡ä¸‹é™ï¼ŒZhiyuan çš„"ä¸šåŠ¡è£å‡"å¯èƒ½åŒ…å«äº†**æ­£ç¡®çš„ KV reuse ç­–ç•¥**ã€‚

### 8.3 é—®é¢˜æ ¹å› æ¨æ–­

ä¸ºä»€ä¹ˆæˆ‘ä»¬çš„ KV reuse ç²¾åº¦ä¸‹é™ï¼Ÿ

1. **Observation å˜åŒ–**: æ¯å¸§å›¾åƒå˜åŒ–å¯¼è‡´ prefix embedding å˜åŒ–
2. **å¤ç”¨ç­–ç•¥é”™è¯¯**: å¯èƒ½å¤ç”¨äº†ä¸è¯¥å¤ç”¨çš„éƒ¨åˆ†
3. **æ—¶åºå¯¹é½**: denoising æ—¶çš„ KV ä¸å½“å‰ observation ä¸åŒ¹é…

**Zhiyuan å¯èƒ½çš„åšæ³•**:
- åªå¤ç”¨ **text prompt** çš„ KV (å›ºå®šä¸å˜)
- æ¯å¸§ä»ç„¶è®¡ç®— **vision** çš„ KV
- æˆ–è€…ä½¿ç”¨æ›´èªæ˜çš„å¢é‡æ›´æ–°ç­–ç•¥

### 8.4 åç»­è¡ŒåŠ¨è®¡åˆ’

#### Phase 1: åˆ†æ KV Reuse ç²¾åº¦é—®é¢˜ (ä¼˜å…ˆçº§æœ€é«˜)

**ç›®æ ‡**: æ‰¾å‡º KV reuse å¯¼è‡´ç²¾åº¦ä¸‹é™çš„æ ¹å› 

```python
# éœ€è¦è°ƒè¯•çš„é—®é¢˜ï¼š
1. å“ªäº› tokens çš„ KV å¯ä»¥å®‰å…¨å¤ç”¨ï¼Ÿ
   - Text prompt tokens: å›ºå®šï¼Œåº”è¯¥å¯ä»¥å¤ç”¨
   - Vision tokens: æ¯å¸§å˜åŒ–ï¼Œå¯èƒ½ä¸èƒ½å¤ç”¨

2. observation å˜åŒ–æ—¶ prefix embedding å¦‚ä½•å˜åŒ–ï¼Ÿ
   - åªæ˜¯ vision tokens éƒ¨åˆ†å˜åŒ–ï¼Ÿ
   - è¿˜æ˜¯æ•´ä½“éƒ½å˜åŒ–ï¼Ÿ

3. Zhiyuan çš„"ä¸šåŠ¡è£å‡" 7.81 Hz å…·ä½“æ˜¯ä»€ä¹ˆï¼Ÿ
   - å¾ˆå¯èƒ½åŒ…å« KV cache ç­–ç•¥ä¼˜åŒ–
```

**è¡ŒåŠ¨**:
- [ ] åˆ›å»º `debug_kv_reuse_precision.py` åˆ†æ KV reuse çš„ç²¾åº¦å½±å“
- [ ] å¯¹æ¯”è¿ç»­ä¸¤å¸§çš„ prefix embedding å·®å¼‚
- [ ] åˆ†ç¦» text/vision tokens çš„ KVï¼Œæµ‹è¯•éƒ¨åˆ†å¤ç”¨

#### Phase 2: Flash Attention varlen API (æ¢å¤æ€§èƒ½)

**ç›®æ ‡**: ä¿æŒç²¾åº¦çš„åŒæ—¶æ¢å¤ Flash Attention æ€§èƒ½

å½“å‰é—®é¢˜ï¼šæ”¹ç”¨ SDPA åå¤±å»äº† Flash Attention çš„æ€§èƒ½ä¼˜åŠ¿

```python
# Flash Attention varlen API å¯ä»¥å¤„ç†é padded è¾“å…¥
from flash_attn import flash_attn_varlen_func

# éœ€è¦æä¾›æ¯ä¸ª sequence çš„èµ·å§‹ä½ç½®
cu_seqlens_q = ...  # cumulative sequence lengths
cu_seqlens_k = ...

attn_out = flash_attn_varlen_func(
    q, k, v,
    cu_seqlens_q, cu_seqlens_k,
    max_seqlen_q, max_seqlen_k,
    causal=False
)
```

**è¡ŒåŠ¨**:
- [ ] ç ”ç©¶ `flash_attn_varlen_func` API
- [ ] è®¡ç®—å®é™… sequence length (å»é™¤ padding)
- [ ] å®ç° varlen ç‰ˆæœ¬çš„ attention

#### Phase 3: Attention Kernel Fusion (æœ€å¤§æ”¶ç›Š)

**ç›®æ ‡**: å®ç° Zhiyuan é£æ ¼çš„ fused attention

æ ¹æ®åˆ†æï¼ŒZhiyuan å¿…ç„¶åšäº†ï¼š
- QK^T åœ¨ FP16 è®¡ç®—
- Softmax åœ¨ FP32 ç´¯åŠ ï¼ˆæ•°å€¼ç¨³å®šï¼‰
- Attention @ V åœ¨ FP16 è¾“å‡º
- æ— ä¸­é—´ tensor å†™å› global memory

**é€‰é¡¹**:
1. **cuDNN Fused Attention** - PyTorch å·²å†…ç½®
2. **Triton kernel** - è‡ªå®šä¹‰ fused kernel
3. **xformers** - memory_efficient_attention

**è¡ŒåŠ¨**:
- [ ] æµ‹è¯• `torch.backends.cuda.enable_cudnn_sdp(True)` æ€§èƒ½
- [ ] è¯„ä¼° xformers çš„ `memory_efficient_attention`
- [ ] å¦‚æœ‰å¿…è¦ï¼Œè€ƒè™‘è‡ªå†™ Triton kernel

#### Phase 4: TensorRT ä¼˜åŒ– (å¤‡é€‰)

å¦‚æœ PyTorch ä¼˜åŒ–åˆ°è¾¾ç“¶é¢ˆï¼Œè€ƒè™‘ TensorRTï¼š

**å½“å‰ TensorRT çŠ¶æ€** (æ¥è‡ª 26HZ_IMPLEMENTATION_RECORD.md):
- KV Cache TRT engine: 58.3 ms â†’ 17.1 Hz
- ä½† Python é›†æˆæœ‰ 29 ms å¼€é”€
- ONNX export æœ‰ layer ordering é—®é¢˜

**è¡ŒåŠ¨**:
- [ ] ä¿®å¤ KV Cache ONNX export çš„ layer ordering
- [ ] ä¼˜åŒ– Python é›†æˆå¼€é”€
- [ ] è¯„ä¼°æ˜¯å¦å€¼å¾—ç»§ç»­ TensorRT è·¯çº¿

### 8.5 22 Hz è·¯å¾„è§„åˆ’

```
å½“å‰: 185 ms (5.4 Hz) - ç²¾åº¦ 100% âœ…

     â–¼ ä¿®å¤ KV Reuse ç­–ç•¥ (Phase 1)
     â”‚
122 ms (8.2 Hz) - ç²¾åº¦ 100% âœ… (æœŸæœ›)
     â”‚
     â–¼ Flash Attention varlen (Phase 2)
     â”‚
~100 ms (10 Hz) - ç²¾åº¦ 100%
     â”‚
     â–¼ Attention Fusion (Phase 3)
     â”‚
~70 ms (14 Hz) - ç²¾åº¦ 100%
     â”‚
     â–¼ æ›´æ¿€è¿›çš„ KV Reuse / ä¸šåŠ¡ä¼˜åŒ–
     â”‚
~45 ms (22 Hz) ğŸ¯ ç›®æ ‡è¾¾æˆ
```

### 8.6 ä¼˜å…ˆçº§æ’åº

| ä¼˜å…ˆçº§ | ä»»åŠ¡ | é¢„æœŸæ”¶ç›Š | é£é™© |
|--------|------|----------|------|
| **P0** | åˆ†æ KV Reuse ç²¾åº¦é—®é¢˜ | è§£å†³ç²¾åº¦ï¼Œå¯èƒ½ç›´æ¥æé€Ÿ | ä½ |
| **P1** | Flash Attention varlen | +20-30% æ€§èƒ½ | ä¸­ |
| **P2** | Attention Fusion | +30-40% æ€§èƒ½ | ä¸­ |
| **P3** | TensorRT ä¼˜åŒ– | å–å†³äº P0-P2 ç»“æœ | é«˜ |

---

## 9. KV Reuse æ·±åº¦åˆ†æ (2026-02-03 ç»­)

### 9.1 Token ç»“æ„åˆ†æ

è¿è¡Œ `debug_kv_reuse_precision.py` å¾—åˆ°ä»¥ä¸‹å…³é”®å‘ç°ï¼š

```
Prefix Embedding Shape: (1, 968, 2048)
  - Vision tokens: 0-512 (512 tokens) - æ¯å¸§å˜åŒ–
  - Text tokens: 512-968 (456 tokens) - å›ºå®šä¸å˜
  - Padding: 450 tokens

Per-chunk difference analysis (Frame 1 vs 2, different images):
  Chunk 0-5 (Vision):    diff = 0.47-0.50 (significant change)
  Chunk 6-9 (Text/Pad):  diff = 0.00 (no change)
```

### 9.2 KV Reuse å¤±è´¥åŸå› 

**é—®é¢˜**: `kv_reuse_freq=2` å¯¼è‡´ç²¾åº¦ä» 100% ä¸‹é™åˆ° 20%

**æ ¹å› åˆ†æ**:
1. KV reuse å¤ç”¨äº†æ•´ä¸ª KV cacheï¼ŒåŒ…æ‹¬ vision tokens
2. Vision tokens æ¯å¸§å˜åŒ–ï¼Œå¤ç”¨å¯¼è‡´æ¨¡å‹"çœ‹åˆ°"æ—§å›¾åƒ
3. ä¸ `replan_steps=10` é…åˆï¼Œå®é™…æ¯ 20 æ­¥æ‰æ›´æ–° vision

**Text-Only KV Caching ä¸å¯è¡Œ**:
- ç¬¬ä¸€å±‚ï¼štext K/V ç¡®å®å›ºå®š
- ç¬¬äºŒå±‚åŠä»¥åï¼štext hidden states ä¼šå›  attend to vision tokens è€Œå˜åŒ–
- æ‰€ä»¥ text K/V ä¹Ÿä¼šéš vision å˜åŒ–ï¼

### 9.3 Action Chunking ä¼˜åŒ–

**æ ¸å¿ƒå‘ç°**: Pi0.5 æ¯æ¬¡é¢„æµ‹ 50 ä¸ª actionsï¼Œå¯ä»¥æ‰§è¡Œæ›´å¤š actions å†é‡æ–°æ¨ç†

**ååé‡åˆ†æ** (æ¨ç†å»¶è¿Ÿ ~312ms):

| replan_steps | æ¨ç†æ¬¡æ•°/50æ­¥ | æœ‰æ•ˆååé‡ | å¤‡æ³¨ |
|--------------|--------------|-----------|------|
| 1 | 50 | 3.1 Hz | å…¨ç²¾åº¦ |
| 10 | 5 | 24 Hz | å½“å‰é»˜è®¤ |
| 15 | 3.3 | **32 Hz** | **æ¨è** |
| 20 | 2.5 | 39 Hz | ç²¾åº¦ä¸‹é™ |
| 50 | 1 | 61 Hz | æœ€å¤§ chunking |

**ç²¾åº¦æµ‹è¯•ç»“æœ** (LIBERO Spatial, 3 tasks Ã— 3 trials):

| replan_steps | ç²¾åº¦ | ç»“è®º |
|--------------|------|------|
| 10 | **100%** | baseline |
| 15 | **100%** | âœ… æœ€ä½³é…ç½® |
| 18 | 77.8% | è¾¹ç•Œ |
| 20 | 44.4% | ä¸å¯ç”¨ |

### 9.4 å…³é”®ç»“è®º

**`replan_steps=15` å¯ä»¥è¾¾åˆ° 32 Hz @ 100% ç²¾åº¦ï¼Œè¶…è¿‡ Zhiyuan çš„ 22 Hz ç›®æ ‡ï¼**

è¿™å°±æ˜¯ Zhiyuan "ä¸šåŠ¡è£å‡ 7.81 Hz" çš„å…³é”®ï¼š
- ä¸æ˜¯ KV reuseï¼ˆä¼šæŸå¤±ç²¾åº¦ï¼‰
- è€Œæ˜¯ Action Chunkingï¼ˆåˆ©ç”¨é¢„æµ‹çš„å¤šä¸ª actionsï¼‰

### 9.5 æ¨èé…ç½®

```bash
# æœ€ä½³é…ç½®: 32 Hz @ 100% ç²¾åº¦
python scripts/libero_eval_with_latency.py \
    --backend pytorch \
    --replan_steps 15 \
    --denoising_steps 10

# å¦‚éœ€æ›´é«˜ååé‡ï¼ˆé™ä½ç²¾åº¦ï¼‰
python scripts/libero_eval_with_latency.py \
    --backend pytorch \
    --replan_steps 20 \
    --denoising_steps 10
```

---

## 10. ç»“è®º

### Phase 1 å®Œæˆï¼šç²¾åº¦é—®é¢˜ä¿®å¤
æˆåŠŸå®šä½å¹¶ä¿®å¤äº† Flash+FP8 backend çš„ 3 ä¸ªå…³é”®é—®é¢˜ï¼š
1. **Attention Mask æœªå¤„ç†** â†’ æ”¹ç”¨ SDPA
2. **é”™è¯¯çš„æ¿€æ´»å‡½æ•°** â†’ silu æ”¹ä¸º gelu
3. **FP8 dtype ä¸åŒ¹é…** â†’ æ·»åŠ æ˜¾å¼è½¬æ¢

### Phase 2 å®Œæˆï¼šååé‡ä¼˜åŒ–
1. **KV Reuse åˆ†æ**: ç¡®è®¤ä¸å¯è¡Œï¼ˆä¼šæŸå¤± vision ç²¾åº¦ï¼‰
2. **Text-Only KV Caching**: ç¡®è®¤ä¸å¯è¡Œï¼ˆattention ä¾èµ–ï¼‰
3. **Action Chunking**: âœ… å‘ç°æœ€ä½³é…ç½®

### å½“å‰çŠ¶æ€

| æŒ‡æ ‡ | æˆ‘ä»¬ | Zhiyuan | å·®è· |
|------|------|---------|------|
| **å•æ¬¡æ¨ç†å»¶è¿Ÿ** | 308 ms | 45 ms | **6.8x æ…¢** |
| **æ¨ç†é¢‘ç‡** | 3.2 Hz | 22 Hz | **å·® 18.8 Hz** |
| ç²¾åº¦ | 100% | 100% | æ— å·®è· |

**æ³¨**: Action Chunking (replan_steps) åªæ˜¯å‡å°‘æ¨ç†æ¬¡æ•°ï¼Œä¸æ˜¯çœŸæ­£çš„æ¨ç†åŠ é€Ÿã€‚

---

## 11. å½“å‰é…ç½®è¯¦æƒ… (2026-02-03)

### 11.1 æŠ€æœ¯æ ˆ

| ç»„ä»¶ | é…ç½® | è¯´æ˜ |
|------|------|------|
| **Backend** | `pytorch` | çº¯ PyTorchï¼Œ**æ—  TensorRT** |
| **ç²¾åº¦** | `bfloat16` | æ¨¡å‹æƒé‡å’Œè®¡ç®—ç²¾åº¦ |
| **Denoising Steps** | `10` | é»˜è®¤é…ç½® |
| **Attention** | `SDPA` | PyTorch åŸç”Ÿ |
| **KV Cache** | å¯ç”¨ | å‡å°‘é‡å¤è®¡ç®— |
| **æ··åˆç²¾åº¦** | å¦ | å…¨ç¨‹ bfloat16 |

### 11.2 å•æ¬¡æ¨ç†å»¶è¿Ÿåˆ†è§£

åŸºäºæµ‹é‡çš„ **308 ms** å•æ¬¡æ¨ç†å»¶è¿Ÿï¼š

| ç»„ä»¶ | ä¼°ç®—å»¶è¿Ÿ | å æ¯” | è¯´æ˜ |
|------|----------|------|------|
| Vision Encoder (SigLIP) | ~62 ms | 20% | å›¾åƒç‰¹å¾æå– |
| Text Embedding | ~15 ms | 5% | Tokenization + Embedding |
| KV Cache è®¡ç®— | ~46 ms | 15% | ä¸€æ¬¡æ€§è®¡ç®— |
| **Denoising (Ã—10)** | **~185 ms** | **60%** | **ä¸»è¦ç“¶é¢ˆ** |

### 11.3 ä¸ Zhiyuan çš„å·®è·åˆ†æ

Zhiyuan è¾¾åˆ° 45 ms çš„ä¼˜åŒ–è·¯å¾„ï¼š

| ä¼˜åŒ–é¡¹ | å»¶è¿Ÿæ”¶ç›Š | ç´¯è®¡å»¶è¿Ÿ |
|--------|----------|----------|
| èµ·ç‚¹ (PyTorch) | - | 142 ms |
| Attention fusion | -30 ms | 112 ms |
| FP8 MLP | -15 ms | 97 ms |
| nvFP4 MLP | -10 ms | 87 ms |
| Reformat æ¶ˆé™¤ | -10 ms | 77 ms |
| ä¸šåŠ¡è£å‡ | -32 ms | **45 ms** |

**æˆ‘ä»¬éœ€è¦çš„ä¼˜åŒ–**ï¼š
- å½“å‰ï¼š308 ms â†’ ç›®æ ‡ï¼š45 ms
- éœ€è¦å‡å°‘ï¼š**263 ms (85%)**

### 11.4 Action Chunking è¯´æ˜

Action Chunking ä¸æ˜¯æ¨ç†åŠ é€Ÿï¼Œåªæ˜¯å‡å°‘æ¨ç†é¢‘ç‡ï¼š

```
replan_steps=15 å«ä¹‰ï¼š
- æ¯æ¬¡æ¨ç†äº§ç”Ÿ 50 ä¸ª actions
- æ‰§è¡Œ 15 ä¸ª actions åæ‰é‡æ–°æ¨ç†
- å‡å°‘æ¨ç†æ¬¡æ•°ï¼Œä½†å•æ¬¡æ¨ç†ä»ç„¶æ˜¯ 308 ms
```

è¿™å¯¹äºæŸäº›åº”ç”¨åœºæ™¯æœ‰ç”¨ï¼Œä½†**ä¸è§£å†³å•æ¬¡æ¨ç†å»¶è¿Ÿé—®é¢˜**ã€‚

---

## 12. åç»­ä¼˜åŒ–æ–¹å‘ (è¾¾åˆ° 22 Hz ç›®æ ‡)

**ç›®æ ‡**: å•æ¬¡æ¨ç†å»¶è¿Ÿä» 308 ms é™åˆ° 45 ms

### 12.1 ä¼˜å…ˆçº§æ’åº

| ä¼˜å…ˆçº§ | ä¼˜åŒ–é¡¹ | é¢„æœŸæ”¶ç›Š | éš¾åº¦ |
|--------|--------|----------|------|
| **P0** | å‡å°‘ denoising_steps (10â†’3) | -200 ms | ä½ |
| **P1** | TensorRT åŠ é€Ÿ | -30~50 ms | ä¸­ |
| **P2** | Flash Attention ä¼˜åŒ– | -10~20 ms | ä¸­ |
| **P3** | FP8/FP4 é‡åŒ– | -10~20 ms | é«˜ |

### 12.2 ç«‹å³å¯åšçš„ä¼˜åŒ–

1. **å‡å°‘ denoising_steps**: ä» 10 æ­¥å‡å°‘åˆ° 3 æ­¥
   - é¢„æœŸå»¶è¿Ÿï¼š~100 ms â†’ 10 Hz
   - éœ€è¦æµ‹è¯•ç²¾åº¦å½±å“

2. **TensorRT åŠ é€Ÿ**:
   - Vision Encoder TRT
   - Denoising ä¸»å¾ªç¯ TRT

### 12.3 22 Hz è·¯å¾„è§„åˆ’

```
å½“å‰: 308 ms (3.2 Hz)
     â”‚
     â–¼ å‡å°‘ denoising_steps 10â†’3
     â”‚
~100 ms (10 Hz)
     â”‚
     â–¼ TensorRT åŠ é€Ÿ
     â”‚
~60 ms (17 Hz)
     â”‚
     â–¼ FP8 + Flash Attention
     â”‚
~45 ms (22 Hz) ğŸ¯ ç›®æ ‡
```

---

## 13. LIBERO Benchmark æµ‹è¯•ç»“æœ (2026-02-03)

### 13.1 æµ‹è¯•é…ç½®

```bash
python scripts/libero_eval_with_latency.py \
    --backend pytorch \
    --replan_steps 15 \
    --denoising_steps 10 \
    --num_tasks 3 \
    --num_trials 3
```

### 13.2 æµ‹è¯•ç»“æœ

| Task | æè¿° | æˆåŠŸç‡ | å¹³å‡å»¶è¿Ÿ |
|------|------|--------|----------|
| Task 0 | pick up the black bowl between the plate and the ramekin | **100%** (3/3) | 307.6 ms |
| Task 1 | pick up the black bowl next to the ramekin and place it | **66.7%** (2/3) | 307.6 ms |
| Task 2 | pick up the black bowl from table center and place it | **100%** (3/3) | 310.0 ms |

### 13.3 æ±‡æ€»ç»Ÿè®¡

| æŒ‡æ ‡ | æ•°å€¼ | è¯´æ˜ |
|------|------|------|
| **æ•´ä½“æˆåŠŸç‡** | **88.9%** (8/9) | ç²¾åº¦æ­£å¸¸ |
| **å¹³å‡æ¨ç†å»¶è¿Ÿ** | **308.3 ms** | **3.2 Hz** |
| å»¶è¿Ÿæ ‡å‡†å·® | 3.6 ms | ç¨³å®š |
| æœ€å°å»¶è¿Ÿ | 305.2 ms | - |
| æœ€å¤§å»¶è¿Ÿ | 337.0 ms | - |
| P95 å»¶è¿Ÿ | 310.3 ms | - |
| æ¨ç†æ¬¡æ•° | 80 æ¬¡ | - |

### 13.4 ä¸ Zhiyuan ç›®æ ‡å¯¹æ¯”

| æŒ‡æ ‡ | å½“å‰ | Zhiyuan ç›®æ ‡ | å·®è· |
|------|------|-------------|------|
| **å•æ¬¡æ¨ç†å»¶è¿Ÿ** | 308 ms | 45 ms | **6.8x æ…¢** |
| **æ¨ç†é¢‘ç‡** | 3.2 Hz | 22 Hz | **å·® 18.8 Hz** |
| ç²¾åº¦ | 88.9% | ~100% | ç•¥ä½ |

### 13.5 ç»“è®ºï¼ˆå·²è¿‡æ—¶ï¼Œè§ Section 14ï¼‰

---

## 14. Denoising Steps ä¼˜åŒ–ç»“æœ (2026-02-03)

### 14.1 æµ‹è¯•ç»“æœæ±‡æ€»

| denoising_steps | å»¶è¿Ÿ | Hz | ç²¾åº¦ | çŠ¶æ€ |
|-----------------|------|-----|------|------|
| 10 | 308 ms | 3.2 Hz | 88.9% | baseline |
| 3 | 182 ms | 5.5 Hz | **100%** | âœ… å¯ç”¨ |
| **2** | **164 ms** | **6.1 Hz** | **100%** | **æœ€ä½³** âœ… |
| 1 | 146 ms | 6.9 Hz | 88.9% | ç²¾åº¦ä¸‹é™ |

### 14.2 å…³é”®å‘ç°

1. **denoising_steps=2 æ˜¯æœ€ä½³é…ç½®**
   - å»¶è¿Ÿï¼š164 ms â†’ 6.1 Hz
   - ç²¾åº¦ï¼š100%ï¼ˆ9/9 æˆåŠŸï¼‰
   - æ¯” 10 steps å¿« **47%**

2. **ç²¾åº¦åè€Œæå‡**
   - 10 steps: 88.9%
   - 2-3 steps: 100%
   - åŸå› å¯èƒ½æ˜¯è¿‡å¤š denoising å¼•å…¥å™ªå£°

3. **1 step ç²¾åº¦ä¸‹é™**
   - ä» 100% é™åˆ° 88.9%
   - è¯´æ˜è‡³å°‘éœ€è¦ 2 æ­¥ denoising

### 14.3 å½“å‰çŠ¶æ€

| æŒ‡æ ‡ | å½“å‰ (steps=2) | Zhiyuan ç›®æ ‡ | å·®è· |
|------|----------------|-------------|------|
| **å•æ¬¡æ¨ç†å»¶è¿Ÿ** | 164 ms | 45 ms | **3.6x æ…¢** |
| **æ¨ç†é¢‘ç‡** | 6.1 Hz | 22 Hz | **å·® 15.9 Hz** |
| ç²¾åº¦ | 100% | ~100% | æ— å·®è· |

### 14.4 ä¼˜åŒ–è¿›åº¦

```
èµ·ç‚¹: 308 ms (3.2 Hz) @ 88.9% ç²¾åº¦
     â”‚
     â–¼ å‡å°‘ denoising_steps 10â†’2 âœ… å·²å®Œæˆ
     â”‚
å½“å‰: 164 ms (6.1 Hz) @ 100% ç²¾åº¦  â† æˆ‘ä»¬åœ¨è¿™é‡Œ
     â”‚
     â–¼ TensorRT åŠ é€Ÿ (ç›®æ ‡ -50 ms)
     â”‚
~114 ms (8.8 Hz)
     â”‚
     â–¼ FP8/Flash Attention (ç›®æ ‡ -30 ms)
     â”‚
~84 ms (12 Hz)
     â”‚
     â–¼ è¿›ä¸€æ­¥ä¼˜åŒ–
     â”‚
45 ms (22 Hz) ğŸ¯ ç›®æ ‡
```

### 14.5 é¢å¤–ä¼˜åŒ–æµ‹è¯•ç»“æœ

#### torch.compile æµ‹è¯•
```
Without torch.compile: 162.0 ms
With torch.compile:    163.1 ms
Speedup: 0.99x (æ— æ•ˆæœ)
```
- åŸå› ï¼šæ¨¡å‹å·²ç»ä¼˜åŒ–è‰¯å¥½ï¼Œtorch.compile æ— æ³•è¿›ä¸€æ­¥ä¼˜åŒ–

#### TensorRT Backend æµ‹è¯•

| Backend | å»¶è¿Ÿ | Hz | ç²¾åº¦ |
|---------|------|-----|------|
| pytorch | 164 ms | 6.1 Hz | 100% |
| tensorrt | 167 ms | 6.0 Hz | 100% |
| tensorrt_pipelined | 163 ms | 6.1 Hz | 100% |

**ç»“è®º**: TensorRT å‡ ä¹æ— åŠ é€Ÿæ•ˆæœ
- TensorRT åªåŠ é€Ÿäº† Vision Encoderï¼ˆå æ€»æ—¶é—´ ~20%ï¼‰
- ä¸»è¦ç“¶é¢ˆæ˜¯ denoising æ­¥éª¤ï¼ˆ~80%ï¼‰ï¼Œä»ç„¶æ˜¯ PyTorch

---

## 15. ä¼˜åŒ–ç“¶é¢ˆåˆ†æ (2026-02-03)

### 15.1 å½“å‰æœ€ä½³ç»“æœ

| é…ç½® | å»¶è¿Ÿ | Hz | ç²¾åº¦ |
|------|------|-----|------|
| **pytorch + denoising_steps=2** | **164 ms** | **6.1 Hz** | **100%** |
| Zhiyuan ç›®æ ‡ | 45 ms | 22 Hz | 100% |
| **å·®è·** | **119 ms** | **15.9 Hz** | - |

### 15.2 æ—¶é—´åˆ†è§£ï¼ˆä¼°ç®—ï¼‰

åŸºäº denoising_steps=2 çš„ 164 msï¼š

| ç»„ä»¶ | å»¶è¿Ÿ | å æ¯” |
|------|------|------|
| Vision Encoder (SigLIP) | ~30 ms | 18% |
| Embed Prefix | ~30 ms | 18% |
| KV Cache è®¡ç®— | ~35 ms | 21% |
| Denoising x2 | ~60 ms | 37% |
| å…¶ä»– (Python overhead) | ~10 ms | 6% |

### 15.3 ä¼˜åŒ–æŒ‘æˆ˜

è¦è¾¾åˆ° 45 ms (22 Hz) ç›®æ ‡ï¼š

1. **Denoising æ­¥éª¤æ— æ³•å†å‡å°‘**
   - 1 step: ç²¾åº¦ä¸‹é™åˆ° 88.9%
   - 2 steps: æœ€ä¼˜å¹³è¡¡ç‚¹

2. **TensorRT åŠ é€Ÿæœ‰é™**
   - Vision Encoder å·²ç”¨ TRTï¼Œæ•ˆæœå¾®å°
   - Denoising éœ€è¦å®Œæ•´ TRT è½¬æ¢ï¼ˆå·¥ç¨‹é‡å¤§ï¼‰

3. **è¿›ä¸€æ­¥ä¼˜åŒ–éœ€è¦**:
   - FP8/FP4 é‡åŒ– MLP å±‚
   - å®Œæ•´æ¨¡å‹ TensorRT è½¬æ¢
   - æˆ–è‡ªå®šä¹‰ CUDA kernels

### 15.4 å®é™…å¯è¡Œçš„ä¼˜åŒ–æ–¹å‘

| ä¼˜åŒ–æ–¹å‘ | é¢„æœŸæ”¶ç›Š | éš¾åº¦ | å¯è¡Œæ€§ |
|----------|----------|------|--------|
| å®Œæ•´ TRT è½¬æ¢ | -50~80 ms | é«˜ | éœ€è¦å¤§é‡å·¥ç¨‹ |
| FP8 é‡åŒ– | -20~30 ms | ä¸­ | éœ€è¦ç²¾åº¦éªŒè¯ |
| è‡ªå®šä¹‰ Triton kernels | -10~20 ms | é«˜ | éœ€è¦æ·±åº¦ä¼˜åŒ– |

### 15.5 é˜¶æ®µæ€§æ€»ç»“

**å·²å®Œæˆçš„ä¼˜åŒ–**:
1. âœ… å‡å°‘ denoising_steps: 10 â†’ 2ï¼ˆå»¶è¿Ÿ 308ms â†’ 164msï¼ŒåŠ é€Ÿ 1.88xï¼‰
2. âœ… æµ‹è¯• torch.compileï¼ˆæ— æ•ˆæœï¼‰
3. âœ… æµ‹è¯• TensorRTï¼ˆå‡ ä¹æ— æ•ˆæœï¼‰

**å½“å‰çŠ¶æ€**:
- å»¶è¿Ÿ: **164 ms â†’ 6.1 Hz**
- ç²¾åº¦: **100%**
- ç›¸æ¯”èµ·ç‚¹ 308 msï¼ŒåŠ é€Ÿ **1.88x**

**è·ç¦» 22 Hz çš„å·®è·**:
- éœ€è¦å†é™ä½ **119 ms (72%)**
- éœ€è¦æ›´æ¿€è¿›çš„å·¥ç¨‹ä¼˜åŒ–ï¼ˆTRT å…¨é‡è½¬æ¢ã€FP8 ç­‰ï¼‰

---

## 16. TRT Python API MLP ä¼˜åŒ– (2026-02-03)

### 16.1 æ–¹æ¡ˆé€‰æ‹©

æ ¹æ®æ™ºå…ƒåˆ†æï¼Œé€‰æ‹© TRT Python API ç›´æ¥æ„å»º networkï¼ˆä¸èµ° ONNXï¼‰ï¼š
- ç²¾ç¡®æ§åˆ¶æ¯å±‚ç²¾åº¦
- é¿å… ONNX è½¬æ¢é—®é¢˜
- å¯ä»¥å®ç°æ··åˆç²¾åº¦ç­–ç•¥

### 16.2 MLP æ€§èƒ½å¯¹æ¯”

| Backend | 1å±‚ (ms) | 18å±‚ (ms) | åŠ é€Ÿ |
|---------|---------|-----------|------|
| PyTorch FP16 | 3.38 | 60.8 | 1.00x |
| PyTorch FP8 Full | 2.18 | 39.3 | 1.55x |
| **TRT FP16 (API)** | **1.90** | **34.2** | **1.78x** |

**å…³é”®å‘ç°**: TRT FP16 æ¯” PyTorch FP8 è¿˜å¿«ï¼åœ¨ Thor ä¸Šä¸éœ€è¦ FP8 é‡åŒ–ä¹Ÿèƒ½è·å¾—æ›´å¥½æ€§èƒ½ã€‚

### 16.3 ç²¾åº¦éªŒè¯

TRT FP16 vs PyTorch FP16:
- Cosine similarity: **0.998**
- Max diff: 0.066
- **ç²¾åº¦å®Œå…¨å¯æ¥å—**

### 16.4 å®Œæ•´ Pipeline åˆ†æ

å½“å‰å»¶è¿Ÿåˆ†è§£ (denoising_steps=2):

| ç»„ä»¶ | å»¶è¿Ÿ | å æ¯” |
|------|------|------|
| Vision + Embed Prefix | 35 ms | 21% |
| **KV Cache (18å±‚)** | **89 ms** | **54%** |
| Denoise x2 | 40 ms | 25% |
| **æ€»è®¡** | **163 ms** | **6.1 Hz** |

KV Cache ä¸­ MLP å  ~59 msï¼Œæ˜¯ä¸»è¦ä¼˜åŒ–ç›®æ ‡ã€‚

### 16.5 TRT MLP ä¼˜åŒ–é¢„æœŸ

| æŒ‡æ ‡ | å½“å‰ | TRTä¼˜åŒ–å | æ”¹è¿› |
|------|------|---------|------|
| KV Cache MLP | 59 ms | 34 ms | -25 ms |
| æ€»å»¶è¿Ÿ | 163 ms | 138 ms | -15% |
| Hz | 6.1 | 7.3 | +20% |

### 16.6 å·²åˆ›å»ºæ–‡ä»¶

- `src/openpi/inference/trt_mlp.py` - TRT MLP æ¨¡å—
  - `TensorRTMLP` ç±»ï¼šä½¿ç”¨ TRT Python API æ„å»ºå¼•æ“
  - `replace_mlp_with_trt()` å‡½æ•°ï¼šæ›¿æ¢æ¨¡å‹ä¸­çš„ MLP å±‚
  - `benchmark_trt_mlp()` å‡½æ•°ï¼šæ€§èƒ½åŸºå‡†æµ‹è¯•

### 16.7 ä¸‹ä¸€æ­¥

1. é›†æˆ TRT MLP åˆ° `compute_prefix_kv_cache()`
2. æµ‹è¯• LIBERO ç²¾åº¦
3. å¦‚æœæˆåŠŸï¼Œè€ƒè™‘å°† Attention ä¹Ÿç”¨ TRT åŠ é€Ÿ

### 16.8 22 Hz è·¯å¾„æ›´æ–°

```
å½“å‰: 163 ms (6.1 Hz)
     â”‚
     â–¼ TRT MLP ä¼˜åŒ– (-25 ms)
     â”‚
138 ms (7.3 Hz) <- ä¸‹ä¸€æ­¥ç›®æ ‡
     â”‚
     â–¼ TRT Attention ä¼˜åŒ– (ä¼°è®¡ -15 ms)
     â”‚
~123 ms (8.1 Hz)
     â”‚
     â–¼ Denoise ä¼˜åŒ– + ä¸šåŠ¡è£å‡
     â”‚
45 ms (22 Hz) ğŸ¯ ç›®æ ‡
```

---

*Last Updated: 2026-02-03*
