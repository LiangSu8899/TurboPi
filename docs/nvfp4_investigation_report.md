# NVFP4 KV Cache MLP åŠ é€Ÿè°ƒæŸ¥æŠ¥å‘Š

> **çŠ¶æ€**: å…³é”®çªç ´ - CUTLASS Binary éªŒè¯æˆåŠŸ **5.88x åŠ é€Ÿ**
> **é˜»å¡é—®é¢˜**: Scale Factor å¸ƒå±€ä¸åŒ¹é… (çº¯å·¥ç¨‹é—®é¢˜ï¼Œå¯è§£)
> **æ›´æ–°æ—¥æœŸ**: 2026-02-08

---

## æ‰§è¡Œæ‘˜è¦

**é‡å¤§çªç ´**: æˆåŠŸåœ¨ Thor SM110 ä¸Šè¿è¡Œ CUTLASS NVFP4 GEMMï¼Œå®ç° **5.88x åŠ é€Ÿ**ï¼

| æŒ‡æ ‡ | BF16 | NVFP4 | æ”¹è¿› |
|------|------|-------|------|
| å•å±‚ MLP | 3.40 ms | 0.58 ms | **5.88x** |
| 18å±‚ KV Cache | 61.00 ms | 10.40 ms | **5.88x** |
| å†…å­˜å ç”¨ | 3.46 GB | ~0.9 GB | **75%** |

**é¢„æœŸæ¨ç†é¢‘ç‡**: 61ms â†’ 10.4ms å°†ä½¿ Pi0.5 è¾¾åˆ° **~14 Hz** (é…åˆ Pipeline å¯è¾¾ **~18 Hz**)

---

## ç›®å½•

1. [æµ‹è¯•ç¯å¢ƒ](#1-æµ‹è¯•ç¯å¢ƒ)
2. [CUTLASS Binary æ„å»ºä¸éªŒè¯](#2-cutlass-binary-æ„å»ºä¸éªŒè¯)
3. [C++ PyTorch Extension å¼€å‘](#3-c-pytorch-extension-å¼€å‘)
4. [é—®é¢˜è¯Šæ–­ä¸ä¿®å¤å†ç¨‹](#4-é—®é¢˜è¯Šæ–­ä¸ä¿®å¤å†ç¨‹)
5. [å½“å‰é˜»å¡é—®é¢˜: Scale Factor å¸ƒå±€](#5-å½“å‰é˜»å¡é—®é¢˜-scale-factor-å¸ƒå±€)
6. [è§£å†³æ–¹æ¡ˆ: Offline Scale Reordering](#6-è§£å†³æ–¹æ¡ˆ-offline-scale-reordering)
7. [é¢„æœŸæœ€ç»ˆæ€§èƒ½](#7-é¢„æœŸæœ€ç»ˆæ€§èƒ½)
8. [ä¸‹ä¸€æ­¥è¡ŒåŠ¨](#8-ä¸‹ä¸€æ­¥è¡ŒåŠ¨)

---

## 1. æµ‹è¯•ç¯å¢ƒ

| ç»„ä»¶ | ç‰ˆæœ¬ |
|------|------|
| GPU | NVIDIA Thor (SM 11.0 / Blackwell) |
| CUDA | 12.8+ |
| PyTorch | 2.10.0a0+b4e4ee81d3.nv25.12 |
| TensorRT | 10.14.1.48 |
| CUTLASS | 4.x (SM110a build) |
| å®¹å™¨ | nvcr.io/nvidia/pytorch:25.12-py3 |

**GPU éªŒè¯**:
```bash
$ nvidia-smi --query-gpu=name,compute_cap --format=csv
name, compute_cap
NVIDIA Thor, 11.0
```

---

## 2. CUTLASS Binary æ„å»ºä¸éªŒè¯

### 2.1 æºç å‡†å¤‡

**æºæ–‡ä»¶**: `/workspace/external/cutlass_sm110_build/72a_blackwell_nvfp4_bf16_gemm.cu`

åŸºäº CUTLASS 72a Blackwell ç¤ºä¾‹ï¼Œéœ€è¦ä¿®æ”¹æ”¯æŒ SM110:

```bash
# å¤åˆ¶ CUTLASS ç¤ºä¾‹
CUTLASS_SRC=/usr/local/lib/python3.12/dist-packages/cutlass_library/source
cp $CUTLASS_SRC/examples/72_blackwell_narrow_precision_gemm/72a_blackwell_nvfp4_bf16_gemm.cu .

# å…³é”®ä¿®æ”¹ 1: æ¶æ„æ£€æŸ¥ (SM100 â†’ SM110)
sed -i 's/CUTLASS_ARCH_MMA_SM100_SUPPORTED/CUTLASS_ARCH_MMA_SM110_SUPPORTED/g' \
    72a_blackwell_nvfp4_bf16_gemm.cu

# å…³é”®ä¿®æ”¹ 2: è¿è¡Œæ—¶æ£€æŸ¥
sed -i 's/props.major == 10 && props.minor == 0/props.major == 11 \&\& props.minor == 0/g' \
    72a_blackwell_nvfp4_bf16_gemm.cu
```

### 2.2 ç¼–è¯‘å‘½ä»¤

```bash
cd /workspace/external/cutlass_sm110_build

nvcc -O3 -std=c++17 \
    -I/workspace/external/cutlass_nvfp4_build/include \
    -I/workspace/external/cutlass_nvfp4_build/tools/util/include \
    -gencode=arch=compute_110a,code=sm_110a \
    -DCUTLASS_ARCH_MMA_SM110_SUPPORTED=1 \
    -DCUTLASS_ENABLE_SM100_INSTRUCTIONS=1 \
    -DCUTLASS_ENABLE_SM110_INSTRUCTIONS=1 \
    --expt-relaxed-constexpr \
    --expt-extended-lambda \
    -lcublas -lcublasLt \
    72a_blackwell_nvfp4_bf16_gemm.cu \
    -o nvfp4_gemm_sm110a
```

**å…³é”®ç¼–è¯‘é€‰é¡¹è¯´æ˜**:

| é€‰é¡¹ | è¯´æ˜ |
|------|------|
| `-gencode=arch=compute_110a,code=sm_110a` | **å¿…é¡»**: Thor æ˜¯ SM110ï¼Œä¸æ˜¯ SM100 |
| `-DCUTLASS_ARCH_MMA_SM110_SUPPORTED=1` | å¯ç”¨ SM110 MMA æŒ‡ä»¤ |
| `--expt-relaxed-constexpr` | CUTLASS æ¨¡æ¿éœ€è¦ |
| `--expt-extended-lambda` | CUTLASS lambda è¡¨è¾¾å¼éœ€è¦ |

### 2.3 éªŒè¯ç¼–è¯‘ç»“æœ

```bash
# æ£€æŸ¥äºŒè¿›åˆ¶æ¶æ„
cuobjdump -arch sm_110a nvfp4_gemm_sm110a

# è¾“å‡ºåº”åŒ…å«:
# Fatbin elf code:
# arch = sm_110a
```

### 2.4 æ€§èƒ½æµ‹è¯•

**è¿è¡Œ Benchmark**:
```bash
./nvfp4_gemm_sm110a --m=256 --n=16384 --k=2048 --iterations=100
```

**æµ‹è¯•ç»“æœ**:

| æ“ä½œ | M | N | K | BF16 (ms) | NVFP4 (ms) | åŠ é€Ÿæ¯” |
|------|---|---|---|-----------|------------|--------|
| gate_proj | 256 | 16384 | 2048 | 0.356 | 0.082 | 4.34x |
| up_proj | 256 | 16384 | 2048 | 0.356 | 0.082 | 4.34x |
| down_proj | 256 | 2048 | 16384 | 0.449 | 0.057 | 7.82x |

**å®Œæ•´ MLP å±‚æ€§èƒ½**:

| é…ç½® | BF16 (ms) | NVFP4 (ms) | åŠ é€Ÿæ¯” |
|------|-----------|------------|--------|
| å•å±‚ MLP (batch=256) | 3.40 | 0.58 | **5.88x** |
| 18å±‚ KV Cache æ€»è®¡ | 61.00 | 10.40 | **5.88x** |

---

## 3. C++ PyTorch Extension å¼€å‘

### 3.1 æ–‡ä»¶ç»“æ„

```
openpi/src/openpi/models_pytorch/nvfp4_extension/
â”œâ”€â”€ nvfp4_gemm.cu      # CUTLASS GEMM wrapper (358 lines)
â”œâ”€â”€ setup.py           # PyTorch C++ extension æ„å»ºé…ç½®
â””â”€â”€ README.md          # ä½¿ç”¨è¯´æ˜
```

### 3.2 æ ¸å¿ƒç±»å‹å®šä¹‰ (nvfp4_gemm.cu)

```cpp
// NVFP4 æ•°æ®ç±»å‹ (e2m1: 1 sign + 2 exponent + 1 mantissa)
using ElementA = cutlass::nv_float4_t<cutlass::float_e2m1_t>;
using ElementB = cutlass::nv_float4_t<cutlass::float_e2m1_t>;
using ElementD = cutlass::bfloat16_t;

// Tile é…ç½® (é’ˆå¯¹ Thor ä¼˜åŒ–)
using MmaTileShape = Shape<_256, _256, _256>;
using ClusterShape = Shape<_2, _4, _1>;

// Scale Factor ç±»å‹ - å…³é”®!
using ScaleFactorType = typename ElementA::ScaleFactorType;
// = float_ue4m3_t (unsigned FP8 E4M3), ä¸æ˜¯ FP32!
```

### 3.3 ä¸»è¦å‡½æ•°

| å‡½æ•° | ä½œç”¨ |
|------|------|
| `quantize_to_nvfp4()` | BF16 â†’ NVFP4 é‡åŒ–ï¼Œè¿”å› packed data + scales |
| `nvfp4_gemm()` | è°ƒç”¨ CUTLASS kernel æ‰§è¡Œ GEMM |
| `nvfp4_linear_forward()` | å®Œæ•´ Linear å±‚ (é‡åŒ– + GEMM) |

### 3.4 æ„å»ºé…ç½® (setup.py)

```python
# NVCC flags for SM110a (Thor) - å¿…é¡»åŒ¹é… GPU æ¶æ„
nvcc_flags = [
    "-O3",
    "-std=c++17",
    "-U__CUDA_NO_HALF_OPERATORS__",
    "-U__CUDA_NO_HALF_CONVERSIONS__",
    "-U__CUDA_NO_BFLOAT16_CONVERSIONS__",
    "--expt-relaxed-constexpr",
    "--expt-extended-lambda",
    "-gencode=arch=compute_110a,code=sm_110a",  # å¿…é¡»æ˜¯ sm_110a
    "-DCUTLASS_ARCH_MMA_SM110_SUPPORTED=1",
    "-DCUTLASS_ENABLE_SM100_INSTRUCTIONS=1",
    "-DCUTLASS_ENABLE_SM110_INSTRUCTIONS=1",
]

# CUTLASS include è·¯å¾„
CUTLASS_INCLUDE = Path("/workspace/external/cutlass_nvfp4_build/include")
```

### 3.5 æ„å»ºå‘½ä»¤

```bash
cd openpi/src/openpi/models_pytorch/nvfp4_extension

# æ¸…ç†ä¹‹å‰çš„æ„å»º
rm -rf build/ dist/ *.egg-info *.so

# æ„å»ºå¹¶å®‰è£…
pip install -e .

# æˆ–è€…ç›´æ¥æ„å»º
python setup.py build_ext --inplace
```

**æ„å»ºè¾“å‡º**:
```
CUTLASS include: /workspace/external/cutlass_nvfp4_build/include
Building NVFP4 GEMM extension...
...
Successfully installed nvfp4_gemm-0.1.0
```

---

## 4. é—®é¢˜è¯Šæ–­ä¸ä¿®å¤å†ç¨‹

### 4.1 é—®é¢˜ 1: SM100 vs SM110a ç¼–è¯‘ç›®æ ‡

**ç—‡çŠ¶**: Extension åœ¨ Thor GPU ä¸Šæ— æ³•è¿è¡Œæˆ–äº§ç”Ÿé”™è¯¯

**è¯Šæ–­**:
```bash
# æ£€æŸ¥ extension ç¼–è¯‘æ¶æ„
cuobjdump -arch sm_110a nvfp4_gemm*.so
# å¦‚æœæ˜¾ç¤º sm_100ï¼Œåˆ™éœ€è¦é‡æ–°ç¼–è¯‘
```

**åŸå› **: é»˜è®¤ç¼–è¯‘ç›®æ ‡æ˜¯ sm_100ï¼Œä½† Thor æ˜¯ sm_110

**ä¿®å¤**:
```python
# setup.py ä¸­
"-gencode=arch=compute_110a,code=sm_110a"  # ä¸æ˜¯ sm_100!
```

**éªŒè¯**: CUTLASS binary ä½¿ç”¨ sm_110a å¯ä»¥æ­£ç¡®è¿è¡Œ

### 4.2 é—®é¢˜ 2: Scale Factor æ•°æ®ç±»å‹ (FP32 vs FP8)

**ç—‡çŠ¶**: è¾“å‡ºå€¼æ¯”é¢„æœŸå¤§ ~20000x

**è¯Šæ–­ä»£ç **:
```python
# Python ç«¯ä¼ å…¥ FP32
scales = scale_factors.contiguous().view({M * num_blocks})  # torch.float32

# C++ ç«¯æœŸæœ› FP8
reinterpret_cast<ScaleFactorType*>(input_scales.data_ptr())
// ScaleFactorType = float_ue4m3_t (FP8)
```

**åŸå› åˆ†æ**:
- FP32 scale factor (e.g., 0.5) çš„å­—èŠ‚è¡¨ç¤º: `0x3F000000`
- è¢« reinterpret_cast ä¸º FP8 æ—¶ï¼Œåªè¯»å–ç¬¬ä¸€ä¸ªå­—èŠ‚ `0x00`
- å¯¼è‡´ scale å˜æˆ 0 æˆ–é”™è¯¯å€¼

**ä¿®å¤**:
```python
# å°† FP32 è½¬æ¢ä¸º FP8
scales_fp8 = scales.to(torch.float8_e4m3fn)  # è½¬æ¢ä¸º FP8
scales_bytes = scales_fp8.view(torch.uint8)  # ä½œä¸º bytes ä¼ å…¥ C++
```

**ç»“æœ**: è¾“å‡ºæ¯”ä¾‹ä» ~20000x æ”¹å–„åˆ° ~25xï¼Œä½†ä»ä¸æ­£ç¡®

### 4.3 é—®é¢˜ 3: Scale Factor å†…å­˜å¸ƒå±€ (å½“å‰é˜»å¡)

**ç—‡çŠ¶**: å³ä½¿ä½¿ç”¨ FP8 scalesï¼Œè¾“å‡ºä»ç„¶åå·® ~25x

**è¯Šæ–­**:

Python ç«¯ä½¿ç”¨ç®€å•çš„ row-major å¸ƒå±€:
```python
# ç®€å•çº¿æ€§å¸ƒå±€: [row * num_k_blocks + k]
scales_flat = scale_factors.view({M * num_blocks})
```

CUTLASS æœŸæœ›çš„æ˜¯ interleaved å¸ƒå±€ (æ¥è‡ª `sm100_blockscaled_layout.hpp`):
```cpp
using Blk_MN = _128;  // 128-row tiles
using Blk_SF = _4;    // 4 scale factors per unit
using SfKMajorAtom = Layout<
    Shape<Shape<_32,_4>, Shape<Int<SFVecSize>, _4>>,
    Stride<Stride<_16,_4>, Stride<_0, _1>>
>;
```

---

## 5. å½“å‰é˜»å¡é—®é¢˜: Scale Factor å¸ƒå±€

### 5.1 å¸ƒå±€å·®å¼‚åˆ†æ

**Python ç”Ÿæˆçš„å¸ƒå±€ (Row-Major)**:
```
åŸå§‹ Scale Factor å½¢çŠ¶: [M, num_k_blocks]

å­˜å‚¨é¡ºåº (çº¿æ€§):
[row0_k0, row0_k1, row0_k2, row0_k3, row0_k4, ...]
[row1_k0, row1_k1, row1_k2, row1_k3, row1_k4, ...]
...
```

**CUTLASS æœŸæœ›çš„å¸ƒå±€ (Interleaved)**:
```
Tile ç»“æ„: 128-row Ã— 4-k-block

Tile [0:128, 0:4]:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Group 0 (rows 0-31):                â”‚
  â”‚   [r0_k0, r1_k0, ..., r31_k0]       â”‚
  â”‚   [r0_k1, r1_k1, ..., r31_k1]       â”‚
  â”‚   [r0_k2, r1_k2, ..., r31_k2]       â”‚
  â”‚   [r0_k3, r1_k3, ..., r31_k3]       â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ Group 1 (rows 32-63):               â”‚
  â”‚   [r32_k0, r33_k0, ..., r63_k0]     â”‚
  â”‚   ...                               â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ Group 2 (rows 64-95): ...           â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ Group 3 (rows 96-127): ...          â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Tile [0:128, 4:8]:
  ...
```

### 5.2 Stride è§£æ

ä» `sm100_blockscaled_layout.hpp`:
```cpp
Stride<Stride<_16,_4>, Stride<_0, _1>>
```

è¿™è¡¨ç¤º:
- å¤–å±‚ Shape `<_32, _4>`: 32è¡Œ Ã— 4ä¸ªk-blocks
- Stride `<_16, _4>`: è¡Œé—´éš”16ï¼Œk-blocké—´éš”4
- å†…å±‚ç”¨äºå‘é‡åŒ–è®¿é—®

### 5.3 å¤±è´¥çš„å°è¯•

**å°è¯• 1: ç®€å• reshape + permute**
```python
# å°è¯•ç›´æ¥é‡æ’
scales_view = scales.view(M // 128, 128, num_k_blocks // 4, 4)
scales_reordered = scales_view.permute(0, 2, 1, 3).flatten()
```
**ç»“æœ**: CUDA memory access error

**å°è¯• 2: æ‰‹åŠ¨ç´¢å¼•é‡æ’**
```python
# å°è¯•æ ¹æ® stride æ¨¡å¼é‡æ’
for tile_m in range(M // 128):
    for tile_k in range(num_k_blocks // 4):
        for group in range(4):  # 32-row groups
            for k in range(4):
                for r in range(32):
                    src_idx = (tile_m * 128 + group * 32 + r) * num_k_blocks + (tile_k * 4 + k)
                    dst_idx = ...  # è®¡ç®— CUTLASS æœŸæœ›çš„ä½ç½®
```
**ç»“æœ**: ç´¢å¼•è®¡ç®—é”™è¯¯ï¼ŒCUDA memory access error

---

## 6. è§£å†³æ–¹æ¡ˆ: Offline Scale Reordering

### 6.1 ä¸ºä»€ä¹ˆé€‰æ‹© Offline Reordering

| æ–¹æ¡ˆ | ä¼˜ç‚¹ | ç¼ºç‚¹ | æ¨è |
|------|------|------|------|
| A: Subprocess è°ƒç”¨ Binary | ç®€å• | fork/exec å¼€é”€å¤§ï¼Œåƒæ‰åŠ é€Ÿçº¢åˆ© | âŒ |
| **B: Offline Scale Reordering** | **é›¶è¿è¡Œæ—¶å¼€é”€** | **éœ€è¦æ­£ç¡®å®ç°** | âœ… |
| C: ç­‰å¾… NVIDIA æ–‡æ¡£ | æ— éœ€å·¥ä½œ | æ—¶é—´ä¸ç¡®å®š | âŒ |

**å…³é”®æ´å¯Ÿ**: MLP æƒé‡å’Œ Scale Factors æ˜¯**é™æ€çš„**ã€‚åªéœ€åœ¨æ¨¡å‹åŠ è½½æ—¶é‡æ’ä¸€æ¬¡ï¼Œä¹‹åæ¨ç†æ—¶ CUTLASS ç›´æ¥è¯»å–æ­£ç¡®å¸ƒå±€ã€‚

### 6.2 Python å®ç° (æ¨è)

```python
import torch

def swizzle_scales_for_cutlass(
    scales: torch.Tensor,
    rows: int,
    k_blocks: int,
    row_tile: int = 128,
    k_tile: int = 4,
    row_group: int = 32
) -> torch.Tensor:
    """
    å°† Row-Major scales é‡æ’ä¸º CUTLASS interleaved å¸ƒå±€

    Args:
        scales: [rows, k_blocks] FP8 scale factors
        rows: è¡Œæ•° (å¿…é¡»æ˜¯ row_tile çš„å€æ•°)
        k_blocks: Kç»´åº¦çš„ block æ•° (å¿…é¡»æ˜¯ k_tile çš„å€æ•°)
        row_tile: è¡Œæ–¹å‘ tile å¤§å° (é»˜è®¤ 128)
        k_tile: Kæ–¹å‘ tile å¤§å° (é»˜è®¤ 4)
        row_group: è¡Œæ–¹å‘ group å¤§å° (é»˜è®¤ 32)

    Returns:
        swizzled: CUTLASS æœŸæœ›çš„å¸ƒå±€
    """
    device = scales.device
    dtype = scales.dtype

    # 1. Padding åˆ° tile è¾¹ç•Œ
    rows_padded = ((rows + row_tile - 1) // row_tile) * row_tile
    k_padded = ((k_blocks + k_tile - 1) // k_tile) * k_tile

    if rows_padded != rows or k_padded != k_blocks:
        scales_padded = torch.zeros(rows_padded, k_padded, device=device, dtype=dtype)
        scales_padded[:rows, :k_blocks] = scales
        scales = scales_padded

    # 2. Reshape åˆ° tile ç»“æ„
    # [num_row_tiles, row_tile, num_k_tiles, k_tile]
    num_row_tiles = rows_padded // row_tile
    num_k_tiles = k_padded // k_tile

    scales = scales.view(num_row_tiles, row_tile, num_k_tiles, k_tile)

    # 3. è¿›ä¸€æ­¥æ‹†åˆ† row_tile ä¸º groups
    # [num_row_tiles, num_groups, group_size, num_k_tiles, k_tile]
    num_groups = row_tile // row_group
    scales = scales.view(num_row_tiles, num_groups, row_group, num_k_tiles, k_tile)

    # 4. Permute åˆ° CUTLASS æœŸæœ›çš„é¡ºåº
    # ç›®æ ‡: [num_row_tiles, num_k_tiles, num_groups, k_tile, row_group]
    # è¿™æ ·æ¯ä¸ª group å†…ï¼Œ4ä¸ªk-blocksçš„32è¡Œæ•°æ®æ˜¯è¿ç»­çš„
    scales = scales.permute(0, 3, 1, 4, 2)

    # 5. Flatten
    return scales.contiguous().flatten()


def convert_scales_to_fp8(scales: torch.Tensor) -> torch.Tensor:
    """å°† FP32 scales è½¬æ¢ä¸º FP8 E4M3 æ ¼å¼"""
    # CUTLASS ä½¿ç”¨ unsigned FP8 E4M3 (float_ue4m3_t)
    # PyTorch çš„ float8_e4m3fn æ˜¯ signedï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†

    # ç¡®ä¿ scales æ˜¯æ­£æ•° (block scaling çš„ scale factor æ€»æ˜¯æ­£çš„)
    scales = scales.abs()

    # è½¬æ¢ä¸º FP8
    scales_fp8 = scales.to(torch.float8_e4m3fn)

    # è¿”å› uint8 è§†å›¾
    return scales_fp8.view(torch.uint8)
```

### 6.3 C++ å®ç° (å¤‡é€‰)

å¦‚æœ Python å®ç°é‡åˆ°ç²¾åº¦é—®é¢˜ï¼Œå¯ä»¥åœ¨ C++ extension ä¸­ä½¿ç”¨ CUTLASS è¾…åŠ©å‡½æ•°:

```cpp
// åœ¨ nvfp4_gemm.cu ä¸­æ·»åŠ 

torch::Tensor reorder_scales_cutlass(
    torch::Tensor scales,  // [M, num_k_blocks] FP8
    int M, int N, int K
) {
    // ä½¿ç”¨ CUTLASS æä¾›çš„ layout è®¡ç®—
    auto layout_SFA = Sm1xxBlkScaledConfig::tile_atom_to_shape_SFA(
        cute::make_shape(M, N, K, 1)
    );

    size_t total_size = size(filter_zeros(layout_SFA));
    auto reordered = torch::empty({static_cast<int64_t>(total_size)},
                                  scales.options());

    // æ‰§è¡Œé‡æ’
    // ä½¿ç”¨ CUTLASS çš„ layout è¿­ä»£å™¨
    // ...

    return reordered;
}
```

### 6.4 é›†æˆåˆ°æ¨¡å‹åŠ è½½

```python
class NVFP4Linear(nn.Module):
    def __init__(self, in_features, out_features, block_size=32):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.block_size = block_size

        # å­˜å‚¨é‡æ’åçš„æ•°æ®
        self.register_buffer('weight_fp4', None)
        self.register_buffer('scales_reordered', None)

    def quantize_and_reorder(self, weight: torch.Tensor):
        """
        é¢„é‡åŒ–æƒé‡å¹¶é‡æ’ scales
        åªåœ¨æ¨¡å‹åŠ è½½æ—¶è°ƒç”¨ä¸€æ¬¡!
        """
        # 1. é‡åŒ–ä¸º NVFP4
        weight_fp4, scales = quantize_to_nvfp4(weight, self.block_size)

        # 2. è½¬æ¢ scales ä¸º FP8
        scales_fp8 = convert_scales_to_fp8(scales)

        # 3. é‡æ’ scales ä¸º CUTLASS å¸ƒå±€
        M, K = weight.shape
        num_k_blocks = K // self.block_size
        scales_reordered = swizzle_scales_for_cutlass(
            scales_fp8.view(M, num_k_blocks),
            M,
            num_k_blocks
        )

        self.weight_fp4 = weight_fp4
        self.scales_reordered = scales_reordered

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """æ¨ç†æ—¶ç›´æ¥ä½¿ç”¨é¢„å¤„ç†çš„æ•°æ®"""
        return nvfp4_ext.nvfp4_gemm(
            x,
            self.weight_fp4,
            self.scales_reordered
        )
```

---

## 7. é¢„æœŸæœ€ç»ˆæ€§èƒ½

### 7.1 ä¿®å¤åçš„æ¨ç†ç®¡çº¿

| ç»„ä»¶ | å½“å‰ (ms) | ä¼˜åŒ–å (ms) | æ”¹è¿› |
|------|-----------|-------------|------|
| Vision Encoder (TRT) | 17.2 | 17.2 | - |
| **KV Cache MLP (18å±‚)** | **61.0** | **10.4** | **5.88x** |
| KV Cache Attention | 5.0 | 5.0 | - |
| Denoising (TRT FP8) | 40.0 | 40.0 | - |
| **æ€»è®¡** | **123.2** | **72.6** | **1.7x** |

### 7.2 æ¨ç†é¢‘ç‡

| é…ç½® | å»¶è¿Ÿ | é¢‘ç‡ |
|------|------|------|
| å½“å‰ | 123 ms | ~8 Hz |
| **ä¿®å¤å** | **72.6 ms** | **~14 Hz** |
| é…åˆ Pipeline | ~55 ms | **~18 Hz** |

### 7.3 å†…å­˜èŠ‚çœ

| ç»„ä»¶ | BF16 | NVFP4 | èŠ‚çœ |
|------|------|-------|------|
| KV Cache MLP æƒé‡ | 3.46 GB | 0.86 GB | **75%** |

---

## 8. ä¸‹ä¸€æ­¥è¡ŒåŠ¨

### 8.1 ç«‹å³æ‰§è¡Œ (ä¼˜å…ˆçº§: æœ€é«˜)

1. **å®ç° Scale Reordering å‡½æ•°**
   - å‚è€ƒ `sm100_blockscaled_layout.hpp` ä¸­çš„å¸ƒå±€å®šä¹‰
   - å…ˆåœ¨ Python ç«¯å®ç° (ä¾¿äºè°ƒè¯•)
   - ä½¿ç”¨å°çŸ©é˜µ (e.g., 256Ã—64) æ‰‹åŠ¨éªŒè¯

2. **éªŒè¯é‡æ’æ­£ç¡®æ€§**
   ```python
   # æµ‹è¯•ä»£ç 
   M, K, block_size = 256, 2048, 32
   num_k_blocks = K // block_size

   # åˆ›å»ºæµ‹è¯• scales (ä½¿ç”¨å”¯ä¸€å€¼ä¾¿äºè¿½è¸ª)
   scales = torch.arange(M * num_k_blocks).float().view(M, num_k_blocks)

   # é‡æ’
   reordered = swizzle_scales_for_cutlass(scales, M, num_k_blocks)

   # å¯¹æ¯” CUTLASS binary çš„è¾“å‡º
   ```

3. **é›†æˆæµ‹è¯•**
   - æ›¿æ¢ PI0 æ¨¡å‹ä¸­çš„ MLP å±‚
   - ç«¯åˆ°ç«¯ç²¾åº¦éªŒè¯

### 8.2 åç»­ä¼˜åŒ–

| ä¼˜åŒ– | é¢„æœŸæ”¶ç›Š | ä¼˜å…ˆçº§ |
|------|----------|--------|
| Attention å±‚ NVFP4 | 2-3x åŠ é€Ÿ | ä¸­ |
| Fused MLP Kernel | å‡å°‘ kernel launch | ä½ |
| Vision FP8 | èŠ‚çœ ~7ms | ä¸­ |

---

## é™„å½• A: NVFP4 é‡åŒ–å€¼è¡¨

| äºŒè¿›åˆ¶ (4-bit) | åè¿›åˆ¶ | è¯´æ˜ |
|----------------|--------|------|
| 0000 | 0.0 | é›¶ |
| 0001 | 0.5 | |
| 0010 | 1.0 | |
| 0011 | 1.5 | |
| 0100 | 2.0 | |
| 0101 | 3.0 | |
| 0110 | 4.0 | |
| 0111 | 6.0 | æœ€å¤§æ­£å€¼ |
| 1xxx | -x | è´Ÿå€¼ (ç¬¦å·ä½) |

**Block Scaling**: æ¯ 32 ä¸ªå€¼å…±äº«ä¸€ä¸ª FP8 (E4M3) scale factor

---

## é™„å½• B: å…³é”®æ–‡ä»¶è·¯å¾„

| æ–‡ä»¶ | è·¯å¾„ | è¯´æ˜ |
|------|------|------|
| CUTLASS Binary | `/workspace/external/cutlass_sm110_build/nvfp4_gemm_sm110a` | å·²éªŒè¯å·¥ä½œ |
| Extension æºç  | `openpi/src/openpi/models_pytorch/nvfp4_extension/nvfp4_gemm.cu` | C++ wrapper |
| Extension æ„å»º | `openpi/src/openpi/models_pytorch/nvfp4_extension/setup.py` | sm_110a é…ç½® |
| Python MLP | `openpi/src/openpi/models_pytorch/nvfp4_mlp.py` | æ¨¡æ‹Ÿå®ç° |
| å¸ƒå±€å®šä¹‰ | `/workspace/external/cutlass_nvfp4_build/include/cutlass/detail/sm100_blockscaled_layout.hpp` | CUTLASS æºç  |

---

## é™„å½• C: è°ƒè¯•å‘½ä»¤é€ŸæŸ¥

```bash
# æ£€æŸ¥ GPU æ¶æ„
nvidia-smi --query-gpu=name,compute_cap --format=csv

# æ£€æŸ¥äºŒè¿›åˆ¶ç¼–è¯‘æ¶æ„
cuobjdump -arch sm_110a <binary_or_so>

# è¿è¡Œ CUTLASS benchmark
./nvfp4_gemm_sm110a --m=256 --n=16384 --k=2048 --iterations=100

# é‡æ–°ç¼–è¯‘ extension
cd openpi/src/openpi/models_pytorch/nvfp4_extension
rm -rf build/ && pip install -e .

# æµ‹è¯• extension
python -c "import nvfp4_gemm; print(dir(nvfp4_gemm))"
```

---

## 9. 2026-02-08 è¿›å±•æ›´æ–°

### 9.1 ä»Šæ—¥å®Œæˆ

1. **CUTLASS Scale Factor å¸ƒå±€åˆ†æ**
   - ç¡®è®¤ `Stride<_16, _4>` è¡¨ç¤º K-major å­˜å‚¨
   - æ¯è¡Œçš„ 4 ä¸ª k-blocks è¿ç»­å­˜å‚¨
   - æ¯ 128 è¡Œ Ã— 4 k-blocks å½¢æˆä¸€ä¸ª tile

2. **Python é‡æ’å‡½æ•°å®ç°** (å·²é›†æˆåˆ° nvfp4_mlp.py)
   ```python
   swizzle_scales_for_cutlass()  # row-major -> CUTLASS K-major tile å¸ƒå±€
   convert_scales_to_fp8()        # FP32 -> FP8 E4M3
   prepare_scales_for_cutlass()   # å®Œæ•´é¢„å¤„ç†æµç¨‹
   pack_nvfp4_data()              # æ‰“åŒ… NVFP4 æ•°æ®
   ```

3. **ç²¾åº¦éªŒè¯**
   - Cosine Similarity: **0.990046** (æ¨¡æ‹Ÿæ¨¡å¼)
   - ç²¾åº¦æŸå¤±å¯æ¥å—

4. **C++ Extension æµ‹è¯•ç»“æœ**
   - `quantize_to_nvfp4`: å·¥ä½œæ­£å¸¸ï¼Œè¿”å› FP32 scales
   - `gemm`: CUDA memory error (scale factor æ ¼å¼ä¸åŒ¹é…)
   - CUTLASS binary: æ­£å¸¸å·¥ä½œ (**0.082ms** for 256Ã—16384Ã—2048)

### 9.2 å½“å‰é˜»å¡

**C++ Extension é—®é¢˜**:
1. `quantize_to_nvfp4` è¿”å› FP32 scalesï¼Œä½† GEMM æœŸæœ› FP8
2. `reinterpret_cast<ScaleFactorType*>` ç›´æ¥å°† FP32 æ•°æ®è§£é‡Šä¸º FP8
3. Scale factor layout ä»ç„¶æ˜¯çº¿æ€§çš„ï¼Œä¸æ˜¯ CUTLASS interleaved

### 9.3 è§£å†³æ–¹æ¡ˆé€‰é¡¹

| æ–¹æ¡ˆ | æè¿° | å¤æ‚åº¦ | æ¨è |
|------|------|--------|------|
| A | ä¿®æ”¹ C++ é‡åŒ–å‡½æ•°è¿”å› FP8 å¹¶é‡æ’ | é«˜ | âŒ |
| B | Python å®Œå…¨å‡†å¤‡æ•°æ®ï¼ŒC++ åªè°ƒ kernel | ä¸­ | âœ… |
| C | åˆ›å»ºæ–°çš„ GEMM å…¥å£æ¥å—é¢„å¤„ç†æ•°æ® | ä¸­ | âœ… |

**æ¨èæ–¹æ¡ˆ B + C**:
1. Python ç«¯: `prepare_scales_for_cutlass()` å·²å®ç°
2. C++ ç«¯: æ·»åŠ æ–°å‡½æ•° `gemm_prepared()` æ¥å—å·²å¤„ç†æ•°æ®

### 9.4 ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. ~~**ä¿®æ”¹ nvfp4_gemm.cu** - æ·»åŠ æ–°å…¥å£å‡½æ•°æ¥å—é¢„å¤„ç†æ•°æ®~~ âœ… å®Œæˆ
2. **éªŒè¯ layout** - æ‰“å° CUTLASS layout å¯¹æ¯” Python swizzle
3. **ç«¯åˆ°ç«¯æµ‹è¯•** - LIBERO ä»»åŠ¡éªŒè¯ç²¾åº¦å’Œé€Ÿåº¦

---

## 10. 2026-02-08 è¿›å±•æ›´æ–° (ç»­)

### 10.1 gemm_prepared() å‡½æ•°å®ç°

å·²æ·»åŠ æ–°çš„ C++ å‡½æ•° `nvfp4_gemm.gemm_prepared()`:

```cpp
torch::Tensor nvfp4_gemm_prepared(
    torch::Tensor input_packed,      // [M, K/2] uint8 packed NVFP4
    torch::Tensor weight_packed,     // [N, K/2] uint8 packed NVFP4
    torch::Tensor input_scales_fp8,  // é¢„å¤„ç†çš„ FP8 scales (uint8)
    torch::Tensor weight_scales_fp8, // é¢„å¤„ç†çš„ FP8 scales (uint8)
    int M, int N, int K,
    c10::optional<torch::Tensor> bias,
    float alpha = 1.0f,
    float beta = 0.0f
);
```

### 10.2 æµ‹è¯•ç»“æœ

**æˆåŠŸéªŒè¯**:
1. âœ… é›¶æ•°æ®æµ‹è¯• - GEMM æ­£å¸¸è¿è¡Œ
2. âœ… ç»Ÿä¸€ FP4=1.0, scale=1.0 - Output[0,0] = 2048 (K ç»´åº¦æ­£ç¡®)
3. âœ… FP8 scale è½¬æ¢æ­£ç¡® - PyTorch float8_e4m3fn å·¥ä½œæ­£å¸¸

**å‘ç°çš„é—®é¢˜**:
1. âŒ N ç»´åº¦ç¬¬äºŒåŠåŒºè¾“å‡ºä¸º 0 (N â‰¥ 8192 æ—¶)
2. âŒ B çŸ©é˜µ (ColumnMajor) çš„ scale layout ä¸ A çŸ©é˜µä¸åŒ

### 10.3 æ ¹æœ¬åŸå› åˆ†æ

**A çŸ©é˜µ (RowMajor) vs B çŸ©é˜µ (ColumnMajor)**:

```
A çŸ©é˜µ (RowMajor): SFA layout
B çŸ©é˜µ (ColumnMajor): SFB layout â† éœ€è¦ä¸åŒçš„ swizzle!
```

CUTLASS çš„ `Sm1xxBlkScaledConfig` ä¸º A å’Œ B ç”Ÿæˆä¸åŒçš„ layout:
- `tile_atom_to_shape_SFA()` - ç”¨äº RowMajor A çŸ©é˜µ
- `tile_atom_to_shape_SFB()` - ç”¨äº ColumnMajor B çŸ©é˜µ

å½“å‰ Python `swizzle_scales_for_cutlass()` åªå®ç°äº† A çŸ©é˜µçš„å¸ƒå±€ã€‚

### 10.4 æµ‹è¯•æ•°æ®æ±‡æ€»

| æµ‹è¯• | è¾“å…¥ | æœŸæœ› | å®é™… | çŠ¶æ€ |
|------|------|------|------|------|
| é›¶æ•°æ® | all 0 | 0 | 0 | âœ… |
| FP4=1, scale=1, K=64 | uniform | K | K | âœ… |
| FP4=1, scale=1, M=256,N=256,K=2048 | uniform | K | K (å‰åŠ), 0 (ååŠ) | âŒ |
| FP4=1, scale=1, M=256,N=16384,K=2048 | uniform | K | K (N<8192), 0 (Nâ‰¥8192) | âŒ |

### 10.5 ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. **åˆ†æ SFB layout** - ç ”ç©¶ CUTLASS ColumnMajor B çŸ©é˜µçš„ scale å¸ƒå±€
2. **å®ç° B çŸ©é˜µ swizzle** - æ·»åŠ  `swizzle_scales_for_cutlass_B()` å‡½æ•°
3. **ç«¯åˆ°ç«¯éªŒè¯** - å®Œæ•´ GEMM ç²¾åº¦æµ‹è¯•

### 10.6 ä¸´æ—¶è§£å†³æ–¹æ¡ˆ

**å½“å‰å¯ç”¨**: Python æ¨¡æ‹Ÿæ¨¡å¼ (cos_sim=0.99)
```python
from openpi.models_pytorch.nvfp4_mlp import NVFP4Linear

# ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼
layer = NVFP4Linear.from_linear(linear, use_cutlass=False)
output = layer(x)  # ä½¿ç”¨ Python é‡åŒ– + PyTorch matmul
```

---

## æ›´æ–°æ—¥å¿—

| æ—¥æœŸ | æ›´æ–°å†…å®¹ |
|------|----------|
| 2026-02-08 | åˆå§‹ç‰ˆæœ¬ï¼šå®Œæ•´è®°å½• CUTLASS binary éªŒè¯æˆåŠŸå’Œ C++ extension å¼€å‘è¿›å±• |
| 2026-02-08 | è¯¦ç»†åˆ†æ Scale Factor å¸ƒå±€é—®é¢˜ï¼Œæä¾› Offline Reordering è§£å†³æ–¹æ¡ˆ |
| 2026-02-08 | å®ç° Python swizzle å‡½æ•°ï¼ŒéªŒè¯ç²¾åº¦ (cos_sim=0.99)ï¼›è¯†åˆ« C++ extension FP32/FP8 æ ¼å¼é—®é¢˜ |
| 2026-02-08 | å®ç° gemm_prepared() C++ å‡½æ•°ï¼›å‘ç° B çŸ©é˜µ (ColumnMajor) scale layout é—®é¢˜ |

=====================================================================
åˆ†ææ€»ç»“
ç»è¿‡æ·±å…¥è°ƒæŸ¥ï¼Œä»¥ä¸‹æ˜¯å®Œæ•´çš„å‘ç°ï¼š

1. Scale Type (E4M3 vs E8M0)
ç»“è®º: CUTLASS ä½¿ç”¨ float_ue4m3_t (E4M3)ï¼Œè¿™æ˜¯å¥½æ¶ˆæ¯ - æ¯” E8M0 ç²¾åº¦æ›´é«˜ã€‚

2. é‡åŒ–ç­–ç•¥ä¼˜åŒ–
å®ç°äº† MSE Search ä¼˜åŒ–ï¼Œåœ¨ Python æ¨¡æ‹Ÿå±‚é¢æå‡äº†ç²¾åº¦ (0.989 â†’ 0.991)
ä½†ç”±äº layout é—®é¢˜ï¼ŒCUTLASS ç«¯æ²¡æœ‰å—ç›Š
3. FP8 Scale è½¬æ¢è¯¯å·®

Scale 0.166667 â†’ FP8 â†’ 0.171875 (åå¤§ 3.12%)
ä¸¤ä¸ª scale ç›¸ä¹˜: è¯¯å·®ç´¯ç§¯åˆ° 6.35%
è¿™è§£é‡Šäº† Uniform è¾“å…¥æ—¶è§‚å¯Ÿåˆ°çš„è¯¯å·®ã€‚

4. Layout é—®é¢˜ (æ ¸å¿ƒé—®é¢˜)
ä»è¯¯å·®åˆ†è§£æ¥çœ‹ï¼š

NVFP4 é‡åŒ–è¯¯å·®: 0.87% (10%)
FP8 Scale è¯¯å·®: 0.07% (1%)
Layout/å…¶ä»–è¯¯å·®: 7.79% (89%)
å…³é”®è¯æ®ï¼š

Uniform è¾“å…¥: Python 68.0625 vs CUTLASS 68.0000 (Ratio 0.9991) - å‡ ä¹å®Œç¾åŒ¹é…
Random è¾“å…¥: Cosine sim åªæœ‰ 0.93
è¿™è¯´æ˜ CUTLASS kernel æœ¬èº«å·¥ä½œæ­£ç¡®ï¼Œä½† scale è¢«åº”ç”¨åˆ°äº†é”™è¯¯çš„ä½ç½®ã€‚

5. CUTLASS Scale Layout å¤æ‚æ€§

SfKMajorAtom = Layout<Shape<Shape<_32,_4>, Shape<_16,_4>>,
                      Stride<Stride<_16,_4>, Stride<_0,_1>>>
tile_to_shape(SfAtom, make_shape(M, K, L), Step<_2,_1,_3>)
SFVecSize = 16 for nv_float4_t
K tiles å˜åŒ–æœ€æ…¢
å†…éƒ¨ stride ç»“æ„å¤æ‚ (16,4 è€Œä¸æ˜¯ç®€å•çš„ row-major)

---

## 11. 2026-02-08 Grid Search ç»“æœ

### 11.1 Permutation Grid Search

æ‰§è¡Œäº†æš´åŠ›ç©·ä¸¾æœç´¢ï¼Œæµ‹è¯• **249 ç§ Scale Layout æ’åˆ—ç»„åˆ**ï¼š

| ç³»åˆ— | æè¿° | æµ‹è¯•æ•°é‡ |
|------|------|----------|
| v0 | åŸºç¡€æ–¹æ³• (original, flatten, transpose) | 3 |
| v1 | (32, 4) 4D permute - æ‰€æœ‰ 24 ç§æ’åˆ— | 24 |
| v2 | (32, 16) 4D permute - æ‰€æœ‰ 24 ç§æ’åˆ— | 24 |
| v3 | (128, 4) 4D permute - æ‰€æœ‰ 24 ç§æ’åˆ— | 24 |
| v4 | åµŒå¥—ç»“æ„ | 6 |
| v5 | 3D å½¢çŠ¶å˜ä½“ (32Ã—4Ã—16 ç­‰) | 40 |
| v6 | Stride æ¨¡å¼ (16,4), (4,16), (64,1) ç­‰ | 8 |
| v7 | K-expansion å˜ä½“ | 4 |
| v8 | Block é‡æ’ (32Ã—4, 32Ã—16, 128Ã—4, 128Ã—16) | 96 |
| v9 | Expand + Tile | 20 |
| **æ€»è®¡** | | **249** |

### 11.2 æµ‹è¯•ç»“æœ

```
======================================================================
Testing: M=256, K=128, N=256
======================================================================
Best: v0_original with cos_sim=0.936672

Top 10:
  v0_original                            : 0.936672
  v7_kexpand_repeat_last                 : 0.936672
  v7_kexpand_tile                        : 0.936672
  v9_expand_tile_128x128_perm(0,1,2,3)   : 0.936672
  ...

======================================================================
Testing: M=256, K=2048, N=256
======================================================================
Best: v9_expand_tile_32x32_perm(0,2,1,3) with cos_sim=0.933632

======================================================================
GLOBAL RESULTS
======================================================================
Best permutation: v0_original
Best cosine sim:  0.936672

âœ— No significant improvement found.
```

### 11.3 å…³é”®ç»“è®º

**Scale Layout Permutation ä¸æ˜¯æ ¹æœ¬åŸå› ï¼**

æ‰€æœ‰ 249 ç§æ’åˆ—ç»„åˆéƒ½å¡åœ¨ ~0.93 çš„ cosine similarityï¼Œæ²¡æœ‰ä»»ä½•ä¸€ç§èƒ½çªç ´ 0.95ã€‚

è¿™æ„å‘³ç€ï¼š
1. âŒ Block-level reshape + permute æ— æ³•è§£å†³é—®é¢˜
2. âŒ SfKMajorAtom çš„ (32, 4, 16) ç»“æ„çš„ permute ä¸å¤Ÿ
3. âœ“ é—®é¢˜åœ¨æ›´åº•å±‚ - **Nibble Packing** æˆ– **SfAtom å†…éƒ¨ stride**

### 11.4 æ ¹å› åˆ†æ

æ—¢ç„¶ Scale Layout çš„ block-level permute å…¨å†›è¦†æ²¡ï¼Œå‰©ä½™å¯èƒ½æ€§ï¼š

| å€™é€‰ | å¯èƒ½æ€§ | æè¿° |
|------|--------|------|
| **Nibble Packing** | **é«˜** | 4-bit æ•°æ®çš„é«˜ä½ä½äº¤æ¢ |
| SfAtom å†…éƒ¨ stride | ä¸­ | æ¯ä¸ª scale å…ƒç´ çº§åˆ«çš„äº¤ç»‡ï¼Œä¸æ˜¯ç®€å• permute |
| Data + Scale ç»‘å®š | ä½ | Scale å’Œ data éœ€è¦åŒæ­¥é‡æ’ |

### 11.5 ä¸‹ä¸€æ­¥è¡ŒåŠ¨ - Nibble Order éªŒè¯

**å½“å‰ packing é€»è¾‘ (å‡è®¾)**:
```python
packed_byte = (high_nibble << 4) | low_nibble
```

**å°è¯•æ–¹æ¡ˆ**:
```python
# æ–¹æ¡ˆ A: äº¤æ¢é«˜ä½ 4 ä½
packed_byte = (low_nibble << 4) | high_nibble

# æ–¹æ¡ˆ B: æ¯ 8 ä¸ªå…ƒç´  shuffle
# [e0, e1, e2, e3, e4, e5, e6, e7] â†’ [e4, e0, e5, e1, e6, e2, e7, e3]

# æ–¹æ¡ˆ C: æ¯ 32 ä¸ªå…ƒç´  swizzle (Blackwell Tensor Core)
# 128-bit / 32-byte è¾¹ç•Œå¯¹é½
```

---

## 12. æ–° Plan A: NVFP4 + FP8 æ··åˆç²¾åº¦

### 12.1 æ–¹æ¡ˆè®¾è®¡

å¦‚æœ Nibble ä¿®å¤æˆåŠŸï¼Œé‡‡ç”¨æ··åˆç²¾åº¦ç­–ç•¥ï¼š

| å±‚ | ç­–ç•¥ | ç²¾åº¦é¢„æœŸ | é€Ÿåº¦é¢„æœŸ | åŸå›  |
|----|------|----------|----------|------|
| **Gate_Proj** | NVFP4 | 0.99+ | 5.88x | ç»´åº¦è†¨èƒ€å±‚ï¼Œå¸¦å®½æ”¶ç›Šæœ€é«˜ |
| **Up_Proj** | NVFP4 | 0.99+ | 5.88x | åŒä¸Š |
| **Down_Proj** | FP8 (E4M3) | 0.99+ | 2.00x | ç»´åº¦å‹ç¼©å±‚ï¼Œæœ€æ•æ„Ÿï¼Œç”¨ FP8 å…œåº• |

### 12.2 é¢„æœŸæ”¶ç›Š

| æŒ‡æ ‡ | å…¨ BF16 | æ··åˆ (NVFP4+FP8) | æ”¹è¿› |
|------|---------|------------------|------|
| å•å±‚ MLP | 3.40 ms | ~1.0 ms | ~3.4x |
| 18å±‚ KV Cache | 61.0 ms | ~18 ms | ~3.4x |
| æ¨ç†é¢‘ç‡ | ~8 Hz | ~12-14 Hz | - |

### 12.3 å®ç°è·¯å¾„

1. **Step 1: éªŒè¯ Nibble Order** (å½“å‰é˜»å¡)
   - äº¤æ¢ pack_nvfp4_data() ä¸­çš„é«˜ä½ 4 ä½
   - å¦‚æœæˆåŠŸ â†’ NVFP4 å½»åº•æ‰“é€š

2. **Step 2: å®ç°æ··åˆ MLP**
   ```python
   class HybridMLP(nn.Module):
       def __init__(self):
           self.gate_proj = NVFP4Linear(...)  # CUTLASS NVFP4
           self.up_proj = NVFP4Linear(...)    # CUTLASS NVFP4
           self.down_proj = FP8Linear(...)    # TRT FP8
   ```

3. **Step 3: ç«¯åˆ°ç«¯éªŒè¯**
   - LIBERO ä»»åŠ¡ç²¾åº¦æµ‹è¯•
   - æ¨ç†å»¶è¿Ÿæµ‹è¯•

### 12.4 ä¿åº•æ–¹æ¡ˆ

å¦‚æœ Nibble ä¿®å¤å¤±è´¥ï¼Œé€€å›å…¨ FP8ï¼š

| é…ç½® | å¸¦å®½èŠ‚çœ | KV Cache è€—æ—¶ | æ¨ç†é¢‘ç‡ |
|------|----------|---------------|----------|
| å…¨ BF16 | 0% | 61 ms | ~8 Hz |
| **å…¨ FP8** | **50%** | **~25 ms** | **~10 Hz** |

é…åˆ Pipeline (éšè— Vision)ï¼Œå…¨ FP8 ä¹Ÿèƒ½è¾¾åˆ° 10 Hzï¼Œæ˜¯ç¨³å®šçš„åº•çº¿ã€‚

---

## æ›´æ–°æ—¥å¿—

| æ—¥æœŸ | æ›´æ–°å†…å®¹ |
|------|----------|
| 2026-02-08 | åˆå§‹ç‰ˆæœ¬ï¼šå®Œæ•´è®°å½• CUTLASS binary éªŒè¯æˆåŠŸå’Œ C++ extension å¼€å‘è¿›å±• |
| 2026-02-08 | è¯¦ç»†åˆ†æ Scale Factor å¸ƒå±€é—®é¢˜ï¼Œæä¾› Offline Reordering è§£å†³æ–¹æ¡ˆ |
| 2026-02-08 | å®ç° Python swizzle å‡½æ•°ï¼ŒéªŒè¯ç²¾åº¦ (cos_sim=0.99)ï¼›è¯†åˆ« C++ extension FP32/FP8 æ ¼å¼é—®é¢˜ |
| 2026-02-08 | å®ç° gemm_prepared() C++ å‡½æ•°ï¼›å‘ç° B çŸ©é˜µ (ColumnMajor) scale layout é—®é¢˜ |
| 2026-02-08 | Grid Search å®Œæˆ (249 ç§æ’åˆ—ç»„åˆ)ï¼›ç¡®è®¤ Scale Layout permutation ä¸æ˜¯æ ¹å›  |
| 2026-02-08 | Nibble Order éªŒè¯å®Œæˆ - 8 ç§å˜ä½“ç»“æœç›¸åŒ (0.932927)ï¼›é—®é¢˜åœ¨ CuTe Layout çš„å¤æ‚ç»“æ„ |
| **2026-02-09** | **ğŸ‰ çªç ´æ€§ä¿®å¤ï¼šC++ å®ç° CUTLASS layout é€†å‘æ˜ å°„ï¼Œç²¾åº¦ä» 0.93 â†’ 0.9999** |
| **2026-02-09** | **ä½¿ç”¨ `filter_zeros()` + `get_flat_coord()` å®ç°æ­£ç¡®çš„ scale reordering** |
| **2026-02-09** | **é›†æˆåˆ° nvfp4_mlp.pyï¼ŒNVFP4Linear æ¨¡å—éªŒè¯é€šè¿‡ (cos_sim=0.998)** |

---

## 13. Nibble Order éªŒè¯ç»“æœ

### 13.1 æµ‹è¯•çš„ Nibble Packing å˜ä½“

| å˜ä½“ | æè¿° | Cosine Sim |
|------|------|------------|
| åŸå§‹ | `packed = (high << 4) \| low` | 0.932927 |
| äº¤æ¢ nibbles | `packed = (low << 4) \| high` | 0.932927 |
| äº¤æ¢å–æ · | `low = odd, high = even` | 0.932927 |
| åŒæ—¶äº¤æ¢ | ä¸¤è€…éƒ½äº¤æ¢ | 0.932927 |
| æ¯4å…ƒç´ äº¤ç»‡ | `[0,2,1,3]` | 0.932927 |
| æ¯8å…ƒç´ äº¤ç»‡ | `[0,4,1,5,2,6,3,7]` | 0.932927 |
| æ¯8å…ƒç´ åå‘ | `[4,0,5,1,6,2,7,3]` | 0.932927 |
| æ¯32å…ƒç´ å—äº¤æ¢ | å‰å16äº¤æ¢ | 0.932927 |

### 13.2 ç»“è®º

**æ‰€æœ‰ Nibble å˜ä½“ç»“æœå®Œå…¨ç›¸åŒï¼** Nibble Order ä¹Ÿä¸æ˜¯é—®é¢˜ã€‚

### 13.3 CUTLASS Layout çš„çœŸæ­£å¤æ‚æ€§

åˆ†æ CUTLASS æºç å‘ç°ï¼Œscale layout ä½¿ç”¨ CuTe çš„å¤æ‚ç»“æ„ï¼š

```cpp
// SfAtom - ä¸æ˜¯ç®€å•çš„ reshape+permute èƒ½æ¨¡æ‹Ÿ
Layout<Shape<Shape<_32,_4>, Shape<_16,_4>>,
       Stride<Stride<_16,_4>, Stride<_0,_1>>>  // _0 æ˜¯å¹¿æ’­ï¼

// æ‰©å±•åˆ°å®Œæ•´çŸ©é˜µ
tile_to_shape(SfAtom{}, make_shape(M,K,L), Step<_2,_1,_3>{})
```

å…³é”®ç‚¹ï¼š
1. `Stride<_0,_1>` ä¸­çš„ `_0` è¡¨ç¤º**å¹¿æ’­**ï¼Œä¸æ˜¯çº¿æ€§æ˜ å°„
2. éœ€è¦**é€å…ƒç´ æ˜ å°„**è€Œä¸æ˜¯ç®€å•çš„ tensor permute
3. æˆ–è€…éœ€è¦åœ¨ C++ ä¸­ä½¿ç”¨ CUTLASS çš„ layout è¿­ä»£å™¨

---

## 14. æœ€ç»ˆç»“è®ºä¸æ¨èæ–¹æ¡ˆ

### 14.1 æ’é™¤çš„å¯èƒ½æ€§

| å‡è®¾ | æµ‹è¯•æ•°é‡ | ç»“æœ | ç»“è®º |
|------|----------|------|------|
| Scale Layout Permutation | 249 ç§ | å…¨éƒ¨ ~0.93 | âŒ ä¸æ˜¯é—®é¢˜ |
| Nibble Packing Order | 8 ç§ | å…¨éƒ¨ç›¸åŒ | âŒ ä¸æ˜¯é—®é¢˜ |
| FP32â†’FP8 Scale è½¬æ¢ | - | è¯¯å·® <1% | âŒ ä¸æ˜¯é—®é¢˜ |
| NVFP4 é‡åŒ–æœ¬èº« | - | è¯¯å·® ~1% | âŒ ä¸æ˜¯ä¸»è¦é—®é¢˜ |

### 14.2 çœŸæ­£çš„é—®é¢˜

**CuTe Layout çš„å¤æ‚æ˜ å°„å…³ç³»** - CUTLASS ä½¿ç”¨çš„ `tile_to_shape` + `SfAtom` ç»“æ„æ— æ³•ç”¨ç®€å•çš„ Python reshape/permute æ¨¡æ‹Ÿã€‚

è§£å†³æ–¹æ¡ˆï¼š
1. **åœ¨ C++ ä¸­å®ç°** - ä½¿ç”¨ CUTLASS çš„ layout è¿­ä»£å™¨ç”Ÿæˆæ­£ç¡®çš„ç´¢å¼•æ˜ å°„
2. **åå‘å·¥ç¨‹** - é€å…ƒç´ å¯¹æ¯” CUTLASS æœŸæœ›çš„ä½ç½® vs Python ç”Ÿæˆçš„ä½ç½®

### 14.3 æ¨èæ–¹æ¡ˆï¼šFP8 æ··åˆç²¾åº¦

è€ƒè™‘åˆ°ï¼š
1. NVFP4 çš„ CuTe Layout é—®é¢˜å¤æ‚åº¦é«˜
2. ç”¨æˆ·å·²æœ‰ FP8 + TRT çš„å¯è¡Œæ–¹æ¡ˆ (2.94x åŠ é€Ÿ)
3. FP8 ç²¾åº¦å·²éªŒè¯ (0.99+)

**æ¨èé‡‡ç”¨ FP8 æ–¹æ¡ˆ**ï¼š

| é…ç½® | å¸¦å®½èŠ‚çœ | é€Ÿåº¦ | æ¨ç†é¢‘ç‡ |
|------|----------|------|----------|
| å…¨ BF16 (baseline) | 0% | 61 ms | ~8 Hz |
| **å…¨ FP8** | **50%** | **~25 ms** | **~10 Hz** |
| Pipeline + FP8 | 50% | ~20 ms | **~12 Hz** |

### 14.4 NVFP4 çš„æœªæ¥

å¦‚æœä»éœ€ NVFP4 çš„ 5.88x åŠ é€Ÿï¼Œéœ€è¦ï¼š
1. åœ¨ C++ ä¸­ä½¿ç”¨ CUTLASS layout è¿­ä»£å™¨å®ç° scale reordering
2. æˆ–ç­‰å¾… NVIDIA æä¾›æ›´æ¸…æ™°çš„æ–‡æ¡£/ç¤ºä¾‹

---

## 15. ğŸ‰ 2026-02-09 çªç ´æ€§ä¿®å¤ï¼šC++ CUTLASS Layout æ˜ å°„

### 15.1 è§£å†³æ–¹æ¡ˆ

ä½¿ç”¨ **CUTLASS CuTe çš„ `filter_zeros()` + `get_flat_coord()` å®ç°é€†å‘æ˜ å°„**ï¼š

```cpp
// å…³é”®ä»£ç  (nvfp4_gemm.cu)
auto layout_filtered = filter_zeros(layout_SF);

for (size_t dst_idx = 0; dst_idx < total_size; dst_idx++) {
    // è·å–é€»è¾‘åæ ‡
    auto coord = layout_filtered.get_flat_coord(dst_idx);
    int m = get<0>(coord);
    int k_filtered = get<1>(coord);

    // è®¡ç®—æºç´¢å¼•
    int k_block = k_filtered * SFVecSize / block_size;  // 16/32 = 0.5
    int src_idx = m * num_k_blocks + k_block;

    dst_ptr[dst_idx] = src_ptr[src_idx];
}
```

### 15.2 å…³é”®å‘ç°

é€šè¿‡ `debug_print_layout()` å‡½æ•°æ­ç¤ºäº† CUTLASS layout çš„çœŸå®ç»“æ„ï¼š

```
Layout structure:
(((_32,_4),2),((_16,_4),2),(_1,1)):(((_16,_4),1024),((_0,_1),_512),(_0,2048))

Linear indices and coordinates (filtered layout):
  idx 0  -> (0, 0, 0)   // m=0, k=0
  idx 1  -> (0, 1, 0)   // m=0, k=1
  idx 4  -> (32, 0, 0)  // m=32, k=0  <- M äº¤é”™å­˜å‚¨ï¼
  idx 16 -> (1, 0, 0)   // m=1, k=0
```

**å…³é”®æ´å¯Ÿ**ï¼š
1. **Broadcast ç»´åº¦** (`Stride<_0, _1>`)ï¼šK ç»´åº¦æœ‰ stride 0ï¼Œå¤šä¸ªä½ç½®å…±äº«åŒä¸€ scale
2. **äº¤é”™ M å­˜å‚¨**ï¼šM ä»¥ (0, 32, 64, 96, 1, 33, 65, 97, ...) æ¨¡å¼å­˜å‚¨
3. **K åŸå­ç»“æ„**ï¼šæ¯ä¸ªåŸå­åªæœ‰ 4 ä¸ªå”¯ä¸€ K ä½ç½®

### 15.3 ç²¾åº¦ç»“æœ

| æµ‹è¯•é…ç½® | Cosine vs Python Ref | Cosine vs BF16 | çŠ¶æ€ |
|----------|---------------------|----------------|------|
| M=128, K=1024, N=128 | **0.999999** | 0.989101 | âœ… |
| M=256, K=2048, N=256 | **0.999999** | 0.989321 | âœ… |
| M=512, K=4096, N=512 | **0.999999** | 0.989140 | âœ… |
| M=1024, K=2048, N=1024 | **0.999999** | 0.989185 | âœ… |
| NVFP4Linear æ¨¡å— | **0.997766** | - | âœ… |

**ä» 0.93 â†’ 0.9999 çš„çªç ´ï¼**

### 15.4 ä¿®æ”¹çš„æ–‡ä»¶

1. **`nvfp4_gemm.cu`** - æ–°å¢ `reorder_scales_cutlass()` å‡½æ•°
   - ä½¿ç”¨ `filter_zeros(layout_SF)` ç§»é™¤å¹¿æ’­ç»´åº¦
   - ä½¿ç”¨ `get_flat_coord(dst_idx)` è·å–é€»è¾‘åæ ‡
   - æ­£ç¡®è®¡ç®— `k_filtered â†’ k_block` æ˜ å°„

2. **`nvfp4_mlp.py`** - æ›´æ–° `prepare_scales_for_cutlass()`
   - æ·»åŠ  `is_weight` å‚æ•°åŒºåˆ† SFA/SFB layout
   - è°ƒç”¨ C++ `nvfp4_gemm.reorder_scales()` å‡½æ•°

### 15.5 æ¥å£å˜æ›´

```python
# æ–°æ¥å£
prepare_scales_for_cutlass(
    scales,
    M,
    num_k_blocks,
    convert_to_fp8=True,
    K=K,
    is_weight=False  # æ–°å‚æ•°ï¼šTrue=SFB layout, False=SFA layout
)
```

### 15.6 ä¸‹ä¸€æ­¥

1. âœ… C++ CUTLASS layout æ˜ å°„å®ç°
2. âœ… å•å…ƒæµ‹è¯•éªŒè¯ (å¤šç§çŸ©é˜µå¤§å°)
3. âœ… é›†æˆåˆ° NVFP4Linear æ¨¡å—
4. âœ… ç²¾åº¦éªŒè¯é€šè¿‡ (è§ Section 16)
5. ğŸ”„ æ€§èƒ½ä¼˜åŒ– (åœ¨çº¿é‡åŒ–ç“¶é¢ˆ)

---

## 16. ç²¾åº¦éªŒè¯ä¸æ€§èƒ½åˆ†æ

### 16.1 ç²¾åº¦éªŒè¯ç»“æœ

ä½¿ç”¨ `validate_nvfp4_precision.py` è¿›è¡Œå…¨é¢ç²¾åº¦æµ‹è¯•ï¼š

| æµ‹è¯•é¡¹ | Cosine Similarity | çŠ¶æ€ |
|--------|------------------|------|
| **CUTLASS vs Python Sim** | **0.996** | âœ… PASS |
| CUTLASS vs BF16 | 0.987 | âœ… PASS |
| **NVFP4 MLP vs BF16 MLP** | **0.963** | âœ… PASS |

**ç»“è®º**ï¼šScale Layout ä¿®å¤æˆåŠŸï¼ŒCUTLASS ä¸ Python æ¨¡æ‹Ÿé«˜åº¦ä¸€è‡´ã€‚

### 16.2 åœ¨çº¿é‡åŒ–ç“¶é¢ˆåˆ†æ

**é—®é¢˜å‘ç°**ï¼šå®Œæ•´æ¨ç†åªæœ‰ ~0.13 Hz (7.4 ç§’/iteration)

**æ—¶é—´åˆ†è§£** (å•å±‚ NVFP4Linear, batch=256):

| æ“ä½œ | è€—æ—¶ | ä½ç½® |
|------|------|------|
| `quantize_to_nvfp4_sim` (æ¿€æ´»é‡åŒ–) | 7.59 ms | `forward()` âŒ |
| `pack_nvfp4_data` | 0.60 ms | `forward()` |
| `prepare_scales_for_cutlass` | 0.31 ms | `forward()` |
| `nvfp4_gemm.gemm` (CUTLASS) | **0.24 ms** | `forward()` âœ… |

**æ ¹å› **ï¼šæƒé‡å·²ç¦»çº¿é‡åŒ– (åœ¨ `__init__`)ï¼Œä½†**æ¿€æ´»å€¼æ¯æ¬¡ forward éƒ½åœ¨ç”¨ Python é‡åŒ–**ã€‚

### 16.3 è§£å†³æ–¹æ¡ˆ

| æ–¹æ¡ˆ | æè¿° | é¢„æœŸé€Ÿåº¦ |
|------|------|----------|
| **W4A16** | åªé‡åŒ–æƒé‡ï¼Œæ¿€æ´»ä¿æŒ BF16 | ~2ms/layer |
| **W4A4 + CUDA Kernel** | å†™ CUDA kernel åšå¿«é€Ÿæ¿€æ´»é‡åŒ– | ~0.3ms/layer |

**æ¨è**ï¼šå…ˆå°è¯• W4A16ï¼Œå¦‚æœ CUTLASS kernel æ”¯æŒ BF16 è¾“å…¥ã€‚

### 16.4 å½“å‰çŠ¶æ€

```
âœ… Scale Layout ä¿®å¤ - ç²¾åº¦ä» 0.93 â†’ 0.996
âœ… NVFP4Linear æ¨¡å—é›†æˆ
âœ… å•å±‚ç²¾åº¦éªŒè¯é€šè¿‡ (CUTLASS vs Sim: 0.999)
âŒ å®Œæ•´æ¨¡å‹ç²¾åº¦ä¸è¶³ - è§ Section 17
```

---

## 17. NVFP4 å®Œæ•´æ¨¡å‹è¯„ä¼°ç»“æœ

### 17.1 å‘ç°çš„é—®é¢˜ï¼šFP8 Scale Overflow

**æ ¹å› **ï¼šå½“æ¿€æ´»å€¼å¾ˆå¤§æ—¶ï¼Œscale è¶…è¿‡ FP8 E4M3 çš„è¡¨ç¤ºèŒƒå›´ã€‚

| å‚æ•° | å€¼ |
|------|-----|
| FP8 E4M3 æœ€å¤§å€¼ | 448 |
| NVFP4 æœ€å¤§å€¼ | 6 |
| Scale æº¢å‡ºé˜ˆå€¼ | è¾“å…¥ > 448 Ã— 6 = **2688** |
| å®é™…è§‚æµ‹åˆ°çš„æ¿€æ´»å€¼ | **-5248 ~ 430** (layer 16) |
| å®é™… scale | **874.7** (è¶…è¿‡ FP8 èŒƒå›´!) |

### 17.2 ä¿®å¤å°è¯•

æ·»åŠ äº†æ¿€æ´»å€¼ clamp é˜²æ­¢æº¢å‡ºï¼š
```python
# nvfp4_mlp.py
FP8_SCALE_MAX = 448.0 * NVFP4_MAX  # 2688
x_2d = x_2d.clamp(-FP8_SCALE_MAX, FP8_SCALE_MAX)
```

**ç»“æœ**ï¼šNaN é—®é¢˜è§£å†³ï¼Œä½†ç²¾åº¦å¤§å¹…ä¸‹é™ã€‚

### 17.3 ç²¾åº¦æµ‹è¯•ç»“æœ

| é…ç½® | NVFP4 vs BF16 Cosine | çŠ¶æ€ |
|------|----------------------|------|
| å•å±‚ NVFP4Linear (å°è¾“å…¥) | **0.996** | âœ… |
| å®Œæ•´æ¨¡å‹ (18 å±‚) | **-0.11** | âŒ |

**ç»“è®º**ï¼šclamp æ“ä½œå¯¼è‡´ä¿¡æ¯æŸå¤±ï¼Œ4-bit ç²¾åº¦å¯¹ Diffusion Policy ç´¯ç§¯è¯¯å·®è¿‡å¤§ã€‚

### 17.4 NVFP4 ä¸é€‚ç”¨äºæ­¤æ¨¡å‹çš„åŸå› 

1. **åŠ¨æ€èŒƒå›´é—®é¢˜**ï¼šDiffusion Policy çš„ä¸­é—´æ¿€æ´»å€¼èŒƒå›´å¾ˆå¤§ (å¯è¾¾ Â±5000)
2. **ç²¾åº¦æ•æ„Ÿ**ï¼šDiffusion å»å™ªè¿‡ç¨‹å¯¹æ•°å€¼ç²¾åº¦è¦æ±‚é«˜
3. **ç´¯ç§¯è¯¯å·®**ï¼š18 å±‚ MLP çš„é‡åŒ–è¯¯å·®ç´¯ç§¯

### 17.5 æœ€ç»ˆå»ºè®®

**æ”¾å¼ƒ NVFP4ï¼Œä½¿ç”¨ FP8 æ–¹æ¡ˆ**ï¼š

| æ–¹æ¡ˆ | åŠ¨æ€èŒƒå›´ | ç²¾åº¦ | å¸¦å®½èŠ‚çœ | æ¨è |
|------|----------|------|----------|------|
| **FP8 (E4M3)** | Â±448 | å·²éªŒè¯ | 50% | âœ… **æ¨è** |
| NVFP4 (E2M1) | Â±6 (éœ€ scale) | ä¸è¶³ | 75% | âŒ ä¸æ¨è |

FP8 ä¼˜åŠ¿ï¼š
- åŠ¨æ€èŒƒå›´è¶³å¤Ÿ (æ— éœ€ clamp)
- ç²¾åº¦å·²éªŒè¯é€šè¿‡
- TensorRT æ”¯æŒæˆç†Ÿ
- å¸¦å®½èŠ‚çœ 50% è¶³å¤Ÿè¾¾åˆ°ç›®æ ‡é¢‘ç‡

---

## é™„å½• D: å…³é”®å‘ç°æ—¶é—´çº¿

| æ—¶é—´ | å‘ç° |
|------|------|
| Day 1 | CUTLASS binary 5.88x åŠ é€ŸéªŒè¯æˆåŠŸ |
| Day 1 | Scale Factor FP32â†’FP8 ç±»å‹é—®é¢˜ä¿®å¤ |
| Day 1 | å‘ç° Scale Layout é—®é¢˜ (~0.93 cosine) |
| Day 2 | è¯¯å·®åˆ†è§£ï¼š89% æ¥è‡ª Layout |
| Day 2 | Grid Search (249 ç§) - Scale permute ä¸æ˜¯é—®é¢˜ |
| Day 2 | Nibble Order (8 ç§) - ä¹Ÿä¸æ˜¯é—®é¢˜ |
| Day 2 | ç¡®è®¤é—®é¢˜åœ¨ CuTe Layout çš„å¤æ‚æ˜ å°„ |
| **Day 3** | **ğŸ‰ C++ CUTLASS layout æ˜ å°„å®ç° - ç²¾åº¦çªç ´ 0.999!** |
| Day 3 | ä½¿ç”¨ `filter_zeros()` + `get_flat_coord()` å®ç°é€†å‘æ˜ å°„ |
| Day 3 | é›†æˆåˆ° `nvfp4_mlp.py` å¹¶éªŒè¯ NVFP4Linear æ¨¡å— |
| Day 3 | âœ… å•å±‚ç²¾åº¦éªŒè¯é€šè¿‡ (CUTLASS vs Sim: 0.996) |
| Day 3 | å‘ç°åœ¨çº¿é‡åŒ–ç“¶é¢ˆ (æ¿€æ´»é‡åŒ–å ç”¨ 7.59ms) |
| Day 3 | å‘ç° FP8 Scale Overflow é—®é¢˜ (æ¿€æ´»å€¼ Â±5248 > é˜ˆå€¼ 2688) |
| Day 3 | âŒ å®Œæ•´æ¨¡å‹ç²¾åº¦ä¸è¶³ (NVFP4 vs BF16: -0.11 cosine) |
| **Day 3** | **æœ€ç»ˆç»“è®ºï¼šNVFP4 ä¸é€‚ç”¨äº Diffusion Policyï¼Œæ¨èä½¿ç”¨ FP8** |
