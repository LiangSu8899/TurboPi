# æ™ºå…ƒ Pi0.5 22Hz ä¼˜åŒ–åˆ†æ

**Date**: 2026-02-03
**åŸºäº**: æ™ºå…ƒå…¬å¼€åˆ†äº« + æˆ‘ä»¬çš„å®éªŒéªŒè¯

---

## 1. æ™ºå…ƒå…¬å¼€ä¿¡æ¯

### 1.1 ä¼˜åŒ–æˆæœ
- **èµ·ç‚¹**: 7 Hz (141.96 ms)
- **ç»ˆç‚¹**: 22 Hz (~45 ms)
- **æ€»åŠ é€Ÿ**: 3.15x

### 1.2 å…¬å¼€çš„æŠ€æœ¯ç‚¹
1. å…¨æ ˆ Jetson Thor æ¨ç†ç³»ç»Ÿï¼ŒJAX/PyTorch æ¨ç†åŠè½¬æ¢
2. æ‰¹é‡å¤„ç†æå‡ç‰¹å¾æå–æ•ˆç‡
3. AE (Action Expert) è®¡ç®—é€»è¾‘é‡æ„ä¿ƒè¿›è®¡ç®—èåˆ
4. PyTorch ç¼–è¯‘ + Triton åŠ é€Ÿï¼ˆå‰æœŸéªŒè¯ï¼‰
5. è‡ªç ”ç¼–è¯‘å™¨ + TVM + TensorRTï¼ˆæœ€ç»ˆæ–¹æ¡ˆï¼‰
6. FP8/nvFP4 ä½ç²¾åº¦é‡åŒ–ï¼ˆè´¡çŒ® 7.04 Hzï¼‰
7. ä¸šåŠ¡åœºæ™¯è£å‡ä¼˜åŒ–ï¼ˆè´¡çŒ® 7.81 Hzï¼‰

---

## 2. æˆ‘ä»¬çš„å®éªŒéªŒè¯

### 2.1 MLP é‡åŒ–å®éªŒç»“è®º

| Mode | Latency | Speedup | é—®é¢˜ |
|------|---------|---------|------|
| FP16 full | 3.47 ms | 1.00x | baseline |
| FP8 full | 4.11 ms | 0.84x | hidden quant å¼€é”€ 2ms |
| **FP8 hybrid** | 3.09 ms | 1.12x | æ­£ç¡®æ–¹æ¡ˆ |

**å…³é”®å‘ç°**:
- å• GEMM FP8 æœ‰ 1.75x åŠ é€Ÿ
- ä½† hidden tensor (970Ã—16384) é‡åŒ–å¼€é”€å®Œå…¨æŠµæ¶ˆæ”¶ç›Š
- **æ™ºå…ƒå¿…ç„¶ä¹Ÿå‘ç°äº†è¿™ä¸ªé—®é¢˜ï¼Œæ‰€ä»¥ç”¨ hybrid æˆ–å…¶ä»–æ–¹æ¡ˆ**

### 2.2 Attention å®éªŒç»“è®º

| Backend | KV Cache Latency | Speedup |
|---------|------------------|---------|
| PyTorch FP16 | 87.65 ms | 1.00x |
| Torch-TRT FP16 | 59.84 ms | 1.46x |
| Torch-TRT FP8 | 60.03 ms | 1.46x (FP8 æœªç”Ÿæ•ˆ) |

**å…³é”®å‘ç°**:
- Torch-TRT FP8 å£°æ˜æ— æ•ˆï¼Œéœ€è¦æ‰‹åŠ¨æ’å…¥ Q/DQ
- Attention ä»æ˜¯æœ€å¤§ç“¶é¢ˆ (~40% æ—¶é—´)
- **æ™ºå…ƒå¿…ç„¶æœ‰å®šåˆ¶ Attention kernel**

### 2.3 Reformat å®éªŒç»“è®º

- FP8 â†” FP16 æ··åˆåœ¨ Thor ä¸Šè§¦å‘å¤§é‡è‡ªåŠ¨ reformat
- Thor æ˜¯ 16/32 å­—èŠ‚å¯¹é½ï¼Œç²¾åº¦è½¬æ¢å¯¼è‡´å¸¦å®½ç¾éš¾
- **æ™ºå…ƒé€šè¿‡ TVM é™æ€å›¾æ¶ˆé™¤ reformat**

---

## 3. æ™ºå…ƒ"æ²¡è¯´çš„ç»†èŠ‚"ï¼ˆå·¥ç¨‹æ¨æ–­ï¼‰

### 3.1 Attention ä¼˜åŒ–ï¼ˆæœ€å¤§æ”¶ç›Šï¼‰

ä»–ä»¬ä¸€å®šåšäº†ï¼š

```python
# å®šåˆ¶ Attention Kernel (ç±»ä¼¼ FlashAttention)
class FusedAttention:
    def forward(self, Q, K, V):
        # 1. QK^T åœ¨ FP16/BF16 è®¡ç®—
        # 2. Softmax åœ¨ FP32 ç´¯åŠ ï¼ˆæ•°å€¼ç¨³å®šï¼‰
        # 3. Attention @ V åœ¨ FP16 è¾“å‡º
        # 4. æ— ä¸­é—´ tensor å†™å› global memory
```

**è¯æ®**:
- å›¾è¡¨ä¸­ MHA ä¼˜åŒ–å¹…åº¦ > FC
- æˆ‘ä»¬å®éªŒè¯æ˜ FP32 Softmax æ˜¯ç²¾åº¦å…³é”®
- Thor Triton æ”¯æŒå†™ fused kernel

### 3.2 nvFP4 MLPï¼ˆç¬¬äºŒå¤§æ”¶ç›Šï¼‰

ä»–ä»¬ä¸€å®šç»•è¿‡äº† TRT Myelin crashï¼š

```python
# æ–¹æ¡ˆ A: TVM è‡ªç ” kernel
# - ç”¨ TVM TensorIR å†™ FP4 matmul
# - ä¸èµ° TRT çš„ Myelin ä¼˜åŒ–

# æ–¹æ¡ˆ B: Triton kernel
# - torch._scaled_mm ä¸æ”¯æŒ FP4
# - ç›´æ¥å†™ Triton FP4 GEMM kernel

# æ–¹æ¡ˆ C: NVIDIA å†…éƒ¨ workaround
# - æ™ºå…ƒå¯èƒ½æœ‰ NVIDIA å·¥ç¨‹å¸ˆæ”¯æŒ
# - è·å¾—äº† Myelin crash çš„ fix
```

**è¯æ®**:
- æˆ‘ä»¬å®éªŒè¯æ˜ TRT åŸç”Ÿ FP4/INT4 ä¼š crash
- ä»–ä»¬å…¬å¼€å£°ç§°ç”¨äº† nvFP4
- å”¯ä¸€è§£é‡Šæ˜¯ç»•è¿‡ TRT æˆ–æœ‰å†…éƒ¨ fix

### 3.3 Reformat æ¶ˆé™¤

ä»–ä»¬ä¸€å®šåšäº†é™æ€å›¾ä¼˜åŒ–ï¼š

```
TVM å·¥ä½œæµ:
ONNX/PyTorch â†’ Relay IR â†’ Graph Optimization â†’ TensorIR â†’ CUDA Kernel

å…³é”®ä¼˜åŒ–:
1. Cast åˆå¹¶: å¤šä¸ª FP8â†”FP16 cast åˆå¹¶
2. Layout å›ºå®š: ç¼–è¯‘æœŸå†³å®šï¼Œè¿è¡ŒæœŸæ—  reformat
3. Kernel fusion: å¤šç®—å­ç¼–è¯‘æˆå•ä¸€ kernel
```

**è¯æ®**:
- ä»–ä»¬å…¬å¼€å£°ç§°ç”¨äº† TVM
- æˆ‘ä»¬å®éªŒè¯æ˜ reformat æ˜¯å¸¦å®½ç“¶é¢ˆ
- TVM çš„ Relay/TensorIR å°±æ˜¯å¹²è¿™ä¸ªçš„

### 3.4 ä¸šåŠ¡åœºæ™¯è£å‡ (7.81 Hz)

å¯èƒ½åŒ…æ‹¬ï¼š

1. **Action Chunking ä¼˜åŒ–**
   - ä¸æ˜¯æ¯å¸§éƒ½è·‘å®Œæ•´æ¨ç†
   - éƒ¨åˆ†å¸§å¤ç”¨å‰ä¸€å¸§ action

2. **Vision ç¼“å­˜**
   - é™æ€åœºæ™¯éƒ¨åˆ† ViT ç»“æœå¯ç¼“å­˜
   - åªæœ‰ wrist ç›¸æœºéœ€è¦æ¯å¸§æ›´æ–°

3. **æ¨¡å‹è’¸é¦/å‰ªæ**
   - ç”¨ 22B å¤§æ¨¡å‹è’¸é¦ 0.5B
   - å‡å°‘ layers æˆ– hidden size

4. **ä»»åŠ¡ç‰¹å®šè£å‰ª**
   - LIBERO åªéœ€è¦ 7DoF
   - æŸäº› attention heads å¯å‰ªæ

**è¿™éƒ¨åˆ†ç¡®å®éœ€è¦å†…éƒ¨æ•°æ®æ ¡å‡†ï¼Œæ— æ³•ç›´æ¥å¤åˆ¶**

---

## 4. ä¼˜åŒ–æ”¶ç›Šåˆ†è§£ï¼ˆæ¨ç®—ï¼‰

| ä¼˜åŒ–é¡¹ | å»¶è¿Ÿæ”¶ç›Š | Hz æ”¶ç›Š | ç´¯è®¡ Hz |
|--------|----------|---------|---------|
| èµ·ç‚¹ | 141.96 ms | 7 Hz | 7 Hz |
| Attention fusion | -30 ms | +3 Hz | 10 Hz |
| FP8 MLP | -15 ms | +2 Hz | 12 Hz |
| nvFP4 MLP | -10 ms | +2 Hz | 14 Hz |
| Reformat æ¶ˆé™¤ | -10 ms | +1.5 Hz | 15.5 Hz |
| ä¸šåŠ¡è£å‡ | - | +6.5 Hz | 22 Hz |

**æ³¨**:
- Hz æ”¶ç›Šä¸æ˜¯çº¿æ€§çš„ï¼Œå»¶è¿Ÿè¶Šä½ï¼ŒHz æ”¶ç›Šè¶Šå¤§
- ä¸šåŠ¡è£å‡çš„ 6.5 Hz å¯èƒ½åŒ…å« KV cache reuse

---

## 5. æˆ‘ä»¬çš„å·®è·åˆ†æ

### 5.1 å½“å‰çŠ¶æ€

| é˜¶æ®µ | æˆ‘ä»¬ | æ™ºå…ƒ |
|------|------|------|
| PyTorch baseline | 87 ms (11 Hz) | 142 ms (7 Hz) |
| å½“å‰æœ€ä¼˜ | 60 ms (17 Hz) | 45 ms (22 Hz) |
| å·®è· | - | 5 Hz |

**æ³¨**: æˆ‘ä»¬çš„ baseline æ¯”æ™ºå…ƒå¿«ï¼Œå¯èƒ½å› ä¸ºå®ç°å·®å¼‚

### 5.2 å·®è·æ¥æº

```
17 Hz â†’ 22 Hz å·®è·åˆ†æ:

1. Attention kernel (-10 ms): æˆ‘ä»¬æ²¡åš
2. nvFP4 (-5 ms): TRT crash æœªè§£å†³
3. Reformat æ¶ˆé™¤ (-5 ms): éœ€è¦é™æ€å›¾
4. ä¸šåŠ¡è£å‡ (+N Hz): éœ€è¦æ•°æ®æ”¯æŒ

æ€»è®¡: ~20 ms + ä¸šåŠ¡ä¼˜åŒ–
```

---

## 6. ä¸‹ä¸€æ­¥è¡ŒåŠ¨å»ºè®®

### 6.1 ä¼˜å…ˆçº§ 1: Attention Kernel (æœ€å¤§æ”¶ç›Š)

**è¡ŒåŠ¨**: å®ç° FP16 + FP32 Acc çš„ fused attention

```python
# é€‰é¡¹ A: cuDNN Fused Attention
torch.backends.cuda.enable_flash_sdp(True)
torch.backends.cuda.enable_cudnn_sdp(True)

# é€‰é¡¹ B: Triton kernel
@triton.jit
def fused_attention_kernel(...):
    # QK^T in FP16
    # Softmax acc in FP32
    # V @ Attention in FP16

# é€‰é¡¹ C: xformers
from xformers.ops import memory_efficient_attention
```

**é¢„æœŸæ”¶ç›Š**: 10-15 ms

### 6.2 ä¼˜å…ˆçº§ 2: Reformat æ¶ˆé™¤

**è¡ŒåŠ¨**: ç»Ÿä¸€æ•°æ®å¸ƒå±€ï¼Œé¿å… FP8â†”FP16 è½¬æ¢

```python
# æ–¹æ¡ˆ 1: çº¯ FP16 pipeline
# - Attention: FP16
# - MLP: FP16 (æ”¾å¼ƒ FP8)
# - é¿å… reformat

# æ–¹æ¡ˆ 2: TVM é™æ€å›¾
# - ç”¨ TVM ç¼–è¯‘æ•´ä¸ª KV cache model
# - ç¼–è¯‘æœŸå›ºå®š layout

# æ–¹æ¡ˆ 3: æ‰‹å†™ CUDA kernel
# - åœ¨ kernel å†…éƒ¨å¤„ç†ç²¾åº¦è½¬æ¢
# - æ—  global memory reformat
```

**é¢„æœŸæ”¶ç›Š**: 5-10 ms

### 6.3 ä¼˜å…ˆçº§ 3: nvFP4 (å¦‚æœèƒ½è§£å†³ crash)

**è¡ŒåŠ¨**: æŠ¥å‘Šç»™ NVIDIA æˆ–å°è¯•ç»•è¿‡

```python
# é€‰é¡¹ A: æŠ¥å‘Š Myelin crash
# - æä¾›å¤ç°æ­¥éª¤
# - ç­‰å¾… TRT 10.15+ ä¿®å¤

# é€‰é¡¹ B: TVM FP4 kernel
# - ç”¨ TVM TensorIR å†™ FP4 matmul
# - ä¸ä¾èµ– TRT

# é€‰é¡¹ C: Triton FP4
# - å‚è€ƒ NVIDIA cutlass fp4 å®ç°
# - å†™ Triton kernel
```

**é¢„æœŸæ”¶ç›Š**: 10 ms (å¦‚æœæˆåŠŸ)

### 6.4 ä¼˜å…ˆçº§ 4: ä¸šåŠ¡ä¼˜åŒ–

**è¡ŒåŠ¨**: éœ€è¦è¯„ä¼°å“ªäº›å¯ä»¥ä¸ä¾èµ–å†…éƒ¨æ•°æ®

1. **KV Cache Reuse** - æˆ‘ä»¬å·²å®ç°
2. **Vision ç¼“å­˜** - å¯ä»¥å®ç°
3. **Action Chunking** - å¯ä»¥å®ç°
4. **æ¨¡å‹å‰ªæ** - éœ€è¦é‡æ–°è®­ç»ƒ

---

## 7. å¯è¡Œæ€§è¯„ä¼°

| ä¼˜åŒ–é¡¹ | æŠ€æœ¯éš¾åº¦ | ä¾èµ– | é¢„æœŸæ”¶ç›Š | å»ºè®® |
|--------|----------|------|----------|------|
| Attention kernel | â­â­â­ | Triton/xformers | 10-15 ms | **ç«‹å³åš** |
| Reformat æ¶ˆé™¤ | â­â­â­ | TVM æˆ–çº¯ FP16 | 5-10 ms | **ç¬¬äºŒä¼˜å…ˆ** |
| nvFP4 | â­â­â­â­ | NVIDIA fix | 10 ms | ç­‰å¾…/ç»•è¿‡ |
| ä¸šåŠ¡è£å‡ | â­â­ | æ•°æ® | N Hz | éƒ¨åˆ†å¯åš |

**ç»“è®º**:
- ä¸åš Attention kernelï¼Œæˆ‘ä»¬å¾ˆéš¾æ¥è¿‘ 22 Hz
- Reformat æ¶ˆé™¤æ˜¯ç¬¬äºŒå…³é”®
- nvFP4 å¯ä»¥æš‚æ—¶æç½®ï¼Œæ”¶ç›Šæœ‰é™ä¸”é˜»å¡

---

## 8. 22 Hz è·¯å¾„è§„åˆ’

```
å½“å‰: 60 ms (17 Hz)
     â”‚
     â–¼ Attention kernel (-12 ms)
     â”‚
48 ms (21 Hz)
     â”‚
     â–¼ Reformat æ¶ˆé™¤ (-5 ms)
     â”‚
43 ms (23 Hz) âœ… è¶…è¶Šæ™ºå…ƒ
     â”‚
     â–¼ nvFP4 (å¯é€‰, -8 ms)
     â”‚
35 ms (28 Hz)
     â”‚
     â–¼ ä¸šåŠ¡ä¼˜åŒ–
     â”‚
30 ms (33 Hz) ğŸš€
```

---

*Last Updated: 2026-02-03*
