# TVM + TensorRT FP4 Kernel ä¼˜åŒ–è®¡åˆ’

> **ç›®æ ‡**: ç”¨ "TensorIR è½¯ä»¶ kernel + TRT plugin" åœ¨ M=1 GEMV ä¸Šè·‘èµ¢ FP8 Tensor Core
>
> **å½“å‰çŠ¶æ€**: âœ… **é‡å¤§çªç ´ï¼** W4A16 Packed FP4 kernel å®ç° **2.45x åŠ é€Ÿ** vs TRT FP8
>
> **æ—¥æœŸ**: 2026-02-10
>
> **æœ€æ–°æ›´æ–°**: 2026-02-10 - å®Œæ•´ MLP å±‚ W4A16 kernel å®ç°ï¼ŒK-dimension tiling ä¼˜åŒ–

---

## ğŸ‰ğŸ‰ é‡å¤§çªç ´ (2026-02-10 æœ€æ–°)

### å®Œæ•´ MLP å±‚ W4A16 Packed FP4 Kernel

æˆ‘ä»¬å®ç°äº†å®Œæ•´çš„ MLP å±‚ W4A16 kernelï¼Œåœ¨ Thor SM110 ä¸Šå®ç°äº† **2.45x åŠ é€Ÿ**ï¼

#### å•å±‚ GEMM æ€§èƒ½

| GEMM | Dimensions (NÃ—K) | W4A16 Fast | TRT FP8 | Speedup | æ­£ç¡®æ€§ |
|------|------------------|------------|---------|---------|--------|
| gate_proj | 16384Ã—2048 | **0.224ms** | 0.53ms | **2.37x** | âœ… cos=1.0 |
| up_proj | 16384Ã—2048 | **0.224ms** | 0.53ms | **2.37x** | âœ… cos=1.0 |
| down_proj | 2048Ã—16384 | **0.202ms** | 0.53ms | **2.62x** | âœ… cos=1.0 |
| **MLP Total** | - | **0.65ms** | 1.59ms | **2.45x** | âœ… |

#### 18å±‚ MLP é¢„æœŸæ”¶ç›Š

| é…ç½® | å•å±‚ MLP | 18å±‚æ€»è®¡ | ç›¸å¯¹åŠ é€Ÿ |
|------|----------|----------|----------|
| TRT FP8/BF16 (å®æµ‹) | ~1.13ms | ~20.4ms | 1.00x |
| **W4A16 Packed (é¢„æœŸ)** | ~0.65ms | **~11.7ms** | **1.74x** |

#### å†…å­˜èŠ‚çœ

| æŒ‡æ ‡ | BF16 | W4A16 Packed | å‹ç¼©æ¯” |
|------|------|--------------|--------|
| å•å±‚æƒé‡ | 134 MB | **17 MB** | **8x** |
| 18å±‚æ€»è®¡ | 2.4 GB | **0.3 GB** | **8x** |

### å…³é”®æŠ€æœ¯å®ç°

```
W4A16 Packed FP4 GEMV (Fast Version):
- æƒé‡: uint8 packed (2 FP4 values per byte)
- æ¿€æ´»: float32
- è®¡ç®—: In-register dequant + CUDA Core accumulation
- Reduction: Shared memory parallel reduction

Thread Block Organization:
- 256 threads per block
- 4 outputs per block (64 threads per output)
- K-dimension tiling for large K (TILE_K = 2048)
- 6-step log2 parallel reduction (64â†’32â†’16â†’8â†’4â†’2â†’1)
```

### nvFP4 E2M1 æ ¼å¼

```
4-bit encoding: [sign][exp1][exp0][mantissa]

LUT values (16 entries):
  0x0-0x7: [0, 0.5, 1, 1.5, 2, 3, 4, 6]       # Positive
  0x8-0xF: [0, -0.5, -1, -1.5, -2, -3, -4, -6] # Negative

Block scaling: scale_per_32_elements
```

### ä»£ç ä½ç½®

**TVM Kernel å®ç°:**
- `openpi/src/openpi/models_pytorch/tvm_kernels/w4a16_packed_gemm.py` - ä¸»å®ç°

**å¯¼å‡ºçš„ CUDA æºç :**
- `openpi/tvm_trt_plugin/w4a16_mlp/w4a16_packed_gemv.cu` - gate/up_proj
- `openpi/tvm_trt_plugin/w4a16_mlp/w4a16_down_proj.cu` - down_proj

---

## å†å²è®°å½•ï¼šæ—©æœŸ Packed FP4 å®éªŒ

### æ—©æœŸå®éªŒç»“æœ (å• kernel)

| Kernel | Time (ms) | vs TRT FP8 | çŠ¶æ€ |
|--------|-----------|------------|------|
| TVM Naive (float32) | 0.93 ms | 0.57x | âŒ |
| CUDA Optimized (float32) | 1.15 ms | 0.46x | âŒ |
| Packed FP4 V1 | 0.44 ms | 1.22x | âœ… |
| Packed FP4 V3 (vectorized) | 0.42 ms | 1.26x | âœ… |
| Packed FP4 V4 (warp reduce) | 0.36 ms | 1.46x | âœ… |
| **W4A16 Fast (current)** | **0.224 ms** | **2.37x** | âœ… |
| TRT FP8 (baseline) | 0.53 ms | 1.0x | - |

æ—©æœŸ CUDA kernels:
- `openpi/tvm_trt_plugin/nvfp4_gemm/nvfp4_gemm_packed.cu` - å·²éªŒè¯è¶…è¶Š FP8

---

## ä¸‹ä¸€æ­¥è®¡åˆ’ (Current)

### é˜¶æ®µ 1: TRT Plugin é›†æˆ âœ… å·²å®Œæˆ

- [x] å°è£… W4A16 kernel ä¸º IPluginV3 (`w4a16_mlp_plugin.h/cu`)
- [x] CUDA kernel launcher (`w4a16_mlp_launcher.cu`)
- [x] ç‹¬ç«‹æµ‹è¯•éªŒè¯ (`test_w4a16_mlp.cu`)
- [x] CMake æ„å»ºç³»ç»Ÿ
- [x] æƒé‡é¢„æ‰“åŒ…å·¥å…· (`w4a16_mlp.py::pack_checkpoint_weights`)
- [x] é›†æˆåˆ°æ¨ç† pipeline (`w4a16_backend.py`)

**Plugin æ–‡ä»¶ä½ç½®:**
```
openpi/tvm_trt_plugin/w4a16_mlp/
â”œâ”€â”€ w4a16_mlp_plugin.h      # IPluginV3 æ¥å£å®šä¹‰
â”œâ”€â”€ w4a16_mlp_plugin.cu     # Plugin å®ç°
â”œâ”€â”€ w4a16_mlp_launcher.cu   # CUDA kernel launcher
â”œâ”€â”€ test_w4a16_mlp.cu       # ç‹¬ç«‹æµ‹è¯•
â”œâ”€â”€ CMakeLists.txt          # æ„å»ºé…ç½®
â”œâ”€â”€ w4a16_packed_gemv.cu    # TVM å¯¼å‡º kernel (gate/up)
â””â”€â”€ w4a16_down_proj.cu      # TVM å¯¼å‡º kernel (down)
```

**æ€§èƒ½å¯¹æ¯”:**
| Kernel | Time (ms) | vs TRT FP8 | å¤‡æ³¨ |
|--------|-----------|------------|------|
| TVM Fast (æ¨è) | 0.224ms | **2.37x** | ä½¿ç”¨ TVM runtime |
| CUDA Launcher | 0.34ms | 1.55x | æ‰‹å†™ CUDA ç‰ˆæœ¬ |

*å»ºè®®: æœ€ç»ˆé›†æˆä½¿ç”¨ TVM-generated kernel è·å¾—æœ€ä½³æ€§èƒ½*

### é˜¶æ®µ 2: Fusion ä¼˜åŒ– âœ… å·²å®Œæˆ

- [x] gate_proj + up_proj + SiLU*mul fusion
- [ ] Multi-layer persistent kernel (å¯é€‰)

**Fusion Kernel å®ç°:**
- `openpi/src/openpi/models_pytorch/tvm_kernels/w4a16_fused_mlp.py`

**æ€§èƒ½å¯¹æ¯”:**
| é…ç½® | Time (ms) | å¤‡æ³¨ |
|------|-----------|------|
| Separate (gate + up + SiLU*mul) | 0.47ms | 2 x 0.224ms + 0.02ms |
| **Fused** | **0.47ms** | cos_sim=1.0 |

**Fusion æ”¶ç›Š:**
- å‡å°‘ä¸­é—´å­˜å‚¨: gate/up ç»“æœä¸ç”¨å†™å› global memory (~128KB)
- å‡å°‘ kernel launch: 3 kernels â†’ 1 kernel
- æ€§èƒ½ç›¸å½“: memory-bound, æƒé‡å¸¦å®½æ˜¯ç“¶é¢ˆ

### é˜¶æ®µ 3: å…¨æ¨¡å‹é›†æˆ âœ… å·²å®Œæˆ

- [x] W4A16 MLP æ¨¡å— (`w4a16_mlp.py`)
- [x] TVM GEMV kernel (`tvm_kernels/w4a16_gemv.py`)
- [x] æ¨ç† backend (`w4a16_backend.py`)
- [x] UnifiedPolicy æ³¨å†Œ (`w4a16_tvm`, `w4a16_tvm_freq1/2/3`)
- [x] é›†æˆæµ‹è¯•è„šæœ¬ (`scripts/test_w4a16_integration.py`)

**æ–°å¢ä»£ç ä½ç½®:**
```
openpi/src/openpi/models_pytorch/
â”œâ”€â”€ w4a16_mlp.py                    # W4A16 MLP æ¨¡å— (TVM é›†æˆ)
â””â”€â”€ tvm_kernels/
    â”œâ”€â”€ w4a16_gemv.py               # TVM GEMV kernel
    â””â”€â”€ w4a16_fused_mlp.py          # Fused gate+up+SiLU*mul kernel

openpi/src/openpi/inference/
â”œâ”€â”€ w4a16_backend.py                # W4A16 æ¨ç† backend
â””â”€â”€ unified_policy.py               # æ³¨å†Œ w4a16_tvm backend

openpi/scripts/
â””â”€â”€ test_w4a16_integration.py       # ç«¯åˆ°ç«¯æµ‹è¯•è„šæœ¬
```

### é˜¶æ®µ 4: ç«¯åˆ°ç«¯éªŒè¯ (è¿›è¡Œä¸­)

- [ ] å…¨æ¨¡å‹ç²¾åº¦éªŒè¯ (cos > 0.99)
- [ ] LIBERO ä»»åŠ¡æˆåŠŸç‡éªŒè¯
- [ ] ç«¯åˆ°ç«¯å»¶è¿Ÿæµ‹è¯•

### é¢„æœŸæœ€ç»ˆæ”¶ç›Š

| é˜¶æ®µ | KV Cache MLP | æ€» Pipeline | Hz |
|------|--------------|-------------|-----|
| å½“å‰ (TRT FP8) | 20.4ms | 83.5ms | 12.0 |
| **W4A16 (é¢„æœŸ)** | **11.7ms** | **~75ms** | **~13.3** |
| W4A16 + Fusion | ~10ms | ~73ms | ~13.7 |

---

## ä½¿ç”¨æ–¹æ³•

### ä½¿ç”¨ W4A16 Backend æ¨ç†

```python
from openpi.inference import UnifiedPolicy

# åˆ›å»ºä½¿ç”¨ W4A16 TVM kernel çš„ policy
policy = UnifiedPolicy(
    checkpoint_dir="/path/to/checkpoint",
    backend="w4a16_tvm",  # ä½¿ç”¨ W4A16 TVM kernel
    num_denoising_steps=3,
)

# è¿è¡Œæ¨ç†
result = policy.infer({
    "observation/image": image,
    "observation/wrist_image": wrist_img,
    "observation/state": state,
    "prompt": "pick up the black bowl",
})
```

**å¯ç”¨ Backend å˜ä½“:**
- `w4a16_tvm` - é»˜è®¤ (KV reuse freq=2)
- `w4a16_tvm_freq1` - æ—  KV å¤ç”¨ (æœ€é«˜ç²¾åº¦)
- `w4a16_tvm_freq2` - æ¯ 2 å¸§å¤ç”¨ KV
- `w4a16_tvm_freq3` - æ¯ 3 å¸§å¤ç”¨ KV (æ›´é«˜åå)
- `w4a16_pytorch` - PyTorch fallback (æ—  TVM)

### ç¦»çº¿æƒé‡æ‰“åŒ…

```python
from openpi.models_pytorch.w4a16_mlp import pack_checkpoint_weights

# å°†æƒé‡é¢„æ‰“åŒ…ä¸º W4A16 æ ¼å¼
pack_checkpoint_weights(
    checkpoint_path="/path/to/original/checkpoint",
    output_path="/path/to/packed/checkpoint",
    block_size=32,
)
```

---

## éªŒè¯å‘½ä»¤

```bash
# æ¿€æ´» TVM ç¯å¢ƒ
source /home/heima-thor/suliang/Robot-llm/mlc-llm_tvm/venv/bin/activate
cd /home/heima-thor/suliang/Turbo-Pi/openpi

# è¿è¡Œé›†æˆæµ‹è¯• (kernel + MLP æ¨¡å—)
python scripts/test_w4a16_integration.py --kernel-only

# è¿è¡Œå®Œæ•´ pipeline æµ‹è¯• (éœ€è¦ checkpoint)
python scripts/test_w4a16_integration.py --checkpoint /path/to/checkpoint

# å•ç‹¬æµ‹è¯• GEMV kernel
python src/openpi/models_pytorch/tvm_kernels/w4a16_gemv.py

# å•ç‹¬æµ‹è¯• Fused MLP kernel
python src/openpi/models_pytorch/tvm_kernels/w4a16_fused_mlp.py
```

---

## 1. Pi0 æ¨¡å‹å…³é”®ç»´åº¦å‚æ•°

### 1.1 PaliGemma (gemma_2b) - ä¸»æ¨¡å‹

| å‚æ•° | æ•°å€¼ | è¯´æ˜ |
|------|------|------|
| **hidden_size** | 2048 | éšè—å±‚ç»´åº¦ |
| **num_heads** | 8 | æ³¨æ„åŠ›å¤´æ•° |
| **head_dim** | 256 | æ¯å¤´ç»´åº¦ |
| **intermediate_size** | 16384 | MLP ä¸­é—´å±‚ |
| **num_layers** | 18 | Transformer å±‚æ•° |
| **num_kv_heads** | 1 | KV å¤´æ•° (GQA) |

### 1.2 Action Expert (gemma_300m) - åŠ¨ä½œç”Ÿæˆ

| å‚æ•° | æ•°å€¼ | è¯´æ˜ |
|------|------|------|
| **hidden_size** | 1024 | éšè—å±‚ç»´åº¦ |
| **num_heads** | 8 | æ³¨æ„åŠ›å¤´æ•° |
| **head_dim** | 256 | æ¯å¤´ç»´åº¦ |
| **intermediate_size** | 4096 | MLP ä¸­é—´å±‚ |
| **num_layers** | 18 | Transformer å±‚æ•° |
| **num_kv_heads** | 1 | KV å¤´æ•° (GQA) |

### 1.3 å…³é”® GEMV ç»´åº¦

```
PaliGemma GEMV shapes (M=1 single token):
- QKV Projection:  [1, 2048] Ã— [2048, 2048]  = [1, 2048]
- O Projection:    [1, 2048] Ã— [2048, 2048]  = [1, 2048]
- MLP gate_proj:   [1, 2048] Ã— [2048, 16384] = [1, 16384]
- MLP up_proj:     [1, 2048] Ã— [2048, 16384] = [1, 16384]
- MLP down_proj:   [1, 16384] Ã— [16384, 2048] = [1, 2048]

Action Expert GEMV shapes:
- QKV Projection:  [1, 1024] Ã— [1024, 1024]  = [1, 1024]
- MLP gate/up:     [1, 1024] Ã— [1024, 4096]  = [1, 4096]
- MLP down_proj:   [1, 4096] Ã— [4096, 1024]  = [1, 1024]
```

---

## 2. å½“å‰ç“¶é¢ˆåˆ†æ

### 2.1 ä¸ºä»€ä¹ˆç°åœ¨è¾“ç»™ FP8 Tensor Core

**å½“å‰ pipeline**:
```
global memory â†’ dequant (software) â†’ FMA accumulate
      â†“              â†“                   â†“
   å¸¦å®½æ¶ˆè€—       è®¡ç®—æ¶ˆè€—            latency
```

**TRT FP8 Tensor Core**:
```
LDMatrix â†’ Tensor Core MMA â†’ Accumulate (fused)
```

### 2.2 å…³é”®å·®è·

| é¡¹ç›® | TVM FP4 å½“å‰ | TRT FP8 Tensor Core |
|------|-------------|---------------------|
| æ•°æ®æ ¼å¼ | 4bit packed | 8bit native |
| æ‰§è¡Œè·¯å¾„ | CUDA core | Tensor Core |
| load pattern | æ ‡é‡/vector | LDMatrix å¯¹é½ |
| dequant | è½¯ä»¶ 4 é˜¶æ®µ | ç¡¬ä»¶ decode |
| register reuse | ä½ | warp-level fused |

**æ ¸å¿ƒé—®é¢˜**: ä¸æ˜¯ compute-boundï¼Œæ˜¯ **memory / L2 / register reuse bound**

### 2.3 Thor SM110 ç‰¹æ€§

| ç‰¹æ€§ | æ•°å€¼ | å½±å“ |
|------|------|------|
| L2 Cache | è¶…å¤§ | shared tiling æ”¶ç›Šæ›´å¤§ |
| Shared Memory | 49152 bytes/SM | å¯æ”¾æ›´å¤§ tile |
| Max Threads | 1024/block | çµæ´» warp é…ç½® |
| Tensor Core | FP8 E4M3/E5M2 | ç¡¬ä»¶åŠ é€Ÿç«äº‰å¯¹æ‰‹ |

---

## 3. å¯èƒ½æ‰“èµ¢ FP8 çš„ç†è®ºåŸºç¡€

### 3.1 å¸¦å®½ä¼˜åŠ¿

```
FP8: 8-bit memory footprint
FP4: 4-bit memory footprint = 2x å¸¦å®½èŠ‚çœ
```

**Thor memory roofline åˆ†æ**:
```
å¦‚æœè¾¾åˆ° memory roofline:
FP4 theoretically = 2x throughput ceiling
```

### 3.2 å…³é”® insight

> ç›®æ ‡ä¸æ˜¯ "ä¼˜åŒ– GEMV kernel"
> è€Œæ˜¯ "æŠŠ FP4 å˜æˆ Tensor Core friendly layout"

---

## 4. äº”å¤§ä¼˜åŒ–è·¯çº¿ (æŒ‰æˆåŠŸæ¦‚ç‡æ’åº)

### 4.1 ğŸ¥‡ è·¯çº¿ 1: Persistent FP4 GEMV + Shared Unpack

**æœ€å¯èƒ½èµ¢çš„è·¯çº¿**

#### æ ¸å¿ƒæ€æƒ³
```
ä¸æ˜¯: æ¯ä¸ª token è§¦å‘ä¸€ä¸ª kernel
è€Œæ˜¯: æ¯ä¸ª SM å¸¸é©»ä¸€ä¸ª weight shard
```

#### é¢„æœŸæ”¶ç›Š
- Google TPU & NVIDIA CUTLASS: **1.4~1.8x latency reduction**
- å®Œå…¨æ¶ˆç­: kernel launch latency, global reload, L2 thrashing

#### å¯¹ M=1 æ¨ç†æå…¶å…³é”®
- Persistent kernel **æœ€å¼ºåœºæ™¯**å°±æ˜¯ M=1

#### TVM å®ç°æ–¹æ¡ˆ
```python
# TensorIR + explicit SM residency
@T.prim_func
def persistent_gemv():
    # Weight shard pinned to SM
    W_shard = T.alloc_buffer((SHARD_N, K), "float32", scope="shared")

    # Persistent loop - SM never exits
    while True:
        # Wait for input token
        # Compute GEMV on weight shard
        # Sync across SMs for full result
```

---

### 4.2 ğŸ¥ˆ è·¯çº¿ 2: Group-wise Shared Memory Weight Staging

**å½“å‰æœ€çœŸå®çªç ´ç‚¹**

#### æ ¸å¿ƒæ€æƒ³
```
å½“å‰: global â†’ register
ç›®æ ‡: global â†’ shared â†’ register â†’ compute
```

å…³é”®: shared memory é‡Œå­˜ **already unpacked FP4 tiles**

#### ä¸ºä»€ä¹ˆå¯¹ FP4 è‡³å…³é‡è¦

FP4 unpack cost:
```
bit extraction + scale multiply = å¤§å¼€é”€
```

å¦‚æœæŠŠ unpack åç»“æœæ”¾ shared:
```
reuse across warp lanes = å¤šæ¬¡å¤ç”¨
```

#### æ€§èƒ½å·®è· (è®ºæ–‡æ•°æ®)
```
unpack in register â†’ slow
unpack in shared tile â†’ fast
å·®è·: 30~60%
```

#### TVM å®ç°æ–¹æ¡ˆ
```python
@T.prim_func
def shared_unpack_gemv():
    # Shared memory for unpacked FP4 tiles
    W_shared = T.alloc_buffer((TILE_N, TILE_K), "float32", scope="shared")
    scale_shared = T.alloc_buffer((TILE_N, TILE_K // 32), "float32", scope="shared")

    for tile_k in range(K // TILE_K):
        # 1. cp.async: global â†’ shared (packed FP4)
        # 2. Cooperative unpack in shared memory
        # 3. Warp compute with shared data
        T.tvm_storage_sync("shared")
```

---

### 4.3 ğŸ¥‰ è·¯çº¿ 3: LDMatrix-style 4-bit Layout

**æœ€éš¾ä½†æ½œåŠ›æœ€å¤§**

#### æ ¸å¿ƒæ€æƒ³

Tensor Core load pattern è¦æ±‚:
```
LDMatrix.x4 â†’ 16x16 fragment layout
```

å½“å‰ FP4 weight layout:
```
bit packed linear â†’ warp lane conflict
```

#### çœŸæ­£é¡¶çº§åšæ³•
```
æŠŠ FP4 weight é‡æ–° layout æˆ tensor-core friendly fragment layout
```

ç±»ä¼¼ CUTLASS FP4 interleaved layout:
- Column interleave
- Warp-striped packing

#### Layout Transform è®¾è®¡
```python
def transform_weight_layout(W_packed, N, K):
    """
    Transform from linear packed to warp-friendly layout.

    Original: [N, K/2] (two FP4 per byte)
    Target:   [N/16, K/16, 16, 16/2] (fragment-aligned)
    """
    # Fragment size for Tensor Core
    FRAG_M, FRAG_K = 16, 16

    # Interleave for warp lane mapping
    W_transformed = np.zeros((N // FRAG_M, K // FRAG_K, FRAG_M, FRAG_K // 2), dtype=np.uint8)

    for ni in range(N // FRAG_M):
        for ki in range(K // FRAG_K):
            for m in range(FRAG_M):
                for k in range(FRAG_K // 2):
                    # Warp-striped mapping
                    src_n = ni * FRAG_M + m
                    src_k = ki * FRAG_K + k * 2
                    W_transformed[ni, ki, m, k] = W_packed[src_n, src_k // 2]

    return W_transformed
```

---

### 4.4 è·¯çº¿ 4: Fuse KV Projection + GEMV

**å·¥ç¨‹æ”¶ç›Šæœ€å¤§**

#### å½“å‰ pipeline
```
QKV projection â†’ GEMV â†’ KV cache write
     â†“              â†“          â†“
   è®¡ç®—          è®¡ç®—      å¸¦å®½æ¶ˆè€—å·¨å¤§
```

#### ä¼˜åŒ–ç›®æ ‡
```
Fuse GEMV + KV write = store once
```

#### å·²éªŒè¯æœ‰æ•ˆ
- FlashDecoding
- DeepSpeed inference

#### KV Cache å æ¯”
```
å½“å‰: 47.4 ms / 83.5 ms = 57% æ—¶é—´åœ¨ KV Cache
è¿™æ˜¯ gold mine!
```

#### TVM å®ç°æ–¹æ¡ˆ
```python
@T.prim_func
def fused_qkv_gemv_kv_write():
    # Single kernel: QKV projection + KV cache update
    for layer in range(num_layers):
        # Compute Q, K, V
        Q = gemv(hidden_states, W_q)
        K = gemv(hidden_states, W_k)
        V = gemv(hidden_states, W_v)

        # Fused KV cache write (no separate store)
        kv_cache[layer, pos] = (K, V)  # In-place
```

---

### 4.5 è·¯çº¿ 5: Quantization-aware Tiling

**å­¦æœ¯æœ€å¼ºè·¯çº¿**

#### æ ¸å¿ƒæ€æƒ³
```
å½“å‰: per-weight scale (æ¯ä¸ªæƒé‡ä¸€ä¸ª scale)
ç›®æ ‡: per-32 channel scale (æ¯ 32 ä¸ªé€šé“å…±äº« scale)
```

#### æ”¶ç›Š
- Scale load å‡å°‘ 32x
- Shared reuse æ›´å¼º
- ä¸ nvFP4 block_size=32 å¯¹é½

---

## 5. è¯¦ç»†å®ç°è®¡åˆ’

### Phase 1: Weight Layout Transform (Week 1)

**ç›®æ ‡**: Make FP4 warp friendly

#### 5.1.1 å®ç°æ­¥éª¤
1. ç ”ç©¶ CUTLASS interleaved layout
2. è®¾è®¡ TVM `transform_layout` primitive
3. å®ç°ç¦»çº¿æƒé‡è½¬æ¢å·¥å…·
4. éªŒè¯è½¬æ¢æ­£ç¡®æ€§

#### 5.1.2 ä»£ç ç»“æ„
```
openpi/src/openpi/models_pytorch/tvm_kernels/
â”œâ”€â”€ weight_layout_transform.py   # æƒé‡ layout è½¬æ¢
â”œâ”€â”€ fp4_interleaved_packer.py    # Warp-friendly packing
â””â”€â”€ test_layout_transform.py     # éªŒè¯æ­£ç¡®æ€§
```

### Phase 2: Shared Memory Unpack Cache (Week 2)

**ç›®æ ‡**: Shared memory staging with unpacked tiles

#### 5.2.1 å®ç°æ­¥éª¤
1. è®¾è®¡ shared memory tile å¤§å°
2. å®ç° cp.async é¢„åŠ è½½
3. Cooperative unpack in shared
4. Benchmark vs register-only

#### 5.2.2 Tile è®¾è®¡
```python
# Pi0 PaliGemma dimensions
HIDDEN = 2048
MLP_DIM = 16384

# Tile sizes for shared memory (49152 bytes max)
# FP32: 4 bytes per element
# Max elements: 49152 / 4 = 12288

# For MLP gate_proj: [1, 2048] Ã— [2048, 16384]
# Tile: [1, TILE_K] Ã— [TILE_N, TILE_K]
TILE_K = 256   # Process 256 input features at a time
TILE_N = 32    # 32 output features per tile

# Shared memory usage:
# W_tile: 32 Ã— 256 Ã— 4 = 32768 bytes
# scale_tile: 32 Ã— 8 Ã— 4 = 1024 bytes
# Total: 33792 bytes < 49152 âœ“
```

### Phase 3: Persistent Kernel (Week 3)

**ç›®æ ‡**: SM pinned weight shard

#### 5.3.1 å®ç°æ­¥éª¤
1. è®¾è®¡ SM residency ç­–ç•¥
2. å®ç° persistent loop
3. å¤„ç† SM é—´åŒæ­¥
4. Benchmark latency reduction

#### 5.3.2 Weight Sharding ç­–ç•¥
```python
# Thor has 72 SMs (SM_110)
NUM_SMS = 72

# PaliGemma MLP down_proj: [16384, 2048]
# Shard across SMs:
SHARD_SIZE = 16384 // NUM_SMS  # ~228 rows per SM

# Each SM holds:
# - Weight shard: 228 Ã— 2048 Ã— 0.5 bytes = 233472 bytes (FP4 packed)
# - Fits in shared memory after optimization
```

### Phase 4: KV Cache Fusion (Week 4)

**ç›®æ ‡**: Fuse GEMV + KV write

#### 5.4.1 å®ç°æ­¥éª¤
1. åˆ†æ KV cache è®¿é—®æ¨¡å¼
2. è®¾è®¡ fused kernel æ¥å£
3. å®ç° TensorIR fused kernel
4. é›†æˆåˆ°æ¨ç† pipeline

---

## 6. Benchmark è®¡åˆ’

### 6.1 Baseline æµ‹é‡

| Kernel | å½“å‰æ—¶é—´ | ç›®æ ‡æ—¶é—´ | æå‡æ¯”ä¾‹ |
|--------|---------|---------|---------|
| TRT FP8 | 0.53 ms | - | baseline |
| TVM FP4 naive | 1.45 ms | - | 0.36x |
| TVM FP4 unroll | 0.83 ms | - | 0.64x |
| TVM FP4 + layout | ? | 0.45 ms | 1.18x |
| TVM FP4 + shared | ? | 0.35 ms | 1.51x |
| TVM FP4 + persistent | ? | 0.30 ms | 1.77x |

### 6.2 æµ‹é‡æŒ‡æ ‡

```python
# benchmark_fp4_optimized.py
metrics = {
    "kernel_time_ms": ...,
    "memory_bandwidth_utilization": ...,
    "compute_utilization": ...,
    "l2_cache_hit_rate": ...,
    "shared_memory_efficiency": ...,
    "warp_execution_efficiency": ...,
}
```

---

## 7. è®ºæ–‡ä¸ç¤¾åŒºå‚è€ƒ

### 7.1 æ ¸å¿ƒè®ºæ–‡

1. **FlashAttention-2**: Tri Dao, 2023
   - Persistent kernel design
   - Shared memory tiling for attention

2. **AWQ**: Ji Lin et al., 2023
   - Activation-aware quantization
   - Group-wise scaling

3. **GPTQ**: Elias Frantar et al., 2023
   - Per-channel quantization
   - Efficient dequantization

4. **SmoothQuant**: Guangxuan Xiao et al., 2023
   - Activation smoothing for quantization
   - FP8/INT8 optimization

### 7.2 å¼€æºå®ç°

1. **CUTLASS**: NVIDIA
   - FP4 interleaved layout å‚è€ƒ
   - Tensor Core fragment mapping

2. **TVM BYOC**: Apache TVM
   - Custom accelerator integration
   - TensorRT plugin generation

3. **vLLM**: UC Berkeley
   - PagedAttention
   - Continuous batching

4. **MLC-LLM**: CMU
   - TVM é‡åŒ–æ¨ç†
   - Mobile deployment

---

## 8. é£é™©ä¸å¤‡é€‰æ–¹æ¡ˆ

### 8.1 é£é™©è¯„ä¼°

| é£é™© | æ¦‚ç‡ | å½±å“ | ç¼“è§£æªæ–½ |
|------|------|------|---------|
| Layout transform ç²¾åº¦æŸå¤± | ä½ | é«˜ | é€æ­¥éªŒè¯æ¯ä¸ªè½¬æ¢ |
| Shared memory å®¹é‡ä¸è¶³ | ä¸­ | ä¸­ | åŠ¨æ€ tile å¤§å° |
| Persistent kernel å¤æ‚ | é«˜ | ä¸­ | å…ˆå®ç° non-persistent ç‰ˆæœ¬ |
| Thor ç‰¹å®šé—®é¢˜ | ä¸­ | é«˜ | ä¿æŒ SM89 å…¼å®¹æ€§ |

### 8.2 å¤‡é€‰æ–¹æ¡ˆ

å¦‚æœ TVM FP4 æ— æ³•è¶…è¶Š FP8:

1. **æ··åˆæ–¹æ¡ˆ**: å…³é”®å±‚ç”¨ FP8ï¼Œéå…³é”®å±‚ç”¨ FP4
2. **å¸¦å®½ä¼˜åŒ–**: å³ä½¿é€Ÿåº¦ç›¸åŒï¼ŒFP4 èŠ‚çœ 50% å¸¦å®½
3. **ç­‰å¾…ç¡¬ä»¶**: Thor ä¸‹ä¸€ä»£å¯èƒ½åŸç”Ÿæ”¯æŒ FP4 Tensor Core

---

## 9. æˆåŠŸæ ‡å‡†

### 9.1 æ€§èƒ½ç›®æ ‡

```
ç›®æ ‡: TVM FP4 kernel < 0.50 ms (è¶…è¶Š TRT FP8 0.53 ms)
```

### 9.2 éªŒæ”¶æ ‡å‡†

1. **æ€§èƒ½**: å• GEMV å»¶è¿Ÿ < 0.50 ms
2. **ç²¾åº¦**: ç›¸å¯¹è¯¯å·® < 1%, ç›¸å…³æ€§ > 0.9999
3. **ç¨³å®šæ€§**: è¿ç»­ 1000 æ¬¡æ¨ç†æ— å¼‚å¸¸
4. **é›†æˆ**: å¯ç¼–è¯‘ä¸º TensorRT Plugin

---

## 10. æ—¶é—´è¡¨

| å‘¨æ¬¡ | ä»»åŠ¡ | äº¤ä»˜ç‰© |
|------|------|--------|
| Week 1 | Weight Layout Transform | `weight_layout_transform.py` |
| Week 2 | Shared Memory Unpack | `shared_unpack_gemv.py` |
| Week 3 | Persistent Kernel | `persistent_gemv.py` |
| Week 4 | KV Fusion + Integration | TRT Plugin é›†æˆ |
| Week 5 | æµ‹è¯•ä¸ä¼˜åŒ– | æœ€ç»ˆ benchmark æŠ¥å‘Š |

---

## 11. é™„å½•: TensorIR Kernel Skeleton

### 11.1 Shared Memory Unpack GEMV

```python
@T.prim_func
def fp4_shared_unpack_gemv(
    A: T.Buffer((1, K), "float32"),           # Activation [1, K]
    W_packed: T.Buffer((N, K // 2), "uint8"), # FP4 packed weight
    scale_W: T.Buffer((N, K // 32), "float32"),
    C: T.Buffer((1, N), "float32"),
):
    T.func_attr({"global_symbol": "fp4_shared_gemv", "tir.noalias": True})

    # Shared memory for unpacked weight tile
    W_shared = T.alloc_buffer((TILE_N, TILE_K), "float32", scope="shared")
    A_shared = T.alloc_buffer((TILE_K,), "float32", scope="shared")

    for tile_n in T.thread_binding(N // TILE_N, thread="blockIdx.x"):
        for tx in T.thread_binding(256, thread="threadIdx.x"):

            # Initialize accumulator
            acc = T.alloc_buffer((TILE_N // 256,), "float32", scope="local")
            for i in range(TILE_N // 256):
                acc[i] = T.float32(0)

            for tile_k in range(K // TILE_K):
                # 1. Cooperative load A tile
                if tx < TILE_K:
                    A_shared[tx] = A[0, tile_k * TILE_K + tx]

                # 2. Cooperative unpack W tile
                for load_idx in range(TILE_N * TILE_K // 2 // 256):
                    # Unpack FP4 â†’ FP32 with scale
                    packed_idx = tx + load_idx * 256
                    n_idx = packed_idx // (TILE_K // 2)
                    k_idx = (packed_idx % (TILE_K // 2)) * 2

                    packed = W_packed[tile_n * TILE_N + n_idx, tile_k * TILE_K // 2 + k_idx // 2]
                    scale = scale_W[tile_n * TILE_N + n_idx, (tile_k * TILE_K + k_idx) // 32]

                    # Unpack two FP4 values
                    fp4_lo = T.cast(packed & 0xF, "float32") * scale
                    fp4_hi = T.cast((packed >> 4) & 0xF, "float32") * scale

                    W_shared[n_idx, k_idx] = fp4_lo
                    W_shared[n_idx, k_idx + 1] = fp4_hi

                T.tvm_storage_sync("shared")

                # 3. Compute with shared data
                for i in range(TILE_N // 256):
                    n_local = tx * (TILE_N // 256) + i
                    for k in range(TILE_K):
                        acc[i] += A_shared[k] * W_shared[n_local, k]

                T.tvm_storage_sync("shared")

            # 4. Write result
            for i in range(TILE_N // 256):
                C[0, tile_n * TILE_N + tx * (TILE_N // 256) + i] = acc[i]
```

### 11.2 Warp Lane Mapping

```python
# For 32-thread warp processing 32x32 tile
def get_warp_lane_mapping(lane_id, frag_m=16, frag_k=16):
    """
    Map warp lane to weight fragment position.

    lane_id: 0-31
    Returns: (row, col) in 16x16 fragment
    """
    # Tensor Core style mapping
    row = (lane_id % 4) * 2 + (lane_id // 16)
    col = (lane_id // 4) % 4 + ((lane_id % 16) // 8) * 4
    return row, col
```

---

## 12. ç»“è®º

### âœ… å·²è¾¾æˆç›®æ ‡

æˆ‘ä»¬æˆåŠŸå®ç°äº†åŸè®¡åˆ’çš„æ ¸å¿ƒç›®æ ‡ï¼š

```
å®é™…ç»“æœ:
TVM W4A16 kernel: 0.224 ms (gate/up_proj)
vs TRT FP8: 0.53 ms
åŠ é€Ÿæ¯”: 2.37x (è¶…è¿‡é¢„æœŸçš„ 1.3-1.8x!)
```

### æˆåŠŸå› ç´ 

1. **çœŸæ­£çš„ 4-bit Packed æ ¼å¼**: uint8 å­˜å‚¨ 2 ä¸ª FP4 å€¼ï¼Œ8x å¸¦å®½èŠ‚çœ
2. **K-dimension Tiling**: å¤„ç†å¤§ K å€¼ (16384) çš„ shared memory é™åˆ¶
3. **Shared Memory LUT**: 16 entries å¿«é€Ÿ dequant æŸ¥è¡¨
4. **Parallel Reduction**: 64 çº¿ç¨‹åä½œ reductionï¼Œ6-step log2

### ä¸‹ä¸€é˜¶æ®µç›®æ ‡

1. **TRT Plugin é›†æˆ**: å°è£…ä¸ºå¯ç”¨äºæ¨ç†çš„ plugin
2. **Fusion ä¼˜åŒ–**: gate+up fusion, SiLU*mul fusion
3. **ç«¯åˆ°ç«¯éªŒè¯**: å…¨æ¨¡å‹é›†æˆå’Œ LIBERO ä»»åŠ¡éªŒè¯

### æŠ€æœ¯äº®ç‚¹

è¿™ä¸ªå®ç°å±•ç¤ºäº†ï¼š
- ç”¨åŸç”Ÿ TVM TensorIR è§£å†³ Thor SM110 ç”Ÿæ€ä¸é€‚é…é—®é¢˜
- è½¯ä»¶ dequant + CUDA Core å¯ä»¥è¶…è¶Š Tensor Core FP8
- Packed format + å¸¦å®½ä¼˜åŒ–æ˜¯ M=1 GEMV çš„å…³é”®
