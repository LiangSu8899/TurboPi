# Thor Triforce ä¼˜åŒ–å·¥ç¨‹è®¡åˆ’ v2.0

## åŸºäºå¸¦å®½å¢™ä¿®æ­£åçš„ç¡¬æ ¸åŠ é€Ÿæ–¹æ¡ˆ

**æ ¸å¿ƒä¿®æ­£**: KV Cache ç“¶é¢ˆæ˜¯ **MLP æƒé‡çš„å†…å­˜å¸¦å®½è¯»å–**ï¼Œä¸æ˜¯ Attention è®¡ç®—ã€‚FlashInfer æ— æ³•æ‰“ç ´ 22ms çš„ç‰©ç†ä¸‹é™ã€‚

---

## ä¸€ã€é—®é¢˜æ ¹å› é‡æ–°å®šä½

### 1.1 å¸¦å®½å¢™åˆ†æ (Memory Wall)

| ç»„ä»¶ | æƒé‡å¤§å° (BF16) | å æ¯” | è¯»å–æ—¶é—´ (@200GB/s) |
|------|-----------------|------|---------------------|
| QKV + O Projection | 324 MB | 8.2% | 1.6 ms |
| **MLP (gate+up+down)** | **3.62 GB** | **91.4%** | **18.1 ms** |
| LayerNorm/RoPE | 18 MB | 0.4% | 0.1 ms |
| **Total** | **3.96 GB** | 100% | **~20 ms (ç†è®ºä¸‹é™)** |

### 1.2 ä¸ºä»€ä¹ˆä¹‹å‰çš„æ–¹æ¡ˆè¿‡äºä¹è§‚

| æ–¹æ¡ˆ | åŸå‡è®¾ | å®é™…æƒ…å†µ |
|------|--------|----------|
| FlashInfer | Attention æ˜¯ç“¶é¢ˆ | âŒ MLP å¸¦å®½æ‰æ˜¯ç“¶é¢ˆ |
| Static Prompt Caching | ç¼“å­˜çœè®¡ç®— | âŒ çœè®¡ç®—ä¸çœå¸¦å®½ |
| TRT FP8 | FP8 å‡åŠæƒé‡ | âŒ Thor TRT scale è¢«å¿½ç•¥ |
| FP4 é‡åŒ– | 1/4 æƒé‡ | âŒ Thor ä¸æ”¯æŒ (Segfault) |

### 1.3 å”¯ä¸€ç ´å±€ç‚¹

**å¿…é¡»æŠŠ 3.62 GB æƒé‡å˜å°**ï¼š

| é‡åŒ–æ–¹æ¡ˆ | æƒé‡å¤§å° | è¯»å–æ—¶é—´ | å¯è¡Œæ€§ |
|----------|----------|----------|--------|
| BF16 | 3.62 GB | 18.1 ms | âœ… å½“å‰ |
| FP8 | 1.81 GB | 9.0 ms | âš ï¸ Thor TRT æœ‰ bug |
| **INT4 (W4A16)** | **0.90 GB** | **4.5 ms** | ğŸ¯ **å”¯ä¸€å‡ºè·¯** |

---

## äºŒã€ä¿®æ­£åçš„ç›®æ ‡è®¾å®š

### 2.1 åŠ¡å®çš„å»¶è¿Ÿç›®æ ‡

| ç»„ä»¶ | å½“å‰ | Plan A (ä¿å®ˆ) | Plan B (æ¿€è¿›) |
|------|------|---------------|---------------|
| Vision TRT | 17.0 ms | 17.0 ms | 17.0 ms |
| KV Cache Prefill | 54.0 ms | **30 ms** | **12 ms** |
| Denoise (10 step) | 102.3 ms | 95 ms | 85 ms |
| Overhead | 3.2 ms | 1.0 ms | 0.5 ms |
| **Total** | **176.5 ms** | **143 ms** | **114.5 ms** |
| **Hz** | **5.7 Hz** | **7.0 Hz** | **8.7 Hz** |

### 2.2 ä¸¤æ¡è·¯çº¿å¯¹æ¯”

| è·¯çº¿ | æ ¸å¿ƒæ‰‹æ®µ | é¢„æœŸæ”¶ç›Š | é£é™© | å·¥ä½œé‡ |
|------|----------|----------|------|--------|
| **Plan A** | FlashInfer + CUDA Graph | 5.7â†’7.0 Hz | ä½ | 2 å‘¨ |
| **Plan B** | INT4 Triton Kernel | 5.7â†’8.7 Hz | ä¸­-é«˜ | 4 å‘¨ |

---

## ä¸‰ã€Phase 0: ç¯å¢ƒéªŒè¯ (Day 1-2)

**è¿™æ˜¯æœ€å…³é”®çš„ä¸€æ­¥ï¼Œå†³å®šåç»­è·¯çº¿**

### 3.1 éªŒè¯è„šæœ¬

```python
#!/usr/bin/env python3
"""
scripts/phase0_environment_check.py

éªŒè¯ Thor å¹³å°çš„è½¯ç¡¬ä»¶æ”¯æŒæƒ…å†µï¼Œå†³å®šä¼˜åŒ–è·¯çº¿
"""

import torch
import time
import subprocess
import sys

def check_gpu_info():
    """æ£€æŸ¥ GPU ä¿¡æ¯"""
    print("=" * 60)
    print("GPU Information")
    print("=" * 60)

    if not torch.cuda.is_available():
        print("âŒ CUDA not available!")
        return False

    device = torch.cuda.current_device()
    props = torch.cuda.get_device_properties(device)

    print(f"Device: {props.name}")
    print(f"Compute Capability: {props.major}.{props.minor}")
    print(f"Total Memory: {props.total_memory / 1e9:.2f} GB")
    print(f"SM Count: {props.multi_processor_count}")

    # Thor åº”è¯¥æ˜¯ SM 10.0 æˆ– 11.0
    if props.major >= 10:
        print("âœ… Blackwell architecture detected (Thor)")
    else:
        print(f"âš ï¸ Non-Blackwell GPU (SM {props.major}.{props.minor})")

    return True


def benchmark_memory_bandwidth():
    """
    æµ‹é‡å®é™…å†…å­˜å¸¦å®½ - è¿™æ˜¯æœ€å…³é”®çš„æ•°æ®
    æ¨¡æ‹Ÿ MLP çš„æƒé‡è¯»å–
    """
    print("\n" + "=" * 60)
    print("Memory Bandwidth Benchmark")
    print("=" * 60)

    device = "cuda"

    # æµ‹è¯•ä¸åŒå¤§å°çš„ Linear å±‚
    configs = [
        (2048, 16384, "MLP Up/Gate (single)"),
        (16384, 2048, "MLP Down (single)"),
        (2048, 2048, "QKV/O Projection"),
    ]

    results = {}

    for in_dim, out_dim, name in configs:
        # æ¨¡æ‹Ÿ batch=1, seq=712
        x = torch.randn(1, 712, in_dim, device=device, dtype=torch.bfloat16)
        layer = torch.nn.Linear(in_dim, out_dim, bias=False,
                                device=device, dtype=torch.bfloat16)

        # æƒé‡å¤§å°
        weight_bytes = in_dim * out_dim * 2  # BF16 = 2 bytes

        # Warmup
        for _ in range(10):
            _ = layer(x)
        torch.cuda.synchronize()

        # Benchmark
        start = time.perf_counter()
        num_iters = 100
        for _ in range(num_iters):
            _ = layer(x)
        torch.cuda.synchronize()

        avg_time_ms = (time.perf_counter() - start) / num_iters * 1000
        effective_bw = weight_bytes / (avg_time_ms / 1000) / 1e9  # GB/s

        results[name] = {
            "time_ms": avg_time_ms,
            "weight_mb": weight_bytes / 1e6,
            "bandwidth_gbps": effective_bw,
        }

        print(f"\n{name}:")
        print(f"  Shape: ({in_dim}, {out_dim})")
        print(f"  Weight: {weight_bytes/1e6:.2f} MB")
        print(f"  Time: {avg_time_ms:.3f} ms")
        print(f"  Effective Bandwidth: {effective_bw:.1f} GB/s")

    # ä¼°ç®—å®Œæ•´ KV Cache MLP æ—¶é—´
    # 18 å±‚ Ã— (gate + up + down)
    mlp_time = (results["MLP Up/Gate (single)"]["time_ms"] * 2 +
                results["MLP Down (single)"]["time_ms"]) * 18

    print(f"\n" + "-" * 40)
    print(f"Estimated KV Cache MLP Time (18 layers): {mlp_time:.1f} ms")
    print(f"Theoretical Minimum (@200 GB/s): 18.1 ms")

    avg_bw = sum(r["bandwidth_gbps"] for r in results.values()) / len(results)
    print(f"\nAverage Effective Bandwidth: {avg_bw:.1f} GB/s")

    if avg_bw < 150:
        print("âš ï¸ Bandwidth significantly below theoretical (200 GB/s)")
        print("   Possible causes: CUDA driver, memory contention, thermal throttling")
    elif avg_bw > 180:
        print("âœ… Bandwidth close to theoretical maximum")

    return results


def check_flashinfer():
    """æ£€æŸ¥ FlashInfer æ˜¯å¦å¯ç”¨"""
    print("\n" + "=" * 60)
    print("FlashInfer Check")
    print("=" * 60)

    try:
        import flashinfer
        print(f"âœ… FlashInfer version: {flashinfer.__version__}")

        # å°è¯•ç®€å•æ“ä½œ
        q = torch.randn(1, 50, 8, 256, device="cuda", dtype=torch.float16)
        k = torch.randn(1, 712, 1, 256, device="cuda", dtype=torch.float16)
        v = torch.randn(1, 712, 1, 256, device="cuda", dtype=torch.float16)

        # è¿™é‡Œå¯èƒ½ä¼šå¤±è´¥ï¼Œå› ä¸º Thor å¯èƒ½ä¸è¢«æ”¯æŒ
        try:
            # FlashInfer API è°ƒç”¨
            print("  Testing FlashInfer attention...")
            # out = flashinfer.single_prefill_with_kv_cache(q, k, v)
            print("  âš ï¸ Need to test actual FlashInfer API on Thor")
        except Exception as e:
            print(f"  âŒ FlashInfer operation failed: {e}")

        return True
    except ImportError:
        print("âŒ FlashInfer not installed")
        print("   Install: pip install flashinfer")
        return False


def check_triton():
    """æ£€æŸ¥ Triton æ˜¯å¦æ”¯æŒ Thor"""
    print("\n" + "=" * 60)
    print("Triton Check")
    print("=" * 60)

    try:
        import triton
        import triton.language as tl

        print(f"âœ… Triton version: {triton.__version__}")

        # ç®€å•çš„ Triton kernel æµ‹è¯•
        @triton.jit
        def add_kernel(x_ptr, y_ptr, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):
            pid = tl.program_id(0)
            offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
            mask = offs < n_elements
            x = tl.load(x_ptr + offs, mask=mask)
            y = tl.load(y_ptr + offs, mask=mask)
            tl.store(out_ptr + offs, x + y, mask=mask)

        # è¿è¡Œæµ‹è¯•
        n = 1024
        x = torch.randn(n, device="cuda")
        y = torch.randn(n, device="cuda")
        out = torch.empty_like(x)

        grid = lambda meta: (triton.cdiv(n, meta['BLOCK_SIZE']),)
        add_kernel[grid](x, y, out, n, BLOCK_SIZE=256)

        # éªŒè¯
        expected = x + y
        if torch.allclose(out, expected):
            print("âœ… Triton kernel execution successful on Thor")
            return True
        else:
            print("âŒ Triton kernel produced incorrect results")
            return False

    except ImportError:
        print("âŒ Triton not installed")
        return False
    except Exception as e:
        print(f"âŒ Triton error: {e}")
        return False


def check_int4_support():
    """æ£€æŸ¥ INT4 é‡åŒ–åº“æ”¯æŒ"""
    print("\n" + "=" * 60)
    print("INT4 Quantization Check")
    print("=" * 60)

    libs = {
        "bitsandbytes": "INT8/FP4 quantization",
        "auto_gptq": "GPTQ INT4",
        "awq": "AWQ INT4",
    }

    available = []
    for lib, desc in libs.items():
        try:
            __import__(lib.replace("-", "_"))
            print(f"âœ… {lib}: {desc}")
            available.append(lib)
        except ImportError:
            print(f"âŒ {lib}: not installed")

    return available


def run_all_checks():
    """è¿è¡Œæ‰€æœ‰æ£€æŸ¥"""
    print("\n" + "=" * 60)
    print("THOR TRIFORCE ENVIRONMENT CHECK")
    print("=" * 60)

    results = {}

    # GPU Info
    results["gpu"] = check_gpu_info()

    # Bandwidth - æœ€å…³é”®çš„æµ‹è¯•
    if results["gpu"]:
        results["bandwidth"] = benchmark_memory_bandwidth()

    # FlashInfer
    results["flashinfer"] = check_flashinfer()

    # Triton
    results["triton"] = check_triton()

    # INT4
    results["int4_libs"] = check_int4_support()

    # å†³ç­–å»ºè®®
    print("\n" + "=" * 60)
    print("RECOMMENDATION")
    print("=" * 60)

    if results.get("bandwidth"):
        avg_bw = sum(r["bandwidth_gbps"] for r in results["bandwidth"].values()) / len(results["bandwidth"])

        if avg_bw < 150:
            print("\nâš ï¸ å¸¦å®½å—é™ä¸¥é‡ï¼Œå»ºè®®:")
            print("   1. æ£€æŸ¥ CUDA driver ç‰ˆæœ¬")
            print("   2. æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–è¿›ç¨‹å ç”¨ GPU")
            print("   3. INT4 é‡åŒ–æ˜¯å”¯ä¸€å‡ºè·¯")

        if results.get("triton"):
            print("\nâœ… Triton å¯ç”¨ï¼Œæ¨è Plan B (INT4 Triton Kernel)")
        else:
            print("\nâš ï¸ Triton ä¸å¯ç”¨ï¼Œåªèƒ½èµ° Plan A (ä¿å®ˆä¼˜åŒ–)")

    return results


if __name__ == "__main__":
    results = run_all_checks()
```

### 3.2 è¿è¡ŒéªŒè¯

```bash
# åœ¨ Thor ä¸Šè¿è¡Œ
docker exec turbo_pi_eval python /workspace/scripts/phase0_environment_check.py

# ä¿å­˜ç»“æœ
docker exec turbo_pi_eval python /workspace/scripts/phase0_environment_check.py > phase0_results.txt 2>&1
```

### 3.3 å†³ç­–æ ‘

```
Phase 0 ç»“æœ
    â”‚
    â”œâ”€â”€ å¸¦å®½ > 180 GB/s?
    â”‚   â”œâ”€â”€ Yes â†’ FP16 è¿˜æœ‰æ•‘ï¼Œå°è¯• FlashInfer
    â”‚   â””â”€â”€ No â†’ å¿…é¡» INT4 é‡åŒ–
    â”‚
    â”œâ”€â”€ Triton å¯ç”¨?
    â”‚   â”œâ”€â”€ Yes â†’ Plan B (Triton INT4 Kernel)
    â”‚   â””â”€â”€ No â†’ Plan A (ä¿å®ˆä¼˜åŒ–) æˆ–ç­‰å¾…è½¯ä»¶æ”¯æŒ
    â”‚
    â””â”€â”€ FlashInfer å¯ç”¨?
        â”œâ”€â”€ Yes â†’ Attention éƒ¨åˆ†å¯ç”¨ FlashInfer
        â””â”€â”€ No â†’ ç”¨ PyTorch + CUDA Graph
```

---

## å››ã€Plan A: ä¿å®ˆä¼˜åŒ–è·¯çº¿ (ä½é£é™©)

**ç›®æ ‡**: 5.7 Hz â†’ 7.0 Hz
**é¢„æœŸ KV Cache**: 54 ms â†’ 30 ms
**å·¥ä½œé‡**: 2 å‘¨

### 4.1 ä¼˜åŒ–å†…å®¹

| ç»„ä»¶ | æ‰‹æ®µ | é¢„æœŸèŠ‚çœ |
|------|------|----------|
| Attention | FlashInfer æˆ– Triton | 5-8 ms |
| Padding | å»é™¤æ— æ•ˆ token | 3-5 ms |
| CUDA Graph | å…¨å›¾å½•åˆ¶ | 2-3 ms |
| Kernel Fusion | torch.compile | 2-3 ms |
| **Total** | | **12-19 ms** |

### 4.2 å®ç°æ­¥éª¤

#### Step 1: CUDA Graph å…¨å›¾å½•åˆ¶

```python
# src/openpi/inference/full_graph_policy.py

class FullGraphPolicy:
    """å…¨å›¾ CUDA Graph å½•åˆ¶"""

    def __init__(self, base_policy):
        self.base_policy = base_policy

        # é™æ€ buffer
        self.static_image = torch.zeros(1, 3, 224, 224, device="cuda", dtype=torch.float16)
        self.static_wrist = torch.zeros(1, 3, 224, 224, device="cuda", dtype=torch.float16)
        self.static_state = torch.zeros(1, 32, device="cuda", dtype=torch.bfloat16)
        self.static_tokens = torch.zeros(1, 200, device="cuda", dtype=torch.long)

        # æ•è·å›¾
        self._capture_graph()

    def _capture_graph(self):
        """æ•è·å®Œæ•´è®¡ç®—å›¾"""
        # Warmup
        for _ in range(5):
            self._forward()
        torch.cuda.synchronize()

        # Capture
        self.graph = torch.cuda.CUDAGraph()
        with torch.cuda.graph(self.graph):
            self._forward()

    def _forward(self):
        """è¢«æ•è·çš„å‰å‘ä¼ æ’­"""
        # Vision + KV + Denoise å…¨éƒ¨åœ¨ä¸€ä¸ª graph é‡Œ
        self.static_output = self.base_policy._forward_impl(
            self.static_image,
            self.static_wrist,
            self.static_state,
            self.static_tokens,
        )

    def infer(self, image, wrist, state, tokens):
        # å¤åˆ¶è¾“å…¥
        self.static_image.copy_(image)
        self.static_wrist.copy_(wrist)
        self.static_state.copy_(state)
        self.static_tokens.copy_(tokens)

        # æ‰§è¡Œ
        self.graph.replay()

        return self.static_output.clone()
```

#### Step 2: Kernel Fusion (torch.compile)

```python
# å¯¹ç¢ç‰‡ç®—å­è¿›è¡Œèåˆ
import torch._dynamo as dynamo

@torch.compile(mode="reduce-overhead", fullgraph=True)
def fused_adaln(x, scale, shift):
    """èåˆ AdaLN: norm â†’ scale â†’ shift â†’ silu"""
    normed = torch.nn.functional.layer_norm(x, x.shape[-1:])
    return (normed * (1 + scale) + shift) * torch.sigmoid(x) * x
```

### 4.3 Plan A éªŒè¯

```bash
# éªŒè¯è„šæœ¬
python scripts/validate_plan_a.py \
    --checkpoint_dir /root/.cache/openpi/checkpoints/pi05_libero \
    --num_runs 100

# é¢„æœŸè¾“å‡º:
# Baseline: 54.0 ms KV Cache
# Plan A: 35-40 ms KV Cache
# Speedup: 1.35-1.54x
```

### 4.4 Plan A æˆåŠŸæ ‡å‡†

- [ ] KV Cache: â‰¤ 35 ms
- [ ] Total: â‰¤ 150 ms
- [ ] Hz: â‰¥ 6.5 Hz
- [ ] LIBERO ç²¾åº¦: â‰¥ 95%

---

## äº”ã€Plan B: æ¿€è¿›ä¼˜åŒ–è·¯çº¿ (ä¸­-é«˜é£é™©)

**ç›®æ ‡**: 5.7 Hz â†’ 8.7 Hz
**é¢„æœŸ KV Cache**: 54 ms â†’ 12 ms
**å·¥ä½œé‡**: 4 å‘¨
**æ ¸å¿ƒ**: Triton W4A16 (INT4) Kernel

### 5.1 ä¸ºä»€ä¹ˆæ˜¯ INT4

| ç²¾åº¦ | æƒé‡å¤§å° | å¸¦å®½æ—¶é—´ | Thor æ”¯æŒ |
|------|----------|----------|-----------|
| BF16 | 3.62 GB | 18.1 ms | âœ… |
| FP8 | 1.81 GB | 9.0 ms | âš ï¸ TRT bug |
| FP4 | 0.90 GB | 4.5 ms | âŒ Segfault |
| **INT4** | **0.90 GB** | **4.5 ms** | ğŸ¯ **Triton æ‰‹å†™** |

### 5.2 INT4 é‡åŒ–ç­–ç•¥

**åªé‡åŒ– KV Cache MLPï¼Œä¸åŠ¨å…¶ä»–éƒ¨åˆ†**ï¼š

| ç»„ä»¶ | é‡åŒ– | åŸå›  |
|------|------|------|
| Vision Encoder | âŒ ä¸é‡åŒ– | CNN/ViT å¯¹é‡åŒ–æ•æ„Ÿ |
| LLM Attention | âŒ ä¸é‡åŒ– | è®¡ç®—é‡å°ï¼Œé‡åŒ–æ”¶ç›Šä½ |
| **LLM MLP** | âœ… **INT4** | ç“¶é¢ˆæ‰€åœ¨ï¼Œå¿…é¡»é‡åŒ– |
| Action Expert | âŒ ä¸é‡åŒ– | å·²ç»å¾ˆå¿« |

### 5.3 Triton W4A16 Kernel

```python
# src/openpi/inference/triton_int4_linear.py

import triton
import triton.language as tl
import torch

@triton.jit
def int4_dequant_matmul_kernel(
    # Pointers
    A_ptr,           # Input: [M, K] FP16
    W_packed_ptr,    # Weight: [K, N//2] INT8 (æ¯ byte å­˜ 2 ä¸ª INT4)
    W_scale_ptr,     # Scale: [K//group_size, N] FP16
    W_zero_ptr,      # Zero point: [K//group_size, N] INT4
    C_ptr,           # Output: [M, N] FP16
    # Dimensions
    M, N, K,
    # Strides
    stride_am, stride_ak,
    stride_wk, stride_wn,
    stride_cm, stride_cn,
    # Block sizes
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    BLOCK_K: tl.constexpr,
    GROUP_SIZE: tl.constexpr,
):
    """
    W4A16 MatMul: INT4 weight, FP16 activation

    å…³é”®ä¼˜åŒ–:
    1. æƒé‡ä» 4.5ms é™åˆ°ç†è®ºå¯ä»¥
    2. FP16 accumulation ä¿è¯ç²¾åº¦
    3. Per-group scale (æ¯ 128 ä¸ªå…ƒç´ ä¸€ä¸ª scale)
    """
    # Program ID
    pid_m = tl.program_id(0)
    pid_n = tl.program_id(1)

    # Offsets
    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    offs_k = tl.arange(0, BLOCK_K)

    # Accumulator (FP32 for precision)
    acc = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)

    # Main loop over K
    for k in range(0, K, BLOCK_K):
        # Load activation [BLOCK_M, BLOCK_K]
        a_ptrs = A_ptr + offs_m[:, None] * stride_am + (k + offs_k[None, :]) * stride_ak
        a = tl.load(a_ptrs, mask=offs_m[:, None] < M, other=0.0)

        # Load packed INT4 weights [BLOCK_K, BLOCK_N//2]
        # æ¯ä¸ª byte å­˜ 2 ä¸ª INT4
        w_packed_ptrs = W_packed_ptr + (k + offs_k[:, None]) * stride_wk + (offs_n[None, :] // 2)
        w_packed = tl.load(w_packed_ptrs)

        # Dequantize INT4 â†’ FP16
        # ä½ 4 ä½å’Œé«˜ 4 ä½
        w_low = (w_packed & 0x0F).to(tl.float16)   # 0-15
        w_high = ((w_packed >> 4) & 0x0F).to(tl.float16)

        # åŠ è½½ scale å’Œ zero point
        group_idx = (k + offs_k[:, None]) // GROUP_SIZE
        scale_ptrs = W_scale_ptr + group_idx * N + offs_n[None, :]
        scale = tl.load(scale_ptrs)

        zero_ptrs = W_zero_ptr + group_idx * (N // 2) + (offs_n[None, :] // 2)
        zero_packed = tl.load(zero_ptrs)
        zero_low = (zero_packed & 0x0F).to(tl.float16)
        zero_high = ((zero_packed >> 4) & 0x0F).to(tl.float16)

        # Dequant: w_fp16 = (w_int4 - zero) * scale
        # äº¤æ›¿å¤„ç†å¶æ•°åˆ—å’Œå¥‡æ•°åˆ—
        w_dequant = tl.where(
            (offs_n[None, :] % 2) == 0,
            (w_low - zero_low) * scale,
            (w_high - zero_high) * scale,
        )

        # MatMul accumulate
        acc += tl.dot(a, w_dequant).to(tl.float32)

    # Store result
    c_ptrs = C_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn
    tl.store(c_ptrs, acc.to(tl.float16), mask=(offs_m[:, None] < M) & (offs_n[None, :] < N))


class TritonINT4Linear(torch.nn.Module):
    """Triton INT4 Linear å±‚å°è£…"""

    def __init__(self, in_features: int, out_features: int, group_size: int = 128):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.group_size = group_size

        # Packed INT4 weights (æ¯ byte å­˜ 2 ä¸ª INT4)
        self.register_buffer(
            "w_packed",
            torch.zeros(in_features, out_features // 2, dtype=torch.uint8)
        )

        # Per-group scale
        num_groups = in_features // group_size
        self.register_buffer(
            "w_scale",
            torch.ones(num_groups, out_features, dtype=torch.float16)
        )

        # Per-group zero point (packed)
        self.register_buffer(
            "w_zero",
            torch.zeros(num_groups, out_features // 2, dtype=torch.uint8)
        )

    @classmethod
    def from_float(cls, linear: torch.nn.Linear, group_size: int = 128):
        """ä» FP16 Linear è½¬æ¢"""
        instance = cls(linear.in_features, linear.out_features, group_size)

        weight = linear.weight.data.float()  # [out, in]
        weight = weight.t()  # [in, out]

        # Per-group quantization
        K, N = weight.shape
        num_groups = K // group_size

        weight_grouped = weight.reshape(num_groups, group_size, N)

        # è®¡ç®— scale å’Œ zero point
        w_min = weight_grouped.min(dim=1).values  # [num_groups, N]
        w_max = weight_grouped.max(dim=1).values

        scale = (w_max - w_min) / 15.0  # INT4: 0-15
        zero = (-w_min / scale).round().clamp(0, 15)

        # é‡åŒ–
        weight_int4 = ((weight_grouped - w_min.unsqueeze(1)) / scale.unsqueeze(1)).round().clamp(0, 15)
        weight_int4 = weight_int4.reshape(K, N).to(torch.uint8)

        # Pack: 2 ä¸ª INT4 â†’ 1 ä¸ª byte
        w_packed = weight_int4[:, 0::2] | (weight_int4[:, 1::2] << 4)

        instance.w_packed.copy_(w_packed)
        instance.w_scale.copy_(scale.half())

        zero_packed = zero[:, 0::2].to(torch.uint8) | (zero[:, 1::2].to(torch.uint8) << 4)
        instance.w_zero.copy_(zero_packed)

        return instance

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        M = x.shape[0] * x.shape[1] if x.dim() == 3 else x.shape[0]
        x_2d = x.reshape(M, self.in_features)

        out = torch.empty(M, self.out_features, device=x.device, dtype=x.dtype)

        # Grid
        BLOCK_M, BLOCK_N, BLOCK_K = 32, 64, 128
        grid = (triton.cdiv(M, BLOCK_M), triton.cdiv(self.out_features, BLOCK_N))

        int4_dequant_matmul_kernel[grid](
            x_2d, self.w_packed, self.w_scale, self.w_zero, out,
            M, self.out_features, self.in_features,
            x_2d.stride(0), x_2d.stride(1),
            self.w_packed.stride(0), self.w_packed.stride(1),
            out.stride(0), out.stride(1),
            BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K,
            GROUP_SIZE=self.group_size,
        )

        return out.reshape(*x.shape[:-1], self.out_features)
```

### 5.4 INT4 é‡åŒ–æµç¨‹

```python
# scripts/quantize_kv_cache_mlp.py

def quantize_kv_cache_mlp(model, group_size=128):
    """åªé‡åŒ– KV Cache é˜¶æ®µçš„ MLP"""

    # éå† LLM backbone çš„æ¯ä¸€å±‚
    for layer_idx in range(model.config.depth):  # 18 layers
        layer = model.paligemma_backbone.layers[layer_idx]

        # é‡åŒ– gate_proj, up_proj, down_proj
        for name in ["gate_proj", "up_proj", "down_proj"]:
            original = getattr(layer.mlp, name)
            quantized = TritonINT4Linear.from_float(original, group_size)
            setattr(layer.mlp, name, quantized)

            # é‡Šæ”¾åŸå§‹æƒé‡
            del original
            torch.cuda.empty_cache()

        print(f"Layer {layer_idx}: MLP quantized to INT4")

    return model
```

### 5.5 Plan B éªŒè¯

```bash
# ç²¾åº¦éªŒè¯
python scripts/validate_int4_precision.py \
    --checkpoint_dir /root/.cache/openpi/checkpoints/pi05_libero \
    --group_size 128 \
    --num_samples 1000

# å»¶è¿ŸéªŒè¯
python scripts/benchmark_int4_kv_cache.py \
    --num_runs 100

# LIBERO è¯„æµ‹
python scripts/libero_eval_int4.py \
    --quick --mode int4
```

### 5.6 Plan B æˆåŠŸæ ‡å‡†

- [ ] INT4 vs FP16 Cosine: â‰¥ 0.98
- [ ] KV Cache: â‰¤ 15 ms
- [ ] Total: â‰¤ 120 ms
- [ ] Hz: â‰¥ 8.3 Hz
- [ ] LIBERO ç²¾åº¦: â‰¥ 90%

---

## å…­ã€Backup æ–¹æ¡ˆ

### 6.1 å„é˜¶æ®µ Backup

| é˜¶æ®µ | ä¸»æ–¹æ¡ˆ | Backup æ–¹æ¡ˆ | è§¦å‘æ¡ä»¶ |
|------|--------|-------------|----------|
| Phase 0 | ç¯å¢ƒéªŒè¯ | ç­‰å¾…è½¯ä»¶æ”¯æŒ | Triton/FlashInfer ä¸å¯ç”¨ |
| Plan A | FlashInfer | PyTorch Attention | FlashInfer åœ¨ Thor å¤±è´¥ |
| Plan B | Triton INT4 | CUTLASS INT4 | Triton kernel æ€§èƒ½å·® |
| ç²¾åº¦ | Per-group INT4 | Per-channel INT4 | ç²¾åº¦æŸå¤± > 10% |

### 6.2 Backup: CUTLASS INT4

å¦‚æœ Triton åœ¨ Thor ä¸Šæ€§èƒ½ä¸ä½³ï¼Œä½¿ç”¨ CUTLASS æ‰‹å†™ INT4 kernelï¼š

```cpp
// ä½¿ç”¨ CUTLASS çš„ INT4 GEMM
// è¿™æ˜¯æ›´åº•å±‚ä½†æ›´å¯æ§çš„æ–¹æ¡ˆ

#include <cutlass/gemm/device/gemm.h>

using Gemm = cutlass::gemm::device::Gemm<
    cutlass::int4b_t,                    // Element A (INT4)
    cutlass::layout::RowMajor,           // Layout A
    cutlass::half_t,                     // Element B (FP16)
    cutlass::layout::ColumnMajor,        // Layout B
    cutlass::half_t,                     // Element C (FP16)
    cutlass::layout::RowMajor,           // Layout C
    int32_t,                             // Accumulator (INT32)
    cutlass::arch::OpClassTensorOp,      // Use Tensor Cores
    cutlass::arch::Sm100                 // Thor = SM 10.0
>;
```

### 6.3 Backup: ç­‰å¾… NVIDIA ä¿®å¤

å¦‚æœæ‰€æœ‰æ–¹æ¡ˆéƒ½å¤±è´¥ï¼Œç­‰å¾…ï¼š
- TensorRT 10.15+ ä¿®å¤ FP8/FP4 scale bug
- TensorRT-LLM æ”¯æŒ Thor
- NVIDIA å‘å¸ƒ Thor ä¸“ç”¨ä¼˜åŒ–åº“

---

## ä¸ƒã€æœ€ç»ˆå†³ç­–æ ‘

```
                    Phase 0: ç¯å¢ƒéªŒè¯
                           â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                  â”‚                  â”‚
   Triton âœ…          Triton âŒ          å…¨å¤±è´¥
        â”‚                  â”‚                  â”‚
        â–¼                  â–¼                  â–¼
   æµ‹è¯•å¸¦å®½            Plan A Only       ç­‰å¾…è½¯ä»¶æ”¯æŒ
        â”‚              (ä¿å®ˆä¼˜åŒ–)         (7.0 Hz max)
        â”‚
   â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
   â”‚         â”‚
 >150GB/s  <150GB/s
   â”‚         â”‚
   â–¼         â–¼
Plan A    Plan B
(FP16)   (INT4 å¿…é¡»)
   â”‚         â”‚
   â–¼         â–¼
7.0 Hz    8.7 Hz
```

---

## å…«ã€æ—¶é—´è¡¨

| é˜¶æ®µ | å·¥ä½œå†…å®¹ | æ—¶é—´ | äº¤ä»˜ç‰© |
|------|----------|------|--------|
| **Week 1 Day 1-2** | Phase 0 ç¯å¢ƒéªŒè¯ | 2 å¤© | å†³ç­–æŠ¥å‘Š |
| **Week 1 Day 3-5** | Plan A å®ç° | 3 å¤© | CUDA Graph + Fusion |
| **Week 2** | Plan A éªŒè¯ + è°ƒä¼˜ | 5 å¤© | 7.0 Hz ç‰ˆæœ¬ |
| **Week 3** | Plan B INT4 Kernel | 5 å¤© | Triton INT4 å®ç° |
| **Week 4** | Plan B éªŒè¯ + è°ƒä¼˜ | 5 å¤© | 8.7 Hz ç‰ˆæœ¬ |
| **Week 5** | é›†æˆæµ‹è¯• + æ–‡æ¡£ | 5 å¤© | æœ€ç»ˆå‘å¸ƒ |

**æ€»è®¡: 5 å‘¨**

---

## ä¹ã€é£é™©è¯„ä¼°æ€»ç»“

| é£é™© | å¯èƒ½æ€§ | å½±å“ | ç¼“è§£ |
|------|--------|------|------|
| Triton Thor ä¸æ”¯æŒ | 30% | é«˜ | å›é€€ CUTLASS |
| INT4 ç²¾åº¦æŸå¤±å¤§ | 40% | ä¸­ | è°ƒæ•´ group_size |
| å¸¦å®½æµ‹é‡åä½ | 20% | ä¸­ | æ£€æŸ¥é©±åŠ¨/æ•£çƒ­ |
| FlashInfer å¤±è´¥ | 50% | ä½ | ç”¨ PyTorch |

---

## åã€æ‰§è¡Œæ¸…å•

### Day 1 ç«‹å³æ‰§è¡Œ

- [ ] è¿è¡Œ `phase0_environment_check.py`
- [ ] è®°å½• Thor å®é™…å¸¦å®½
- [ ] ç¡®è®¤ Triton/FlashInfer å¯ç”¨æ€§
- [ ] åŸºäºç»“æœå†³å®š Plan A æˆ– Plan B

### Week 1 äº¤ä»˜

- [ ] Plan A åŸºç¡€å®ç°
- [ ] CUDA Graph å…¨å›¾å½•åˆ¶
- [ ] åˆæ­¥æ€§èƒ½æ•°æ®

### Week 2 äº¤ä»˜

- [ ] Plan A å®Œæ•´éªŒè¯
- [ ] LIBERO ç²¾åº¦æµ‹è¯•
- [ ] å†³å®šæ˜¯å¦å¯åŠ¨ Plan B

### Week 3-4 äº¤ä»˜ (å¦‚éœ€ Plan B)

- [ ] INT4 é‡åŒ–å®ç°
- [ ] Triton Kernel è°ƒä¼˜
- [ ] æœ€ç»ˆæ€§èƒ½éªŒè¯

---

## åä¸€ã€Phase 0 éªŒè¯ç»“æœ (2026-02-08)

### æµ‹è¯•ç»“æœæ±‡æ€»

#### Triton æ€§èƒ½æµ‹è¯•

| å®ç° | å»¶è¿Ÿ | vs cuBLAS |
|------|------|-----------|
| torch.matmul (cuBLAS) | 0.47 ms | 1.00x |
| **Triton FP16 MatMul** | **1.14 ms** | **0.41x** |

**ç»“è®º**: âŒ Triton åœ¨ Thor SM 11.0 ä¸Šæ€§èƒ½åªæœ‰ cuBLAS çš„ 41%ï¼Œä¸å¯ç”¨ã€‚

#### é‡åŒ–åº“æµ‹è¯•

| æ–¹æ¡ˆ | æ€§èƒ½ vs BF16 | çŠ¶æ€ |
|------|-------------|------|
| Triton INT4 | 0.02x | âŒ Triton æœ¬èº«æ…¢ |
| torchao INT8 | 0.09x | âŒ ç¾éš¾æ€§æ€§èƒ½ |
| torchao INT4 | N/A | âŒ ç¼ºå°‘ fbgemm-gpu-genai |
| torch._int_mm (cuBLAS INT8) | 0.98x | âŒ æ— åŠ é€Ÿ |
| CUDA Graph | 1.00x | â‰ˆ æ— æå‡ |

### æœ€ç»ˆç»“è®º

**æ‰€æœ‰é‡åŒ–åŠ é€Ÿæ–¹æ¡ˆåœ¨ Thor ä¸Šéƒ½ä¸å¯ç”¨**:

1. **Triton**: åŸºç¡€ FP16 MatMul å°±æ¯” cuBLAS æ…¢ 2.5x
2. **torchao**: æ²¡æœ‰ Thor SM 11.0 ä¼˜åŒ–çš„ kernel
3. **cuBLAS INT8**: ä¸ FP16 æ€§èƒ½ç›¸åŒï¼Œæ— ç¡¬ä»¶åŠ é€Ÿ
4. **CUDA Graph**: å‡ ä¹æ²¡æœ‰æ”¶ç›Š

### æ›´æ–°åçš„ä¼˜åŒ–è·¯çº¿

```
åŸè®¡åˆ’:
  Plan A (ä¿å®ˆ): 5.7 Hz â†’ 7.0 Hz
  Plan B (æ¿€è¿›): 5.7 Hz â†’ 8.7 Hz

éªŒè¯å:
  âŒ Plan A/B éƒ½ä¸å¯è¡Œ
  âœ… å½“å‰æœ€ä½³: ç»´æŒ 5.7 Hz
```

### åç»­å»ºè®®

| æ–¹å‘ | ä¼˜å…ˆçº§ | é¢„æœŸæ”¶ç›Š | å·¥ä½œé‡ |
|------|--------|----------|--------|
| ç­‰å¾… NVIDIA Thor è½¯ä»¶æ”¯æŒ | é«˜ | æœªçŸ¥ | ç­‰å¾… |
| **å‡å°‘ denoising steps** | **é«˜** | **12 Hz** | **1 å¤©** |
| æ¨¡å‹è’¸é¦ | ä¸­ | 2-3x | 4-6 å‘¨ |
| å‡å°‘ Transformer å±‚æ•° | ä¸­ | 1.5x | 2 å‘¨ |

### æ‰§è¡Œæ¸…å•æ›´æ–°

- [x] è¿è¡Œ `phase0_environment_check.py`
- [x] æµ‹è¯• Triton FP16 æ€§èƒ½ (å¤±è´¥)
- [x] æµ‹è¯• torchao INT8/INT4 (å¤±è´¥)
- [x] æµ‹è¯• cuBLAS INT8 (æ— åŠ é€Ÿ)
- [x] æµ‹è¯• CUDA Graph (æ— æ”¶ç›Š)
- [ ] éªŒè¯ 3-step denoising ç²¾åº¦ (ä¸‹ä¸€æ­¥)

---

## Phase NVFP4: CUTLASS SM110a NVFP4 çªç ´ (2025-02-08)

### é‡å¤§å‘ç°

ç»è¿‡æ·±å…¥è°ƒç ”å’Œæµ‹è¯•ï¼ŒæˆåŠŸåœ¨ Thor SM110 ä¸Šè¿è¡Œ CUTLASS NVFP4 GEMMï¼

### æµ‹è¯•æ–¹æ³•è®º

1. **TensorRT-LLM FP4 Ops**: å‘ç° TRT-LLM çš„ NVFP4 å†…æ ¸åªç¼–è¯‘äº† SM90a/SM100/SM120ï¼Œ**ç¼ºå°‘ SM110**
2. **CUTLASS æºç ç¼–è¯‘**: ä¿®æ”¹ CUTLASS 72a example æ”¯æŒ SM110a æ¶æ„
3. **æ¶æ„æ£€æŸ¥ç»•è¿‡**: ä¿®æ”¹ `CUTLASS_ARCH_MMA_SM100_SUPPORTED` â†’ `CUTLASS_ARCH_MMA_SM110_SUPPORTED`

### æ€§èƒ½å¯¹æ¯” (NVFP4 vs cuBLAS BF16)

```
======================================================================
NVFP4 vs cuBLAS BF16 Benchmark on Thor SM110
======================================================================

Problem Size                   | BF16 (ms)    | NVFP4 (ms)   | Speedup
------------------------------------------------------------------------------------------
256x16384x2048                 | 0.356        | 0.082        | 4.34x
256x2048x16384                 | 0.449        | 0.057        | 7.82x
512x8192x2048                  | 0.231        | 0.082        | 2.82x
512x2048x8192                  | 0.162        | 0.061        | 2.63x
1024x4096x2048                 | 0.156        | 0.082        | 1.90x
```

### å…³é”®é™åˆ¶

1. **å°ºå¯¸é™åˆ¶**: æŸäº› M*N ç»„åˆä¼šå¤±è´¥ (å¦‚ 512x16384, 712x16384)
2. **å¯¹é½è¦æ±‚**: M å’Œ N éœ€è¦å¯¹é½åˆ°ç‰¹å®šå€æ•°
3. **Pi0.5 å®é™… batch 712 ä¸æ”¯æŒ**: éœ€è¦ padding æˆ–æ‹†åˆ†

### æ¶æ„å…¼å®¹æ€§å‘ç°

| æ¶æ„ | MMA æŒ‡ä»¤ | Thor å…¼å®¹æ€§ |
|------|----------|-------------|
| SM100 (B100/B200) | tcgen05.mma.blockscaled | âœ… éƒ¨åˆ†å…¼å®¹ |
| SM120 (RTX 50xx) | mma.sync.aligned.block_scale | âŒ ä¸å…¼å®¹ |
| SM110 (Thor) | tcgen05.mma.blockscaled | âœ… è‡ªç¼–è¯‘æˆåŠŸ |

### ç¼–è¯‘æ–¹æ³•

```bash
# åœ¨ Docker å®¹å™¨ä¸­
cd /workspace/external/cutlass_sm110_build

# å¤åˆ¶å¹¶ä¿®æ”¹ç¤ºä¾‹
cp /usr/local/lib/python3.12/dist-packages/cutlass_library/source/examples/72_blackwell_narrow_precision_gemm/72a_blackwell_nvfp4_bf16_gemm.cu .

# ä¿®æ”¹æ¶æ„æ£€æŸ¥
sed -i 's/CUTLASS_ARCH_MMA_SM100_SUPPORTED/CUTLASS_ARCH_MMA_SM110_SUPPORTED/g' 72a_blackwell_nvfp4_bf16_gemm.cu

# ç¼–è¯‘
CUTLASS_PATH=/usr/local/lib/python3.12/dist-packages/cutlass_library/source
nvcc -O3 -std=c++17 -arch=sm_110a \
    --expt-relaxed-constexpr \
    -I$CUTLASS_PATH/include \
    -I$CUTLASS_PATH/tools/util/include \
    -I$CUTLASS_PATH/examples/common \
    72a_blackwell_nvfp4_bf16_gemm.cu \
    -o nvfp4_gemm_sm110a
```

### ç†è®ºåŠ é€Ÿæ½œåŠ›

å‡è®¾ NVFP4 å¯ä»¥ç”¨äº MLP:
- å½“å‰ MLP æƒé‡è¯»å–: 18.1 ms (BF16)
- NVFP4 ç†è®º: 4.5 ms (1/4 æƒé‡)
- å®æµ‹åŠ é€Ÿ: 2.8x - 7.8x (å–å†³äºå°ºå¯¸)

| ç»„ä»¶ | å½“å‰ | NVFP4 ä¼˜åŒ– |
|------|------|-----------|
| MLP GEMM | 18.1 ms | ~4-6 ms |
| KV Cache æ€»è®¡ | 54 ms | ~25-35 ms |

### ä¸‹ä¸€æ­¥è®¡åˆ’

1. **å°è£… PyTorch Op**: å°† CUTLASS NVFP4 kernel å°è£…ä¸ºå¯è°ƒç”¨çš„ PyTorch æ‰©å±•
2. **é‡åŒ–æƒé‡**: å®ç° Pi0.5 æ¨¡å‹æƒé‡çš„ FP4 é‡åŒ–
3. **é›†æˆæµ‹è¯•**: éªŒè¯ç«¯åˆ°ç«¯ç²¾åº¦å’Œæ€§èƒ½
4. **è§£å†³å°ºå¯¸é™åˆ¶**: é€šè¿‡ padding æˆ–æ‹†åˆ†æ”¯æŒ batch=712

### ç¤¾åŒºåé¦ˆå»ºè®®

å»ºè®®å‘ NVIDIA åé¦ˆ:
1. TensorRT-LLM ç¼ºå°‘ SM110 NVFP4 å†…æ ¸ç¼–è¯‘
2. CUTLASS æ–‡æ¡£ç¼ºå°‘ SM110 ç¤ºä¾‹
3. è¯·æ±‚å®˜æ–¹ Thor NVFP4 æ”¯æŒ
