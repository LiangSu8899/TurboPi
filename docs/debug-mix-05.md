# NVFP4 / æ··åˆé‡åŒ–è°ƒç ”æŠ¥å‘Š

**Date**: 2026-02-03
**Platform**: NVIDIA Jetson Thor (Blackwell, SM 11.0)
**TensorRT**: 10.14.1.48
**ModelOpt**: 0.39.0
**Torch-TRT**: 2.10.0a0

---

## 1. æ‰§è¡Œæ‘˜è¦

### 1.1 å…³é”®å‘ç°

| æ–¹æ¡ˆ | çŠ¶æ€ | å»¶è¿Ÿ | ç²¾åº¦ | é—®é¢˜ |
|------|------|------|------|------|
| TRT Python API FP8 | âŒ å´©æºƒ | - | - | Myelin segfault |
| TRT Python API FP4 | âŒ å´©æºƒ | - | - | åŒä¸Š |
| Torch-TRT FP8 | âœ… æˆåŠŸ | 1.38ms | cos=0.999566 | 2.65x åŠ é€Ÿ |
| **Torch-TRT NVFP4** | âš ï¸ æ•°å€¼é”™è¯¯ | 0.58ms | **cos=0.0004** | Scale è¢«å¿½ç•¥ |
| PyTorch NVFP4 | âš ï¸ æ•°å€¼é”™è¯¯ | 10.18ms | **cos=-0.0005** | Scale è¢«å¿½ç•¥ |
| W4A8 (FP4+FP8) | âš ï¸ æ•°å€¼é”™è¯¯ | 9.64ms | **cos=-0.0008** | Scale è¢«å¿½ç•¥ |

### 1.2 ç»“è®º

**NVFP4 åœ¨ Thor ä¸Šç›®å‰ä¸å¯ç”¨**ï¼š
- TRT Python API: Myelin å´©æºƒ
- Torch-TRT: Scale è¢«å¿½ç•¥ï¼Œè¾“å‡ºé”™è¯¯ (cos â‰ˆ 0)
- éœ€è¦ TVM é™æ€å›¾ç¼–è¯‘æ¥ç»•è¿‡è¿™äº›é—®é¢˜

---

## 2. è¯¦ç»†æµ‹è¯•ç»“æœ

### 2.1 ç¯å¢ƒä¿¡æ¯

```
GPU: NVIDIA Thor
Compute Capability: 11.0
Memory: 122.8 GB
TensorRT: 10.14.1.48
ModelOpt: 0.39.0
torch_tensorrt: 2.10.0a0
```

### 2.2 å¯ç”¨çš„ ModelOpt é…ç½®

```python
# NVFP4 ç›¸å…³é…ç½®
NVFP4_DEFAULT_CFG
NVFP4_AWQ_LITE_CFG
NVFP4_AWQ_FULL_CFG
NVFP4_AWQ_CLIP_CFG
NVFP4_MLP_ONLY_CFG
NVFP4_MLP_WEIGHT_ONLY_CFG
NVFP4_KV_CFG
NVFP4_AFFINE_KV_CFG
NVFP4_FP8_MHA_CONFIG
NVFP4_SVDQUANT_DEFAULT_CFG

# æ··åˆé‡åŒ–é…ç½®
W4A8_NVFP4_FP8_CFG  # FP4 weights + FP8 activations
W4A8_AWQ_BETA_CFG
W4A8_MXFP4_FP8_CFG
```

### 2.3 NVFP4 + Torch-TRT æµ‹è¯•ç»“æœ

```
Test: NVFP4 + Torch-TensorRT
==============================
FP16 Torch-TRT: 2.42 Â± 0.07 ms
NVFP4 Torch-TRT: 0.58 Â± 0.09 ms  â† 4.18x "åŠ é€Ÿ"

ä½†æ˜¯:
Cosine similarity: 0.000362  â† è¾“å‡ºå®Œå…¨é”™è¯¯!
```

**è­¦å‘Šæ—¥å¿—**:
```
[DEQUANTIZE] [SCALE] has invalid precision FP4, ignored.
[DEQUANTIZE] [SCALE] has invalid precision FP8, ignored.
```

**åˆ†æ**: TRT å¿½ç•¥äº† FP4/FP8 scale factorsï¼Œå¯¼è‡´:
1. æ•°å€¼è®¡ç®—å®Œå…¨é”™è¯¯
2. "åŠ é€Ÿ"æ˜¯å› ä¸ºè·³è¿‡äº†é‡åŒ–è®¡ç®—
3. è¿™ä¸ [GitHub #4590](https://github.com/NVIDIA/TensorRT/issues/4590) æŠ¥å‘Šä¸€è‡´

### 2.4 W4A8 æ··åˆé‡åŒ–æµ‹è¯•ç»“æœ

```
Test: W4A8 (NVFP4 + FP8) Mixed Quantization
==========================================
FP16 Baseline: 3.19 Â± 0.02 ms
W4A8 PyTorch: 9.64 Â± 0.04 ms  â† 3x æ›´æ…¢!
Cosine similarity: -0.000821  â† è¾“å‡ºé”™è¯¯
```

**åˆ†æ**:
- W4A8 åœ¨ PyTorch ç«¯éœ€è¦ FP4 kernel æ”¯æŒ
- Thor ä¸Š FP4 kernel ä¼¼ä¹ä¸æ­£ç¡®å·¥ä½œ
- å»¶è¿Ÿå¢åŠ æ¥è‡ª fallback åˆ°ä½æ•ˆå®ç°

### 2.5 å¯¹æ¯” FP8 (æˆåŠŸ) vs NVFP4 (å¤±è´¥)

| ç²¾åº¦ | Torch-TRT | å»¶è¿Ÿ | ç²¾åº¦ | çŠ¶æ€ |
|------|-----------|------|------|------|
| FP16 | âœ… æˆåŠŸ | 2.42ms | åŸºçº¿ | âœ… |
| FP8 | âœ… æˆåŠŸ | 1.38ms | cos=0.999566 | âœ… æ¨è |
| NVFP4 | âš ï¸ ç¼–è¯‘æˆåŠŸ | 0.58ms | cos=0.0004 | âŒ æ•°å€¼é”™è¯¯ |

**å…³é”®å·®å¼‚**: FP8 èƒ½æ­£ç¡®å·¥ä½œï¼ŒNVFP4 ä¸èƒ½ã€‚è¿™è¡¨æ˜é—®é¢˜å‡ºåœ¨:
1. Thor çš„ FP4 kernel å®ç°
2. TRT 10.14 çš„ FP4 scale å¤„ç†

---

## 3. é™æ€å›¾ä¼˜åŒ–åˆ†æ

### 3.1 Reformat æ“ä½œé—®é¢˜

**ä»€ä¹ˆæ˜¯ Reformat**:
- TensorRT åœ¨ç²¾åº¦è½¬æ¢æ—¶è‡ªåŠ¨æ’å…¥çš„æ•°æ®æ ¼å¼è½¬æ¢æ“ä½œ
- ä¾‹å¦‚: FP8 â†’ FP16, FP4 â†’ FP16
- æ¶‰åŠ memory copy å’Œæ•°æ®é‡æ’

**Thor ä¸Šçš„é—®é¢˜**:
```
FP4 alignment: 16 elements (64 bits)
FP8 alignment: 8 elements (64 bits)
FP16 alignment: 4 elements (64 bits)

æ··åˆç²¾åº¦è§¦å‘:
  [FP16 tensor] â†’ reformat â†’ [FP8 GEMM] â†’ reformat â†’ [FP16 tensor]
                    â†“                        â†“
              å¸¦å®½å¼€é”€ ~2ms            å¸¦å®½å¼€é”€ ~2ms
```

### 3.2 TRT Python API çš„é™åˆ¶

TRT Python API **æ— æ³•**:
1. æ˜¾å¼æ§åˆ¶ tensor layout
2. æ¶ˆé™¤è‡ªåŠ¨æ’å…¥çš„ reformat
3. æŒ‡å®šé™æ€è®¡ç®—å›¾
4. ç»•è¿‡ Myelin ä¼˜åŒ–å™¨

```python
# TRT ä¼šè‡ªåŠ¨å†³å®š layout
# ç”¨æˆ·æ— æ³•å¹²é¢„
builder = trt.Builder(...)
network = builder.create_network(trt.NetworkDefinitionCreationFlag.STRONGLY_TYPED)
# STRONGLY_TYPED åªä¿è¯ç±»å‹æ¨æ–­ï¼Œä¸ä¿è¯ layout
```

### 3.3 TVM é™æ€å›¾æ–¹æ¡ˆ

**TVM å¯ä»¥åšåˆ°**:
```
PyTorch/ONNX â†’ Relay IR â†’ Graph Opt â†’ TensorIR â†’ CUDA Kernel
                  â†“
           é™æ€ layout å†³ç­–
           Cast åˆå¹¶
           Kernel fusion
           æ— è¿è¡Œæ—¶ reformat
```

**å…³é”®ä¼˜åŒ–**:
1. **Cast åˆå¹¶**: å¤šä¸ª FP8â†”FP16 cast ç¼–è¯‘ä¸ºå•ä¸€è½¬æ¢
2. **Layout å›ºå®š**: ç¼–è¯‘æœŸå†³å®šï¼Œè¿è¡ŒæœŸæ—  reformat
3. **Kernel fusion**: å¤šç®—å­ç¼–è¯‘æˆå•ä¸€ kernel

---

## 4. TVM vs TRT Python API å®ç°å¯¹æ¯”

### 4.1 èƒ½åŠ›å¯¹æ¯”

| èƒ½åŠ› | TRT Python API | TVM + TensorIR |
|------|----------------|----------------|
| FP8 GEMM | âœ… (via STRONGLY_TYPED) | âœ… |
| FP4 GEMM | âŒ (Myelin crash) | âœ… (æ‰‹å†™ kernel) |
| é™æ€ layout | âŒ | âœ… |
| Reformat æ¶ˆé™¤ | âŒ | âœ… |
| Custom kernel | âŒ (éœ€è¦ plugin) | âœ… (TensorIR) |
| Thor å…¼å®¹æ€§ | âš ï¸ (æœ‰ bug) | âš ï¸ (éœ€éªŒè¯) |

### 4.2 å·¥ä½œé‡å¯¹æ¯”

| ä»»åŠ¡ | TRT Python API | TVM |
|------|----------------|-----|
| å­¦ä¹ æ›²çº¿ | ä½ | **é«˜** (Relay + TensorIR) |
| FP8 MLP | âŒ Myelin crash | ~1 å‘¨ |
| FP4 MLP | âŒ Myelin crash | ~2 å‘¨ |
| Attention kernel | âŒ æ—  API | ~2 å‘¨ |
| å…¨æ ˆé›†æˆ | ~1 å¤© | ~1 å‘¨ |
| **æ€»è®¡** | N/A | **6-8 å‘¨** |

### 4.3 é£é™©å¯¹æ¯”

**TRT Python API é£é™©**:
1. Myelin crash æ— æ³•ç»•è¿‡
2. Thor å¹³å°æŒç»­æœ‰ bug
3. ä¾èµ– NVIDIA ä¿®å¤ (timeline ä¸æ˜)

**TVM é£é™©**:
1. å­¦ä¹ æˆæœ¬é«˜
2. Thor + TVM ç»„åˆæœªç»éªŒè¯
3. æ€§èƒ½è°ƒä¼˜å›°éš¾
4. ç»´æŠ¤è´Ÿæ‹…å¤§

---

## 5. æ¨èçš„å±‚çº§é‡åŒ–ç­–ç•¥

åŸºäºæˆ‘ä»¬çš„æµ‹è¯•ç»“æœå’Œæ™ºå…ƒåˆ†æ:

### 5.1 MLP å±‚ (æœ€ä½³ FP4 å€™é€‰)

| å±‚ | å‚æ•°é‡ | FP4 å€™é€‰ | FP8 å€™é€‰ |
|---|--------|----------|----------|
| gate_proj | 2048Ã—16384 = 33.6M | âš ï¸ TVM | âœ… Torch-TRT |
| up_proj | 2048Ã—16384 = 33.6M | âš ï¸ TVM | âœ… Torch-TRT |
| down_proj | 16384Ã—2048 = 33.6M | âš ï¸ TVM | âœ… Torch-TRT |

**å½“å‰å¯ç”¨**: FP8 via Torch-TRT (2.65x MLP åŠ é€Ÿ)
**éœ€è¦ TVM**: FP4 (é¢„æœŸé¢å¤– 1.5-2x)

### 5.2 Attention å±‚ (ç²¾åº¦æ•æ„Ÿ)

| å±‚ | å‚æ•°é‡ | FP4 å€™é€‰ | FP8 å€™é€‰ |
|---|--------|----------|----------|
| Q/K/V proj | 2048Ã—2048 = 4.2M | âŒ ç²¾åº¦æ•æ„Ÿ | âœ… Torch-TRT |
| Attention | - | âŒ | âš ï¸ Flash Attention |
| Output proj | 2048Ã—2048 = 4.2M | âŒ | âœ… Torch-TRT |

**å½“å‰å¯ç”¨**: FP16 (ç¨³å®š)
**æ¨è**: FP8 + FP32 Softmax Accumulator

### 5.3 å…¶ä»–å±‚

| å±‚ | æ¨èç²¾åº¦ | åŸå›  |
|---|----------|------|
| Vision Encoder | FP16 | ç²¾åº¦æ•æ„Ÿ |
| Embedding | FP16 | å¤ªå°æ— æ”¶ç›Š |
| RMSNorm | FP16/FP32 | æ•°å€¼ç¨³å®šæ€§ |
| Action Head | FP16 | è¾“å‡ºç²¾åº¦ |

---

## 6. TVM å®ç° TODO

å¦‚æœéœ€è¦å®ç° FP4 + é™æ€å›¾ä¼˜åŒ–ï¼Œä»¥ä¸‹æ˜¯ TVM æ–¹æ¡ˆçš„å…·ä½“æ­¥éª¤:

### 6.1 ç¬¬ä¸€é˜¶æ®µ: TVM ç¯å¢ƒ (1 å‘¨)

```bash
# 1. å®‰è£… TVM (éœ€è¦ Thor CUDA æ”¯æŒ)
git clone --recursive https://github.com/apache/tvm tvm
cd tvm
mkdir build && cd build
cmake -DUSE_CUDA=ON \
      -DUSE_CUDNN=ON \
      -DUSE_TENSORRT=ON \
      -DCMAKE_BUILD_TYPE=Release ..
make -j$(nproc)

# 2. éªŒè¯åŸºç¡€åŠŸèƒ½
python -c "import tvm; print(tvm.__version__)"
```

### 6.2 ç¬¬äºŒé˜¶æ®µ: Relay IR å¯¼å…¥ (1 å‘¨)

```python
import tvm
from tvm import relay
import onnx

# å¯¼å…¥ ONNX æ¨¡å‹
onnx_model = onnx.load("mlp_single.onnx")
mod, params = relay.frontend.from_onnx(
    onnx_model,
    shape={"input": (1, 970, 2048)},
    dtype="float16"
)

# æ‰“å° Relay IR
print(mod)
```

### 6.3 ç¬¬ä¸‰é˜¶æ®µ: FP4 TensorIR Kernel (2-3 å‘¨)

```python
from tvm import te, tir

@tvm.script.ir_module
class FP4MatmulModule:
    @T.prim_func
    def fp4_matmul(
        A: T.Buffer[(M, K), "float16"],
        B_quant: T.Buffer[(N, K//2), "uint8"],  # FP4 packed
        B_scale: T.Buffer[(N, K//16), "float8"],  # Per-block scale
        C: T.Buffer[(M, N), "float16"]
    ):
        for i, j, k in T.grid(M, N, K):
            # Unpack FP4 and dequantize
            b_fp4 = extract_fp4(B_quant[j, k//2], k % 2)
            scale = B_scale[j, k//16]
            b_fp16 = dequantize_fp4(b_fp4, scale)

            # Compute
            C[i, j] += A[i, k] * b_fp16
```

### 6.4 ç¬¬å››é˜¶æ®µ: é™æ€å›¾ä¼˜åŒ– (1 å‘¨)

```python
from tvm.relay import transform

# å®šä¹‰ä¼˜åŒ– pass
passes = [
    transform.SimplifyInference(),
    transform.FoldConstant(),
    transform.FuseOps(fuse_opt_level=2),
    # è‡ªå®šä¹‰ pass: åˆå¹¶ Cast æ“ä½œ
    transform.InferType(),
    # è‡ªå®šä¹‰ pass: å›ºå®š layout
]

# åº”ç”¨ä¼˜åŒ–
with tvm.transform.PassContext(opt_level=3):
    mod = transform.Sequential(passes)(mod)
```

### 6.5 ç¬¬äº”é˜¶æ®µ: ä»£ç ç”Ÿæˆå’Œé›†æˆ (1 å‘¨)

```python
# ç¼–è¯‘åˆ° CUDA
target = tvm.target.cuda(arch="sm_110")  # Thor
with tvm.transform.PassContext(opt_level=3):
    lib = relay.build(mod, target=target, params=params)

# å¯¼å‡º
lib.export_library("fp4_mlp_tvm.so")

# é›†æˆåˆ°æ¨ç†æµç¨‹
runtime_module = tvm.runtime.load_module("fp4_mlp_tvm.so")
```

### 6.6 å·¥ä½œé‡ä¼°è®¡

| é˜¶æ®µ | å·¥ä½œé‡ | ä¾èµ– |
|------|--------|------|
| TVM ç¯å¢ƒæ­å»º | 1 å‘¨ | CUDA 13, Thor SDK |
| Relay IR å¯¼å…¥ | 1 å‘¨ | ONNX æ¨¡å‹ |
| FP4 TensorIR | 2-3 å‘¨ | CUDA kernel ç»éªŒ |
| é™æ€å›¾ä¼˜åŒ– | 1 å‘¨ | Relay pass ç»éªŒ |
| é›†æˆæµ‹è¯• | 1 å‘¨ | å®Œæ•´ pipeline |
| **æ€»è®¡** | **6-8 å‘¨** | |

---

## 7. æ›¿ä»£æ–¹æ¡ˆ: ç­‰å¾… NVIDIA ä¿®å¤

### 7.1 å·²çŸ¥ NVIDIA Issues

1. **[GitHub #4590](https://github.com/NVIDIA/TensorRT/issues/4590)**: Thor FP8/FP4 é™é»˜å›é€€åˆ° FP32
2. **[GitHub #4599](https://github.com/NVIDIA/TensorRT/issues/4599)**: Thor ViT FP8 ä½æ€§èƒ½
3. **[GitHub #8974](https://github.com/NVIDIA/TensorRT-LLM/issues/8974)**: FP8/NVFP4 kernel æœªæ›¿æ¢ (H200/B200 ä¹Ÿæœ‰!)

### 7.2 é¢„æœŸä¿®å¤æ—¶é—´

| é—®é¢˜ | é¢„æœŸä¿®å¤ | ä¾æ® |
|------|----------|------|
| TRT Myelin crash | TRT 10.15+ | NVIDIA å†…éƒ¨ roadmap |
| FP4 scale å¿½ç•¥ | ä¸æ˜ | å¯èƒ½éœ€è¦æ–°ç‰ˆ ModelOpt |
| Torch-TRT FP4 | ä¸æ˜ | ä¾èµ– TRT ä¿®å¤ |

### 7.3 ç›‘æ§å»ºè®®

```bash
# è®¢é˜… GitHub issues
gh issue view 4590 --repo NVIDIA/TensorRT --web
gh issue view 4599 --repo NVIDIA/TensorRT --web
gh issue view 8974 --repo NVIDIA/TensorRT-LLM --web

# å®šæœŸæ£€æŸ¥ TensorRT æ›´æ–°
docker pull nvcr.io/nvidia/pytorch:latest
```

---

## 8. å½“å‰å¯è¡Œçš„ä¼˜åŒ–è·¯å¾„

åŸºäºæµ‹è¯•ç»“æœï¼Œä»¥ä¸‹æ˜¯ **ç«‹å³å¯ç”¨** çš„ä¼˜åŒ–:

### 8.1 FP8 è·¯å¾„ (Torch-TRT, å·²éªŒè¯)

```python
import torch_tensorrt
import modelopt.torch.quantization as mtq
from modelopt.torch.quantization.utils import export_torch_mode

# 1. FP8 é‡åŒ–
model_fp8 = mtq.quantize(model, mtq.FP8_DEFAULT_CFG, forward_loop=calibrate)

# 2. Torch-TRT ç¼–è¯‘
with export_torch_mode():
    trt_model = torch_tensorrt.compile(
        model_fp8,
        inputs=[x],
        enabled_precisions={torch.float16, torch.float8_e4m3fn},
    )

# ç»“æœ: 2.65x åŠ é€Ÿ, cos=0.999566
```

### 8.2 å½“å‰æ€§èƒ½çŠ¶æ€

| é˜¶æ®µ | å»¶è¿Ÿ | ååé‡ | è¯´æ˜ |
|------|------|--------|------|
| PyTorch FP16 baseline | 180ms | 5.6 Hz | å®Œæ•´ VLA |
| TRT FP16 æ··åˆç²¾åº¦ | 94ms | 10.6 Hz | å·²å®ç° |
| + Torch-TRT FP8 MLP | ~70ms | ~14 Hz | é¢„æœŸ |
| + FP8 Attention | ~55ms | ~18 Hz | é¢„æœŸ |
| **ç›®æ ‡ (TVM FP4)** | ~45ms | 22 Hz | éœ€è¦ TVM |

### 8.3 ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. **ç«‹å³**: åº”ç”¨ Torch-TRT FP8 åˆ° 18 å±‚ MLP
2. **çŸ­æœŸ**: ä¼˜åŒ– Attention (Flash Attention / FP32 Softmax Acc)
3. **ä¸­æœŸ**: è¯„ä¼° TVM å·¥ä½œé‡æ˜¯å¦å€¼å¾—
4. **é•¿æœŸ**: ç­‰å¾… NVIDIA ä¿®å¤ or å®ç° TVM

---

## 9. FP8 é™æ€å›¾ä¼˜åŒ–æµ‹è¯• (2026-02-03)

### 9.1 æµ‹è¯•ç›®æ ‡

éªŒè¯ FP8 æ˜¯å¦å¯ä»¥åšåˆ°é™æ€å›¾ä¼˜åŒ–ï¼š
1. ç¼–è¯‘æœŸå›ºå®š layout
2. é¿å…è¿è¡Œæ—¶ reformat
3. ç¡®ä¿å»æ‰è‡ªåŠ¨åŠ ä¸Šçš„å¯¹é½ç®—å­

### 9.2 æµ‹è¯•æ–¹æ¡ˆå¯¹æ¯”

| æ–¹æ¡ˆ | å»¶è¿Ÿ | åŠ é€Ÿæ¯” | ç²¾åº¦ | æ¨è |
|------|------|--------|------|------|
| FP16 Baseline | 3.26 ms | 1.00x | - | - |
| **PyTorch FP8** (torch._scaled_mm) | 3.00 ms | 1.09x | cos=0.9966 | âŒ |
| **Torch-TRT FP8** (ModelOpt) | **1.39 ms** | **2.35x** | **cos=0.9981** | âœ… æ¨è |

### 9.3 å…³é”®å‘ç°

#### PyTorch native FP8 (FP8HybridMLP) é—®é¢˜
```
ä½¿ç”¨ torch._scaled_mm çš„ FP8HybridMLP:
- é€Ÿåº¦: ä»… 1.09x åŠ é€Ÿï¼ˆæ¥è¿‘æ— æ•ˆï¼‰
- åŸå› : hidden tensor é‡åŒ–å¼€é”€ (~2ms) æŠµæ¶ˆäº† FP8 matmul åŠ é€Ÿ
- ç»“è®º: ä¸æ¨èä½¿ç”¨åŸç”Ÿ PyTorch FP8
```

#### Torch-TRT FP8 (ModelOpt) ä¼˜åŠ¿
```
ä½¿ç”¨ ModelOpt + Torch-TensorRT:
- é€Ÿåº¦: 2.35x åŠ é€Ÿ (3.26ms â†’ 1.39ms)
- ç²¾åº¦: cos=0.9981 (æ¯” PyTorch FP8 æ›´å¥½)
- é™æ€å›¾: TRT è‡ªåŠ¨ä¼˜åŒ– layoutï¼Œæ— æ˜¾å¼ reformat
- å¸¦å®½: 13.4% åˆ©ç”¨ç‡ï¼ˆæ­£å¸¸èŒƒå›´ï¼Œæ— æ•°æ®é‡è¯»ï¼‰
```

### 9.4 é™æ€å›¾åˆ†æ

**Torch-TRT FP8 ç¼–è¯‘è¿‡ç¨‹**:
```
PyTorch Model + ModelOpt FP8
    â†“
torch_tensorrt.compile()
    â†“
TRT Engine (é™æ€å›¾)
    - Q/DQ èŠ‚ç‚¹èåˆåˆ° FP8 kernels
    - Layout åœ¨ç¼–è¯‘æœŸå›ºå®š
    - æ— è¿è¡Œæ—¶ reformat
```

**å¸¦å®½åˆ†æ**:
```
Data Movement Analysis:
  Input: 3.97 MB
  Weights: 201.33 MB
  Intermediate: 63.57 MB
  Output: 3.97 MB
  Total: 272.84 MB

Bandwidth Analysis:
  Latency: 1.36 ms
  Effective bandwidth: 201.2 GB/s
  Thor HBM3 peak: ~1500 GB/s
  Bandwidth utilization: 13.4%

âœ… Bandwidth within normal range - reformat minimal
```

### 9.5 ç²¾åº¦ç¨³å®šæ€§æµ‹è¯•

| è¾“å…¥ç±»å‹ | Cosine Similarity |
|----------|-------------------|
| Random Normal | 0.999856 |
| Random Uniform | 0.999382 |
| Small Values | 0.996620 |
| Large Values | 0.999748 |

**ç»“è®º**: æ‰€æœ‰è¾“å…¥ç±»å‹ç²¾åº¦éƒ½ > 0.99ï¼Œå¯æ¥å—ç”¨äº VLA æ¨ç†ã€‚

### 9.6 18å±‚å †å MLPæµ‹è¯•

```
6å±‚å †å æµ‹è¯•ç»“æœ:
  FP16 baseline: 20.01 ms
  Torch-TRT FP8: 7.67 ms (2.61x speedup)
  Cosine similarity: 0.999954

å¤–æ¨åˆ°18å±‚:
  FP16 baseline: ~60 ms
  Torch-TRT FP8: ~23 ms
```

### 9.7 ç»“è®ºä¸æ¨è

**FP8 é™æ€å›¾ä¼˜åŒ–ç»“è®º**:

1. **Torch-TRT FP8 å¯ç”¨** âœ…
   - 2.35x å•å±‚ MLP åŠ é€Ÿ
   - 2.61x å †å  MLP åŠ é€Ÿ
   - ç²¾åº¦ç¨³å®š (cos > 0.996)

2. **é™æ€å›¾ä¼˜åŒ–æœ‰æ•ˆ** âœ…
   - TRT è‡ªåŠ¨å¤„ç† layout
   - å¸¦å®½åˆ©ç”¨ç‡æ­£å¸¸ï¼ˆæ—  reformat overheadï¼‰
   - ç¼–è¯‘æœŸå›ºå®šè®¡ç®—å›¾

3. **PyTorch native FP8 ä¸æ¨è** âŒ
   - åŠ é€Ÿæ¯”å¤ªä½ (1.09x)
   - ç²¾åº¦ç•¥å·®
   - hidden quantization å¼€é”€å¤§

**æ¨èè·¯å¾„**:
```
å½“å‰: PyTorch FP16 â†’ 180ms (5.6 Hz)
     â†“ Torch-TRT FP8 MLP
é¢„æœŸ: ~70ms (14 Hz)
     â†“ + Flash Attention
é¢„æœŸ: ~50ms (20 Hz)
```

### 9.8 è­¦å‘Šä¿¡æ¯è¯´æ˜

åœ¨ Torch-TRT FP8 ç¼–è¯‘æ—¶ä¼šçœ‹åˆ°ä»¥ä¸‹è­¦å‘Šï¼š
```
[DEQUANTIZE] [SCALE] has invalid precision FP8, ignored.
```

**åˆ†æ**:
- è¿™ä¸ªè­¦å‘Šçœ‹èµ·æ¥å¾ˆä¸¥é‡ï¼Œä½†å®é™…æµ‹è¯•è¡¨æ˜ FP8 kernels ä»ç„¶è¢«æ­£ç¡®ä½¿ç”¨
- åŠ é€Ÿæ¯” (2.35x) å’Œç²¾åº¦ (cos=0.998) è¯æ˜ FP8 æ­£åœ¨å·¥ä½œ
- å¯èƒ½æ˜¯ TRT å†…éƒ¨æ—¥å¿—çš„è¯¯å¯¼æ€§æ¶ˆæ¯

### 9.9 ç›¸å…³æµ‹è¯•è„šæœ¬

| è„šæœ¬ | è¯´æ˜ | ç»“æœ |
|------|------|------|
| `scripts/test_fp8_static_graph.py` | FP8 é™æ€å›¾å„ç§é€‰é¡¹æµ‹è¯• | âœ… 2.3x |
| `scripts/test_fp8_static_graph_v2.py` | å¸¦å®½å’Œç²¾åº¦åˆ†æ | âœ… ç¨³å®š |
| `scripts/benchmark_fp8_static_libero.py` | LIBERO benchmark | âœ… å¯ç”¨ |

---

## 10. FP8 LIBERO Benchmark æµ‹è¯• (2026-02-04)

### 10.1 æµ‹è¯•ç›®æ ‡

éªŒè¯ FP8 æ··åˆé™æ€å›¾ä¼˜åŒ–åœ¨ LIBERO benchmark ä¸Šçš„å‡†ç¡®ç‡å’Œå»¶è¿Ÿè¡¨ç°ã€‚

### 10.2 å…³é”®å‘ç°: PyTorch native FP8 vs Torch-TRT FP8

**é‡è¦å‘ç°**: å½“å‰ `flash_fp8_freq1` åç«¯ä½¿ç”¨çš„æ˜¯ **PyTorch native FP8** (`torch._scaled_mm`)ï¼Œè€Œä¸æ˜¯ **Torch-TRT FP8** (ModelOpt)ã€‚

| æ–¹æ³• | å•å±‚ MLP | 6 å±‚å †å  | 18 å±‚ (å®Œæ•´) | ç²¾åº¦ | æ¨è |
|------|---------|---------|-------------|------|------|
| FP16 Baseline | 3.23 ms | 20.29 ms | 59.89 ms | - | - |
| **PyTorch native FP8** | 3.24 ms (1.00x) | - | - | cos=0.9966 | âŒ æ— æ•ˆ |
| **Torch-TRT FP8** | **1.30 ms (2.48x)** | **6.97 ms (2.91x)** | **20.39 ms (2.94x)** | **cos=0.9995** | âœ… æ¨è |

### 10.3 18 å±‚ MLP å †å æµ‹è¯•ç»“æœ

```
======================================================================
Test 3: Full 18-Layer KV Cache with Torch-TRT FP8
======================================================================

  FP16 Baseline (18 layers): 59.89 +/- 0.26 ms
  Per-layer: 3.33 ms

  Torch-TRT FP8 (18 layers): 20.39 +/- 0.07 ms
  Per-layer: 1.13 ms

  Speedup: 2.94x
  Cosine similarity: 0.999482
```

### 10.4 LIBERO å‡†ç¡®ç‡æµ‹è¯•

ä½¿ç”¨ `flash_fp8_freq1` åç«¯ï¼ˆPyTorch native FP8ï¼‰è¿›è¡Œ LIBERO quick testï¼š

```
Task suite: libero_spatial
Backend: flash_fp8_freq1, Denoising steps: 3

>>> Task 0: 3/3 (100.0%)
>>> Task 1: 3/3 (100.0%)
>>> Task 2: 3/3 (100.0%)

>>> Final Results: 9/9 (100.0%)
```

**å‡†ç¡®ç‡**: âœ… **100%** (9/9 quick test)

### 10.5 å»¶è¿Ÿæµ‹è¯•ç»“æœ

| Backend | å»¶è¿Ÿ | ååé‡ | ç›¸æ¯” FP16 |
|---------|------|--------|-----------|
| PyTorch FP16 (baseline) | 181.0 ms | 5.5 Hz | 1.00x |
| flash_fp8_freq1 (PyTorch FP8) | 182.9 ms | 5.5 Hz | 0.99x âŒ |

**ç»“è®º**: PyTorch native FP8 (`torch._scaled_mm`) åœ¨å®Œæ•´ç®¡é“ä¸­ **æ²¡æœ‰ä»»ä½•åŠ é€Ÿ**ï¼

### 10.6 æ ¹æœ¬åŸå› åˆ†æ

**ä¸ºä»€ä¹ˆ PyTorch native FP8 æ²¡æœ‰åŠ é€Ÿï¼Ÿ**

1. **Hidden tensor é‡åŒ–å¼€é”€è¿‡å¤§**
   - Hidden tensor å¤§å°: seq Ã— mlp_dim = 970 Ã— 16384 = 15.9M å…ƒç´ 
   - é‡åŒ–å¼€é”€: ~2ms per layer
   - 18 å±‚æ€»å¼€é”€: ~36ms
   - å®Œå…¨æŠµæ¶ˆäº† FP8 matmul çš„åŠ é€Ÿ

2. **Torch-TRT FP8 ä¸ºä»€ä¹ˆå¿«ï¼Ÿ**
   - TRT åœ¨ç¼–è¯‘æœŸèåˆ Q/DQ èŠ‚ç‚¹
   - é™æ€å›¾ä¼˜åŒ–ï¼Œæ— è¿è¡Œæ—¶é‡åŒ–å¼€é”€
   - ç›´æ¥ä½¿ç”¨ FP8 Tensor Core kernels

### 10.7 é¢„æœŸæ€§èƒ½æå‡ï¼ˆä½¿ç”¨ Torch-TRT FP8ï¼‰

```
å½“å‰çŠ¶æ€ (PyTorch native FP8):
  å®Œæ•´ç®¡é“: 180 ms (5.5 Hz)
  MLP éƒ¨åˆ†: ~60 ms (18 å±‚ Ã— 3.33 ms)

åº”ç”¨ Torch-TRT FP8 å:
  MLP éƒ¨åˆ†: ~20 ms (18 å±‚ Ã— 1.13 ms)
  MLP èŠ‚çœ: 40 ms

  é¢„æœŸå®Œæ•´ç®¡é“: ~140 ms (7.1 Hz)
  åŠ é€Ÿæ¯”: 1.29x
```

### 10.8 æ¨èçš„ä¸‹ä¸€æ­¥

1. **ç«‹å³**: å°† `FlashFP8KVCacheEngine` ä¸­çš„ `FP8HybridMLP` æ›¿æ¢ä¸º Torch-TRT FP8 ç¼–è¯‘ç‰ˆæœ¬
2. **é›†æˆæ–¹æ¡ˆ**:
   ```python
   # å½“å‰ (æ— æ•ˆ)
   class FP8HybridMLP:
       def forward(self, x):
           # ä½¿ç”¨ torch._scaled_mm - æ— åŠ é€Ÿ
           gate = torch._scaled_mm(x_fp8, self.gate_w_fp8.t(), ...)

   # æ¨è (æœ‰æ•ˆ)
   import torch_tensorrt
   import modelopt.torch.quantization as mtq

   # ç¼–è¯‘æœŸé‡åŒ–
   model_fp8 = mtq.quantize(model, mtq.FP8_DEFAULT_CFG, forward_loop=calibrate)
   trt_mlp = torch_tensorrt.compile(model_fp8, ...)
   ```

3. **é¢„æœŸæœ€ç»ˆæ€§èƒ½**:
   - å½“å‰: 180 ms (5.5 Hz)
   - + Torch-TRT FP8 MLP: **140 ms (7.1 Hz)**
   - + Flash Attention ä¼˜åŒ–: ~120 ms (8.3 Hz)
   - + æµæ°´çº¿å¹¶è¡Œ: ~100 ms (10 Hz)

### 10.9 æµ‹è¯•è„šæœ¬

| è„šæœ¬ | è¯´æ˜ | ç»“æœ |
|------|------|------|
| `scripts/benchmark_torch_trt_fp8_libero.py` | Torch-TRT vs PyTorch FP8 å¯¹æ¯” | âœ… 2.94x |
| `scripts/libero_eval_unified.py --backend flash_fp8_freq1` | LIBERO å‡†ç¡®ç‡ | âœ… 100% |
| `scripts/benchmark_fp8_static_libero.py` | Backend å»¶è¿Ÿå¯¹æ¯” | âœ… å®Œæˆ |

### 10.10 æ€»ç»“

| æŒ‡æ ‡ | å½“å‰ (flash_fp8_freq1) | é¢„æœŸ (Torch-TRT FP8) |
|------|------------------------|----------------------|
| LIBERO å‡†ç¡®ç‡ | 100% (9/9) | ~100% (cos=0.9995) |
| å»¶è¿Ÿ | 182.9 ms | ~140 ms |
| ååé‡ | 5.5 Hz | **7.1 Hz** |
| MLP åŠ é€Ÿæ¯” | 1.00x (æ— æ•ˆ) | **2.94x** |

**å…³é”®ç»“è®º**:
- âœ… LIBERO å‡†ç¡®ç‡: 100%ï¼ˆFP8 ç²¾åº¦è¶³å¤Ÿï¼‰
- âŒ PyTorch native FP8: æ— åŠ é€Ÿï¼ˆhidden é‡åŒ–å¼€é”€æŠµæ¶ˆæ”¶ç›Šï¼‰
- âœ… Torch-TRT FP8: 2.94x MLP åŠ é€Ÿï¼Œå¯æå‡è‡³ 7.1 Hz
- ğŸ“‹ ä¸‹ä¸€æ­¥: é›†æˆ Torch-TRT FP8 åˆ°å®Œæ•´ç®¡é“

---

## 11. Torch-TRT FP8 å®Œæ•´é›†æˆæµ‹è¯• (2026-02-04)

### 11.1 æµ‹è¯•ç›®æ ‡

å°† Torch-TRT FP8 MLP å®Œæ•´é›†æˆåˆ° LIBERO benchmark ä¸­ï¼ŒéªŒè¯å‡†ç¡®ç‡å’Œå»¶è¿Ÿã€‚

### 11.2 å®ç°æ–¹æ¡ˆ

åˆ›å»ºäº† `torch_trt_fp8` åç«¯ï¼Œä¸ºæ¯ä¸ª Transformer å±‚ç¼–è¯‘ç‹¬ç«‹çš„ TRT FP8 MLPï¼š

```python
# å…³é”®ä¿®å¤: æ¯å±‚ä½¿ç”¨ç‹¬ç«‹çš„ TRT MLPï¼ˆä¹‹å‰çš„ bug æ˜¯æ‰€æœ‰å±‚å…±ç”¨ layer 0 çš„æƒé‡ï¼‰
def compile_trt_fp8_mlps(model, device="cuda"):
    trt_mlps = []
    for i, layer in enumerate(model.layers):
        # æ¯å±‚ç‹¬ç«‹ç¼–è¯‘ï¼Œä½¿ç”¨è¯¥å±‚çš„æƒé‡
        trt_mlp = compile_trt_fp8_mlp_for_layer(layer, i, device)
        trt_mlps.append(trt_mlp)
    return trt_mlps
```

### 11.3 Bug ä¿®å¤è¿‡ç¨‹

#### Bug 1: æ‰€æœ‰å±‚å…±ç”¨ layer 0 æƒé‡ (å·²åœ¨ 11.2 ä¿®å¤)
æœ€åˆçš„å®ç°ä¸­ï¼Œæ‰€æœ‰ 18 å±‚å…±ç”¨åŒä¸€ä¸ª TRT MLPï¼ˆä½¿ç”¨ layer 0 çš„æƒé‡ï¼‰ã€‚

#### Bug 2: forward æ–¹æ³•é€»è¾‘é”™è¯¯ (å…³é”®!)
åŸå§‹çš„ forward æ–¹æ³•åªåšäº† attentionï¼Œæ²¡æœ‰åŠ ä¸Š MLP è¾“å‡ºï¼Œå¯¼è‡´ KV cache åŸºäºé”™è¯¯çš„ä¸­é—´çŠ¶æ€è®¡ç®—ï¼š
```python
# BUG: åªåš attentionï¼Œæ²¡æœ‰ MLPï¼
for layer in self.layers:
    normed = layer.input_layernorm(x)
    attn_output, k, v = layer.self_attn(normed, cos, sin, attention_mask)
    x = x + attn_output  # ç¼ºå°‘ MLP è¾“å‡º!
    all_keys.append(k)
```

**ä¿®å¤**: ä½¿ç”¨å®Œæ•´çš„ layer forwardï¼š
```python
# ä¿®å¤: æ¯å±‚éƒ½é€šè¿‡å®Œæ•´çš„ forwardï¼ˆåŒ…æ‹¬ TRT MLPï¼‰
for layer in self.layers:
    x, k, v = layer(x, cos, sin, attention_mask)  # ä½¿ç”¨å®Œæ•´çš„ layer forward
    all_keys.append(k)
    all_values.append(v)
```

### 11.4 ä¿®å¤å LIBERO æµ‹è¯•ç»“æœ

```
Backend: torch_trt_fp8 (æ¯å±‚ç‹¬ç«‹ TRT FP8 MLP, ä¿®å¤ forward bug)
Denoising steps: 3
TRT Compiled: 18/18 layers

>>> Task 0: 3/3 (100.0%)
>>> Task 1: 3/3 (100.0%)
>>> Task 2: 3/3 (100.0%)

>>> Final Results (libero_spatial): 9/9 (100.0%)
```

**å‡†ç¡®ç‡**: âœ… **100%** (9/9 å…¨éƒ¨æˆåŠŸ!)

### 11.5 å»¶è¿Ÿæµ‹è¯•ç»“æœ

| Backend | å»¶è¿Ÿ | ååé‡ | åŠ é€Ÿæ¯” |
|---------|------|--------|--------|
| `flash_fp8_freq1` | 188.09 ms | 5.32 Hz | 1.00x |
| `torch_trt_fp8` | 187.29 ms | 5.34 Hz | **1.00x** |

**å‘ç°**: å»¶è¿Ÿå‡ ä¹ç›¸åŒï¼Œæ²¡æœ‰åŠ é€Ÿï¼

### 11.6 ä¸ºä»€ä¹ˆæ²¡æœ‰å»¶è¿Ÿæ”¹å–„ï¼Ÿ

TensorRT æ—¥å¿—æ˜¾ç¤º FP8 scale è¢«å¿½ç•¥ï¼š
```
[DEQUANTIZE] [SCALE] has invalid precision FP8, ignored.
```

è¿™è¯´æ˜ï¼š
1. Thor å¹³å°çš„ TensorRT æ²¡æœ‰æ­£ç¡®æ”¯æŒ FP8 quantization scales
2. TRT å®é™…è¿è¡Œçš„æ˜¯ **FP16** è€Œä¸æ˜¯ FP8
3. æ‰€ä»¥æ²¡æœ‰è·å¾— FP8 çš„ 2.94x åŠ é€Ÿ

### 11.7 å…³é”®æ•™è®­

**0% å‡†ç¡®ç‡çš„æ ¹å› æ˜¯ä»£ç  bugï¼Œä¸æ˜¯ FP8 ç²¾åº¦é—®é¢˜ï¼**

| é—®é¢˜ | è¯¯åˆ¤ | çœŸå®åŸå›  |
|------|------|----------|
| 0% å‡†ç¡®ç‡ | "FP8 ç²¾åº¦ä¸è¶³" | forward æ–¹æ³•é€»è¾‘é”™è¯¯ |

ä¿®å¤ bug åï¼Œå‡†ç¡®ç‡ç«‹å³è¾¾åˆ° 100%ï¼Œè¯æ˜ FP8 ç²¾åº¦å®Œå…¨è¶³å¤Ÿç”¨äºæœºå™¨äººæ§åˆ¶ã€‚

### 11.8 ç»“è®º

**Torch-TRT FP8 åœ¨ Thor å¹³å°ä¸Šçš„çŠ¶æ€**:

| æŒ‡æ ‡ | ä¿®å¤å‰ | ä¿®å¤å |
|------|--------|--------|
| å‡†ç¡®ç‡ | 0% | **100%** âœ… |
| å»¶è¿Ÿ | - | 187.29 ms |
| åŠ é€Ÿæ¯” | - | 1.00x (æ— åŠ é€Ÿ) |

**æ ¹å› åˆ†æ**:
1. âœ… **å‡†ç¡®ç‡é—®é¢˜å·²ä¿®å¤**: æ˜¯ä»£ç  bugï¼Œä¸æ˜¯ FP8 ç²¾åº¦é—®é¢˜
2. âš ï¸ **å»¶è¿Ÿæ— æ”¹å–„**: Thor ä¸Š TRT FP8 scale è¢«å¿½ç•¥ï¼Œå®é™…è¿è¡Œ FP16

### 11.9 æ¨èæ–¹æ¡ˆ

| æ–¹æ¡ˆ | å‡†ç¡®ç‡ | å»¶è¿Ÿ | æ¨è |
|------|--------|------|------|
| `flash_fp8_freq1` | 100% | 188.09 ms | âœ… å½“å‰æœ€ä½³ |
| `torch_trt_fp8` | 100% | 187.29 ms | âš ï¸ å¯ç”¨ä½†æ— åŠ é€Ÿ |
| Vision TRT + PyTorch KV | - | ~140 ms | ğŸ“‹ ä¸‹ä¸€æ­¥ |

### 11.10 ä¸‹ä¸€æ­¥ä¼˜åŒ–æ–¹å‘

ç”±äº Thor å¹³å° FP8 æ²¡æœ‰çœŸæ­£çš„åŠ é€Ÿï¼Œå»ºè®®è½¬å‘å…¶ä»–ä¼˜åŒ–è·¯å¾„ï¼š

1. **Vision Encoder TRT**: SigLIP 44ms â†’ 12.5ms (å·²éªŒè¯)
2. **KV Cache Reuse**: å‡å°‘é‡å¤è®¡ç®—
3. **Async Pipeline**: æµæ°´çº¿å¹¶è¡Œ
4. **Denoise TRT**: Action Expert åŠ é€Ÿ

---

## 12. å®Œæ•´æµ‹è¯•è„šæœ¬åˆ—è¡¨

| è„šæœ¬ | è¯´æ˜ | ç»“æœ |
|------|------|------|
| `scripts/test_nvfp4_mixed_quant.py` | NVFP4 + W4A8 æµ‹è¯• | âš ï¸ æ•°å€¼é”™è¯¯ |
| `scripts/test_torch_trt_fp8.py` | Torch-TRT FP8 æµ‹è¯• | âœ… 2.65x |
| `scripts/test_fp8_static_graph.py` | FP8 é™æ€å›¾æµ‹è¯• | âœ… 2.3x |
| `scripts/test_fp8_static_graph_v2.py` | FP8 å¸¦å®½åˆ†æ | âœ… ç¨³å®š |
| `scripts/benchmark_fp8_static_libero.py` | LIBERO FP8 backend å¯¹æ¯” | âœ… å®Œæˆ |
| `scripts/benchmark_torch_trt_fp8_libero.py` | **Torch-TRT vs PyTorch FP8 å¯¹æ¯”** | âœ… **2.94x** |
| `scripts/libero_eval_unified.py` | LIBERO å‡†ç¡®ç‡è¯„ä¼° | âœ… 100% |
| `scripts/debug_trt_fp8_mlp.py` | TRT FP8 MLP ç²¾åº¦è¯Šæ–­ | âš ï¸ ç²¾åº¦ä¸è¶³ |
| `scripts/build_trt_fp8_aligned.py` | TRT API FP8 æµ‹è¯• | âŒ å´©æºƒ |
| `scripts/build_trt_fp4_mlp.py` | TRT API FP4 æµ‹è¯• | âŒ å´©æºƒ |

---

## 13. å‚è€ƒèµ„æ–™

### NVIDIA å®˜æ–¹
- [NVFP4 Blog](https://developer.nvidia.com/blog/introducing-nvfp4-for-efficient-and-accurate-low-precision-inference/)
- [TensorRT Quantization Guide](https://docs.nvidia.com/deeplearning/tensorrt/latest/inference-library/work-quantized-types.html)
- [Model Optimizer GitHub](https://github.com/NVIDIA/Model-Optimizer)

### TVM
- [TVM ONNX Tutorial](https://tvm.apache.org/docs/how_to/compile_models/from_onnx.html)
- [TVM BYOC Framework](https://tvm.apache.org/docs/v0.10.0/dev/how_to/relay_bring_your_own_codegen.html)

### GitHub Issues
- [#4590: FP8/FP4 silent fallback on Thor](https://github.com/NVIDIA/TensorRT/issues/4590)
- [#4599: ViT FP8 low performance on Thor](https://github.com/NVIDIA/TensorRT/issues/4599)
- [#8974: FP8/NVFP4 kernel not replaced](https://github.com/NVIDIA/TensorRT-LLM/issues/8974)

---

*Last Updated: 2026-02-04 (Torch-TRT FP8 å®Œæ•´é›†æˆæµ‹è¯• - ä¿®å¤bugå100%å‡†ç¡®ç‡ï¼Œä½†æ— å»¶è¿ŸåŠ é€Ÿ)*
