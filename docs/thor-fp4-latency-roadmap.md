# Thor FP4 Latency-Winning Roadmap v2

> **æ ¸å¿ƒç­–ç•¥è°ƒæ•´**: ä¼˜å…ˆKV Projection + Cache Fuseï¼Œè€ŒéMulti-Layer Persistent MLP

---

## ä¸ºä»€ä¹ˆä¼˜å…ˆKV Fuseï¼Ÿ

### ROIåˆ†æ
| ä¼˜åŒ–ç›®æ ‡ | å½“å‰å æ¯” | æ½œåœ¨æå‡ | æ•´ä½“æ”¶ç›Š |
|----------|----------|----------|----------|
| **KV Pipeline** | **57%** | 50%â†“ | **~1.4x æ¨ç†é€Ÿåº¦** |
| MLP | ~25% | 40%â†“ | ~10% |

### æŠ€æœ¯ä¼˜åŠ¿
1. **KVæ˜¯çº¯memory-bound** â†’ FP4æœ€å®¹æ˜“èµ¢çš„åœ°æ–¹
2. **GQA (num_kv_heads=1)** â†’ KV weightè¢«8ä¸ªqueryå…±äº«ï¼ŒFP4 broadcastæå…¶å‹å¥½
3. **å®ç°éš¾åº¦ä½** â†’ å±€éƒ¨ä¼˜åŒ–ï¼Œæ˜“éªŒè¯ï¼Œå¯¹TRT pluginå‹å¥½
4. **è¡Œä¸šè¶‹åŠ¿** â†’ FlashDecoding/PagedAttention/DeepSpeedå…¨éƒ¨é¦–å…ˆæ”»KV

---

## Pi0.5 æ¨¡å‹ç»“æ„

### PaLiGemma (Gemma 2B)
| Parameter | Value | å¯¹KV Fuseçš„æ„ä¹‰ |
|-----------|-------|-----------------|
| hidden_size | 2048 | Q/K/V projectionè¾“å…¥ç»´åº¦ |
| num_layers | 18 | éœ€è¦ä¼˜åŒ–çš„å±‚æ•° |
| num_heads | 8 | Query heads |
| **num_kv_heads** | **1 (GQA)** | **æé«˜weight reuse** |
| head_dim | 256 | æ¯ä¸ªheadçš„ç»´åº¦ |

### Action Expert (Gemma 300M)
| Parameter | Value |
|-----------|-------|
| hidden_size | 1024 |
| num_layers | 18 |
| num_heads | 8 |
| **num_kv_heads** | **1 (GQA)** |
| head_dim | 256 |

---

## KV Fuse æ ¸å¿ƒä¼˜åŠ¿

### ä¼ ç»Ÿè·¯å¾„
```
load Wq weight (BF16)     â†’ GEMV â†’ Q
load Wk weight (BF16)     â†’ GEMV â†’ K â†’ transpose â†’ global store
load Wv weight (BF16)     â†’ GEMV â†’ V â†’ transpose â†’ global store
```
- 3æ¬¡weight load
- 2æ¬¡global store + transpose
- æ— æ³•fusion

### FP4 Fused è·¯å¾„
```
load Wqkv packed (FP4)    â†’ decode in smem (ä¸€æ¬¡)
                          â†’ compute Q, K, V (weight reuse)
                          â†’ coalesced KV store (ç›´æ¥layout)
```
- Weight load: 3x â†’ 1x (FP4æ›´å°)
- Decode overhead: amortized across Q/K/V
- Global write: 2x â†’ ç›´æ¥æ­£ç¡®layout

### é¢„æœŸæ”¶ç›Š
```
KV 57% â†’ ~28%
æ•´ä½“: ~1.4x æ¨ç†é€Ÿåº¦
```

---

## æ‰§è¡Œè®¡åˆ’

### ğŸ¥‡ Phase 1: KV Projection + Cache Write Fuse (Week 1-2)

#### 1.1 Shared Memory FP4 Decode Cache
```cuda
// æ ¸å¿ƒæŠ€æœ¯
__shared__ uint8_t w_packed_smem[TILE_K * TILE_N / 2];
__shared__ float w_scale_smem[TILE_N * (TILE_K / 32)];
__shared__ float w_decoded_smem[TILE_K * TILE_N];

// cp.async å¼‚æ­¥åŠ è½½
cp_async_cg(w_packed_smem, &W_qkv[tile_offset]);
cp_async_wait_group(0);

// åœ¨ smem ä¸­ decode
for (int i = tid; i < TILE_K * TILE_N / 2; i += blockDim.x) {
    uint8_t packed = w_packed_smem[i];
    int scale_idx = (i * 2) / 32;
    float scale = w_scale_smem[scale_idx];
    w_decoded_smem[i * 2] = NVFP4_DECODE[packed & 0xF] * scale;
    w_decoded_smem[i * 2 + 1] = NVFP4_DECODE[packed >> 4] * scale;
}
__syncthreads();

// è®¡ç®—æ—¶å¤ç”¨ decoded weights
```

#### 1.2 Fused QKV Projection Kernel
```cuda
// Grid: (num_blocks_n, batch * seq, num_layers)
// Block: 256 threads

template<int HIDDEN_SIZE, int HEAD_DIM, int NUM_HEADS, int NUM_KV_HEADS>
__global__ void fused_qkv_fp4_kernel(
    const half* __restrict__ x,           // [B, S, hidden]
    const uint8_t* __restrict__ Wq,       // [num_heads * head_dim, hidden/2]
    const uint8_t* __restrict__ Wk,       // [num_kv_heads * head_dim, hidden/2]
    const uint8_t* __restrict__ Wv,       // [num_kv_heads * head_dim, hidden/2]
    const half* __restrict__ scale_q,
    const half* __restrict__ scale_k,
    const half* __restrict__ scale_v,
    half* __restrict__ Q_out,             // [B, S, num_heads, head_dim]
    half* __restrict__ K_cache,           // [B, max_seq, num_kv_heads, head_dim]
    half* __restrict__ V_cache,           // [B, max_seq, num_kv_heads, head_dim]
    int batch_size, int seq_len, int cache_pos
) {
    // 1. Load x to shared memory
    __shared__ half x_smem[HIDDEN_SIZE];
    cooperative_load(x, x_smem, blockIdx.y);

    // 2. For each output head dimension tile
    int head_idx = blockIdx.x / (HEAD_DIM / TILE_N);
    int tile_n = blockIdx.x % (HEAD_DIM / TILE_N);

    // 3. Decode FP4 weights in shared memory
    __shared__ half wq_decoded[TILE_K][TILE_N];
    __shared__ half wk_decoded[TILE_K][TILE_N];  // Reuse for GQA
    __shared__ half wv_decoded[TILE_K][TILE_N];

    // 4. Compute Q (for all 8 heads using different weight tiles)
    // 5. Compute K, V (for 1 KV head, shared across all Q heads)

    // 6. Direct write to KV cache with correct layout
    // é¿å… transpose!
    if (threadIdx.x < TILE_N) {
        int kv_idx = head_idx / (NUM_HEADS / NUM_KV_HEADS);  // 0 for GQA
        K_cache[batch_idx * max_seq * NUM_KV_HEADS * HEAD_DIM +
                cache_pos * NUM_KV_HEADS * HEAD_DIM +
                kv_idx * HEAD_DIM +
                tile_n * TILE_N + threadIdx.x] = k_result[threadIdx.x];
        // V similar
    }
}
```

#### 1.3 Warp-Cooperative KV Write
```cuda
// ç›´æ¥å†™å…¥æ­£ç¡®çš„ KV cache layout: [B, max_seq, num_kv_heads, head_dim]
// é¿å…ä¸­é—´ transpose

// Warp 0-3: è®¡ç®— Q heads 0-3
// Warp 4-7: è®¡ç®— Q heads 4-7
// Warp 0: åŒæ—¶è®¡ç®— K, V (å› ä¸º GQA åªæœ‰ 1 ä¸ª KV head)

// Coalesced write pattern:
// 32 threads å†™ 32 ä¸ªè¿ç»­çš„ head_dim å…ƒç´ 
```

#### 1.4 éªŒè¯å®éªŒ
```python
# å¯¹æ¯”
# A: Separate Q/K/V projections + KV cache write (cuBLAS)
# B: Fused QKV FP4 projection with direct KV cache write

def test_qkv_kv_fuse():
    # PaLiGemma dimensions
    hidden_size = 2048
    num_heads = 8
    num_kv_heads = 1
    head_dim = 256
    seq_len = 455  # å®é™… prefix pass

    # Benchmark both paths
    baseline_time = benchmark_separate_qkv(...)
    fused_time = benchmark_fused_qkv_fp4(...)

    print(f"Baseline: {baseline_time:.3f} ms")
    print(f"Fused FP4: {fused_time:.3f} ms")
    print(f"Speedup: {baseline_time / fused_time:.2f}x")
```

**é¢„æœŸç»“æœ**: å»¶è¿Ÿé™ä½ > 50% (KV pipelineéƒ¨åˆ†)

---

### ğŸ¥ˆ Phase 2: KV Cache Persistent Kernel (Week 2-3)

#### 2.1 SM Resident KV Shard
```cuda
// ä¿æŒ KV cache shard åœ¨ SM shared memory
// é¿å…åå¤ global memory round-trip

// Grid: persistent (num_SMs blocks)
// Each block holds a shard of KV cache

__global__ void persistent_kv_kernel(...) {
    // Shared memory: 48KB per block
    // å¯ä»¥ hold: 48KB / (256 * 2 * 2) = ~48 tokens per head
    __shared__ half kv_shard[48][256][2];  // [tokens, head_dim, k/v]

    while (has_work()) {
        // 1. Receive new token embedding
        // 2. Compute K, V projection (FP4)
        // 3. Update local KV shard
        // 4. Attention compute with local shard
        // 5. Only spill to global when shard full
    }
}
```

#### 2.2 Attention + KV Fuse
```cuda
// è¿›ä¸€æ­¥èåˆ attention è®¡ç®—
// é¿å… KV å†™å‡ºå†è¯»å›

fused_attention_with_kv_update(
    x,           // å½“å‰ token embedding
    Wq, Wk, Wv,  // FP4 weights
    kv_cache,    // åªæœ‰ cache miss æ—¶æ‰è®¿é—®
    output
) {
    // 1. Compute Q, K, V from x
    // 2. Update KV cache (å¦‚æœéœ€è¦)
    // 3. Attention: softmax(Q @ K^T) @ V
    // 4. Output projection
    // å…¨éƒ¨åœ¨ä¸€ä¸ª kernel é‡Œå®Œæˆ
}
```

---

### ğŸ¥‰ Phase 3: Multi-Layer Persistent MLP (Week 3-4)

> åªæœ‰åœ¨ Phase 1, 2 æˆåŠŸåæ‰å€¼å¾—åš

#### 3.1 Persistent MLP Kernel
```cuda
// åˆå¹¶å¤šå±‚ MLP åˆ°ä¸€ä¸ª persistent kernel
// å‡å°‘ kernel launch overhead å’Œ L2 thrashing

__global__ void persistent_mlp_kernel(
    const half* x,
    const uint8_t* gate_weights[NUM_LAYERS],
    const uint8_t* up_weights[NUM_LAYERS],
    const uint8_t* down_weights[NUM_LAYERS],
    // scales...
    half* output,
    int num_layers
) {
    // Shared memory for activation caching
    __shared__ half x_smem[HIDDEN_SIZE];
    __shared__ half intermediate_smem[INTERMEDIATE_SIZE];

    for (int layer = 0; layer < num_layers; layer++) {
        // 1. gate = silu(x @ gate_weight[layer])
        // 2. up = x @ up_weight[layer]
        // 3. x = (gate * up) @ down_weight[layer]
        // å…¨éƒ¨åœ¨ smem ä¸­å®Œæˆ
    }
}
```

---

### ğŸ Phase 4: TVM/TRT Integration (Week 4-5)

#### 4.1 TVM TensorIR Schedule
```python
@T.prim_func
def fused_qkv_kv_cache_fp4(
    x: T.Buffer[(B, S, 2048), "float16"],
    Wq_packed: T.Buffer[(2048, 1024), "uint8"],
    Wk_packed: T.Buffer[(256, 1024), "uint8"],
    Wv_packed: T.Buffer[(256, 1024), "uint8"],
    scale_q: T.Buffer[(2048, 64), "float16"],
    scale_k: T.Buffer[(256, 64), "float16"],
    scale_v: T.Buffer[(256, 64), "float16"],
    Q_out: T.Buffer[(B, S, 8, 256), "float16"],
    K_cache: T.Buffer[(B, MAX_SEQ, 1, 256), "float16"],
    V_cache: T.Buffer[(B, MAX_SEQ, 1, 256), "float16"],
    cache_pos: T.int32,
):
    # Tile ç­–ç•¥
    for bx in T.thread_binding(NUM_BLOCKS, "blockIdx.x"):
        for tx in T.thread_binding(256, "threadIdx.x"):
            # Shared memory decode
            with T.block("decode"):
                # cp.async load FP4 weights
                # decode in smem
                pass

            # Compute QKV
            with T.block("qkv_gemv"):
                # GEMV with decoded weights
                pass

            # Direct KV cache write
            with T.block("kv_write"):
                # Coalesced write to correct layout
                pass
```

#### 4.2 TRT Plugin å°è£…
```cpp
class FusedQKVKVCacheFP4Plugin : public IPluginV2DynamicExt {
public:
    // å¯¼å‡ºä¸º TRT plugin
    // é›†æˆåˆ°ç°æœ‰ TRT pipeline

    int enqueue(
        const PluginTensorDesc* inputDesc,
        const PluginTensorDesc* outputDesc,
        const void* const* inputs,
        void* const* outputs,
        void* workspace,
        cudaStream_t stream
    ) override {
        // Launch fused_qkv_kv_cache_fp4 kernel
        fused_qkv_kv_cache_fp4_kernel<<<grid, block, smem, stream>>>(
            inputs[0],  // x
            inputs[1],  // Wq_packed
            // ...
            outputs[0], // Q_out
            outputs[1], // K_cache (updated in-place)
            outputs[2], // V_cache (updated in-place)
            cache_pos
        );
        return 0;
    }
};
```

---

## Kernel è®¾è®¡ç»†èŠ‚

### Grid Mapping (Phase 1)
```
Grid:  (num_head_tiles, batch * seq, 1)
Block: (256, 1, 1)

num_head_tiles = num_heads * (head_dim / TILE_N)
              = 8 * (256 / 32) = 64 for Q
              = 1 * (256 / 32) = 8  for K, V

Total blocks per token: 64 + 8 + 8 = 80
```

### Warp Role Split
```
Block = 256 threads = 8 warps

Warp 0-5: Q projection (6 warps, 8 heads, ~1.33 heads/warp)
Warp 6:   K projection (1 warp, 1 KV head)
Warp 7:   V projection (1 warp, 1 KV head)

æˆ–è€…æ›´é«˜æ•ˆçš„ split:
Warp 0-7: äº¤æ›¿å¤„ç† Q/K/V tiles
          æ¯ä¸ª warp å¤„ç†ä¸åŒçš„ output tile
          å…±äº« decoded weight in smem
```

### Shared Memory Layout
```
Total: 48KB available

x_smem:        2048 * 2 = 4KB    (input activation)
wq_decoded:    32 * 32 * 2 = 2KB (one tile)
wk_decoded:    32 * 32 * 2 = 2KB (one tile)
wv_decoded:    32 * 32 * 2 = 2KB (one tile)
scale_smem:    32 * 2 = 64B      (scale tile)
accumulators:  åœ¨ register

Total used: ~10KB, è¶³å¤Ÿ!
```

### Packed FP4 Decode Strategy
```cuda
// Group scale broadcast - å‡å°‘ scale lookup
// æ¯ 32 ä¸ª FP4 å€¼å…±äº«ä¸€ä¸ª scale

__device__ void decode_tile_fp4(
    const uint8_t* packed,  // [TILE_K * TILE_N / 2]
    const half* scales,     // [TILE_N * (TILE_K / 32)]
    half* decoded           // [TILE_K, TILE_N]
) {
    int tid = threadIdx.x;
    int num_elements = TILE_K * TILE_N / 2;

    for (int i = tid; i < num_elements; i += blockDim.x) {
        int k = (i * 2) / TILE_N;
        int n = (i * 2) % TILE_N;
        int scale_idx = n * (TILE_K / 32) + k / 32;

        half scale = scales[scale_idx];
        uint8_t p = packed[i];

        decoded[k * TILE_N + n] = __hmul(NVFP4_LUT[p & 0xF], scale);
        decoded[(k+1) * TILE_N + n] = __hmul(NVFP4_LUT[p >> 4], scale);
    }
}
```

### KV Store Vectorization
```cuda
// ä½¿ç”¨ float4 (8 ä¸ª half) è¿›è¡Œ coalesced write

__device__ void store_kv_vectorized(
    half* k_cache,  // [B, max_seq, num_kv_heads, head_dim]
    half* v_cache,
    const half* k_result,  // [head_dim]
    const half* v_result,
    int batch_idx, int cache_pos, int kv_head_idx
) {
    // æ¯ 8 ä¸ªçº¿ç¨‹åä½œå†™ 64 bytes = 32 half values
    int lane = threadIdx.x % 32;

    if (lane < 32) {  // head_dim = 256 = 32 * 8 halfs
        float4* k_ptr = reinterpret_cast<float4*>(
            &k_cache[batch_idx * max_seq * num_kv_heads * head_dim +
                     cache_pos * num_kv_heads * head_dim +
                     kv_head_idx * head_dim +
                     lane * 8]);
        float4* v_ptr = reinterpret_cast<float4*>(
            &v_cache[/* same offset */]);

        *k_ptr = *reinterpret_cast<const float4*>(&k_result[lane * 8]);
        *v_ptr = *reinterpret_cast<const float4*>(&v_result[lane * 8]);
    }
}
```

---

## éªŒè¯é‡Œç¨‹ç¢‘

### Week 1
- [ ] Shared memory FP4 decode cache å®ç°
- [ ] å•ç‹¬ Q projection FP4 kernel éªŒè¯
- [ ] Decode latency æµ‹é‡ (ç›®æ ‡: < 0.05ms)

### Week 2
- [ ] Fused QKV kernel å®Œæˆ
- [ ] KV cache direct write å®ç°
- [ ] QKV fuse speedup éªŒè¯ (ç›®æ ‡: > 1.5x vs baseline)

### Week 3
- [ ] KV persistent kernel prototype
- [ ] Attention + KV fuse å®éªŒ
- [ ] End-to-end latency æµ‹é‡

### Week 4
- [ ] TVM TensorIR integration
- [ ] TRT plugin å°è£…
- [ ] Production deployment

---

## é¢„æœŸæœ€ç»ˆæ”¶ç›Š

| ä¼˜åŒ– | Latency è´¡çŒ® | ä¼˜åŒ–å |
|------|-------------|--------|
| KV Pipeline (57%) | 57% â†’ 28% | 29% saved |
| MLP (25%) | ä¿æŒæˆ–å¾®ä¼˜ | - |
| Others (18%) | ä¿æŒ | - |

**æ•´ä½“é¢„æœŸ**: 173ms â†’ ~120ms (~1.4x speedup, ~8.3 Hz)

---

## é£é™©ä¸å¤‡é€‰æ–¹æ¡ˆ

### é£é™©1: Shared memory decode overhead
**å¤‡é€‰**: Fragment-layout packing, ç›´æ¥MMA-compatible layout

### é£é™©2: GQA KV broadcast æ•ˆç‡
**å¤‡é€‰**: Warp shuffle broadcast

### é£é™©3: TRT plugin integration å¤æ‚
**å¤‡é€‰**: å…ˆç”¨ PyTorch CUDA extension éªŒè¯ï¼Œåç»­å†è¿ç§»

---

## å‚è€ƒå®ç°

- [FlashDecoding](https://github.com/Dao-AILab/flash-attention): KV cache optimization pattern
- [vLLM PagedAttention](https://github.com/vllm-project/vllm): KV cache memory management
- [CUTLASS FP4](https://github.com/NVIDIA/cutlass): Fragment layout reference
- [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM): Production KV cache integration
